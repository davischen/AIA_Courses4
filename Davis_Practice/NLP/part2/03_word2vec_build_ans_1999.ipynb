{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# how to build words relationship ? word2vec !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jovyan/Davis_Practice/NLP'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "from gensim.models import word2vec\n",
    "import random\n",
    "import logging\n",
    "import os\n",
    "\n",
    "## turn back to main directory\n",
    "os.chdir(\"../\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s: %(levelname)s: %(message)s')\n",
    "logging.root.setLevel(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load 'article_cutted'\n",
    "with open('article_cutted', 'rb') as file:\n",
    "    data = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_io.BufferedReader name='article_cutted'>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build word2vec\n",
    "# sg=0 CBOW ; sg=1 skip-gram\n",
    "model = word2vec.Word2Vec(size=256, min_count=5, window=5, sg=0, negative=15, iter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-31 08:51:15,955: INFO: collecting all words and their counts\n",
      "2019-05-31 08:51:15,957: INFO: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-05-31 08:51:15,973: INFO: collected 7225 word types from a corpus of 55629 raw words and 6047 sentences\n",
      "2019-05-31 08:51:15,974: INFO: Loading a fresh vocabulary\n",
      "2019-05-31 08:51:15,980: INFO: min_count=5 retains 1497 unique words (20% of original 7225, drops 5728)\n",
      "2019-05-31 08:51:15,980: INFO: min_count=5 leaves 46249 word corpus (83% of original 55629, drops 9380)\n",
      "2019-05-31 08:51:15,986: INFO: deleting the raw counts dictionary of 7225 items\n",
      "2019-05-31 08:51:15,989: INFO: sample=0.001 downsamples 59 most-common words\n",
      "2019-05-31 08:51:15,989: INFO: downsampling leaves estimated 35217 word corpus (76.1% of prior 46249)\n",
      "2019-05-31 08:51:15,990: INFO: estimated required memory for 1497 words and 256 dimensions: 3814356 bytes\n",
      "2019-05-31 08:51:15,995: INFO: resetting layer weights\n"
     ]
    }
   ],
   "source": [
    "# build vocabulary\n",
    "model.build_vocab(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-31 08:51:22,227: INFO: training model with 3 workers on 1497 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=15 window=5\n",
      "2019-05-31 08:51:22,348: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-31 08:51:22,367: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-31 08:51:22,375: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-31 08:51:22,376: INFO: training on 55629 raw words (35170 effective words) took 0.1s, 374072 effective words/s\n",
      "2019-05-31 08:51:22,378: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-31 08:51:22,385: INFO: training model with 3 workers on 1497 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=15 window=5\n",
      "2019-05-31 08:51:22,469: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-31 08:51:22,473: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-31 08:51:22,481: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-31 08:51:22,482: INFO: training on 55629 raw words (35208 effective words) took 0.1s, 397977 effective words/s\n",
      "2019-05-31 08:51:22,483: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-31 08:51:22,490: INFO: training model with 3 workers on 1497 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=15 window=5\n",
      "2019-05-31 08:51:22,588: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-31 08:51:22,605: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-31 08:51:22,614: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-31 08:51:22,615: INFO: training on 55629 raw words (35308 effective words) took 0.1s, 379437 effective words/s\n",
      "2019-05-31 08:51:22,616: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-31 08:51:22,623: INFO: training model with 3 workers on 1497 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=15 window=5\n",
      "2019-05-31 08:51:22,785: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-31 08:51:22,789: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-31 08:51:22,793: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-31 08:51:22,794: INFO: training on 55629 raw words (35223 effective words) took 0.1s, 391686 effective words/s\n",
      "2019-05-31 08:51:22,795: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-31 08:51:22,802: INFO: training model with 3 workers on 1497 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=15 window=5\n",
      "2019-05-31 08:51:22,949: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-31 08:51:22,968: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-31 08:51:22,972: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-31 08:51:22,973: INFO: training on 55629 raw words (35178 effective words) took 0.1s, 396249 effective words/s\n",
      "2019-05-31 08:51:22,974: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-31 08:51:22,981: INFO: training model with 3 workers on 1497 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=15 window=5\n",
      "2019-05-31 08:51:23,108: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-31 08:51:23,115: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-31 08:51:23,125: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-31 08:51:23,126: INFO: training on 55629 raw words (35192 effective words) took 0.1s, 338619 effective words/s\n",
      "2019-05-31 08:51:23,128: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-31 08:51:23,135: INFO: training model with 3 workers on 1497 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=15 window=5\n",
      "2019-05-31 08:51:23,227: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-31 08:51:23,233: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-31 08:51:23,240: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-31 08:51:23,240: INFO: training on 55629 raw words (35232 effective words) took 0.1s, 408480 effective words/s\n",
      "2019-05-31 08:51:23,243: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-31 08:51:23,248: INFO: training model with 3 workers on 1497 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=15 window=5\n",
      "2019-05-31 08:51:23,332: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-31 08:51:23,343: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-31 08:51:23,345: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-31 08:51:23,345: INFO: training on 55629 raw words (35243 effective words) took 0.1s, 397311 effective words/s\n",
      "2019-05-31 08:51:23,346: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-31 08:51:23,354: INFO: training model with 3 workers on 1497 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=15 window=5\n",
      "2019-05-31 08:51:23,509: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-31 08:51:23,520: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-31 08:51:23,527: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-31 08:51:23,527: INFO: training on 55629 raw words (35193 effective words) took 0.1s, 339268 effective words/s\n",
      "2019-05-31 08:51:23,529: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-31 08:51:23,536: INFO: training model with 3 workers on 1497 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=15 window=5\n",
      "2019-05-31 08:51:23,681: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-31 08:51:23,696: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-31 08:51:23,706: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-31 08:51:23,707: INFO: training on 55629 raw words (35113 effective words) took 0.1s, 386211 effective words/s\n",
      "2019-05-31 08:51:23,708: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-31 08:51:23,715: INFO: training model with 3 workers on 1497 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=15 window=5\n",
      "2019-05-31 08:51:23,803: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-31 08:51:23,816: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-31 08:51:23,820: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-31 08:51:23,821: INFO: training on 55629 raw words (35105 effective words) took 0.1s, 404560 effective words/s\n",
      "2019-05-31 08:51:23,822: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-31 08:51:23,831: INFO: training model with 3 workers on 1497 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=15 window=5\n",
      "2019-05-31 08:51:23,903: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-31 08:51:23,927: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-31 08:51:23,934: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-31 08:51:23,934: INFO: training on 55629 raw words (35229 effective words) took 0.1s, 383245 effective words/s\n",
      "2019-05-31 08:51:23,937: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-31 08:51:23,942: INFO: training model with 3 workers on 1497 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=15 window=5\n",
      "2019-05-31 08:51:24,081: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-31 08:51:24,093: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-31 08:51:24,105: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-31 08:51:24,105: INFO: training on 55629 raw words (35232 effective words) took 0.1s, 386015 effective words/s\n",
      "2019-05-31 08:51:24,107: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-31 08:51:24,112: INFO: training model with 3 workers on 1497 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=15 window=5\n",
      "2019-05-31 08:51:24,289: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-31 08:51:24,307: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-31 08:51:24,316: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-31 08:51:24,317: INFO: training on 55629 raw words (35234 effective words) took 0.1s, 271411 effective words/s\n",
      "2019-05-31 08:51:24,317: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-31 08:51:24,324: INFO: training model with 3 workers on 1497 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=15 window=5\n",
      "2019-05-31 08:51:24,493: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-31 08:51:24,499: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-31 08:51:24,503: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-31 08:51:24,503: INFO: training on 55629 raw words (35254 effective words) took 0.1s, 281830 effective words/s\n",
      "2019-05-31 08:51:24,504: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-31 08:51:24,511: INFO: training model with 3 workers on 1497 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=15 window=5\n",
      "2019-05-31 08:51:24,583: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-31 08:51:24,603: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-31 08:51:24,608: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-31 08:51:24,609: INFO: training on 55629 raw words (35177 effective words) took 0.1s, 391710 effective words/s\n",
      "2019-05-31 08:51:24,610: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-31 08:51:24,617: INFO: training model with 3 workers on 1497 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=15 window=5\n",
      "2019-05-31 08:51:24,705: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-31 08:51:24,706: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-31 08:51:24,712: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-31 08:51:24,712: INFO: training on 55629 raw words (35265 effective words) took 0.1s, 401711 effective words/s\n",
      "2019-05-31 08:51:24,715: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-31 08:51:24,720: INFO: training model with 3 workers on 1497 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=15 window=5\n",
      "2019-05-31 08:51:24,802: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-31 08:51:24,805: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-31 08:51:24,812: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-31 08:51:24,813: INFO: training on 55629 raw words (35212 effective words) took 0.1s, 418126 effective words/s\n",
      "2019-05-31 08:51:24,814: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-31 08:51:24,821: INFO: training model with 3 workers on 1497 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=15 window=5\n",
      "2019-05-31 08:51:24,977: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-31 08:51:24,979: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-31 08:51:24,981: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-31 08:51:24,982: INFO: training on 55629 raw words (35282 effective words) took 0.1s, 410846 effective words/s\n",
      "2019-05-31 08:51:24,983: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-31 08:51:24,990: INFO: training model with 3 workers on 1497 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=15 window=5\n",
      "2019-05-31 08:51:25,097: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-31 08:51:25,109: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-31 08:51:25,112: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-31 08:51:25,112: INFO: training on 55629 raw words (35144 effective words) took 0.1s, 306617 effective words/s\n",
      "2019-05-31 08:51:25,113: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    }
   ],
   "source": [
    "# train word2vec model ; shuffle data every epoch\n",
    "for i in range(20):\n",
    "    random.shuffle(data)\n",
    "    model.train(data, total_examples=len(data), epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.42171732, -0.14316127,  0.22812986, -0.23166692,  0.4249821 ,\n",
       "       -0.39662614, -0.8665342 , -0.22458965, -0.3214846 ,  0.5544727 ,\n",
       "        0.17551701,  0.27424327,  0.31189498, -0.08069796, -0.30147955,\n",
       "        0.40410388, -0.16224088,  0.07183569, -0.35312012,  0.28864604,\n",
       "       -0.05905059, -0.18634038, -0.24745551, -0.30264768, -0.25932807,\n",
       "       -0.07447345, -0.429827  , -0.32638618,  0.10868125,  0.05198107,\n",
       "       -0.07205133,  0.18586773,  0.15189913,  0.00585433,  0.0538315 ,\n",
       "        0.2047057 , -0.09133755, -0.0601277 , -0.27846324,  0.43209168,\n",
       "        0.20245829,  0.03976545,  0.09162848, -0.1886259 , -0.2232007 ,\n",
       "        0.38176847,  0.04687103, -0.29485402,  0.56277996,  0.48761463,\n",
       "        0.18485282, -0.12082795, -0.17879981,  0.3571175 , -0.31569555,\n",
       "        0.37688053, -0.1786795 , -0.33796126,  0.29192096,  0.01759403,\n",
       "        0.22019096,  0.2653555 , -0.23883654,  0.09753218,  0.09507053,\n",
       "        0.10228985,  0.1430462 ,  0.3724235 ,  0.5410018 ,  0.3429834 ,\n",
       "        0.11409634,  0.40436524, -0.00927744, -0.05044665, -0.6093519 ,\n",
       "       -0.22690824,  0.17912686, -0.17559057, -0.26691097,  0.29026955,\n",
       "       -0.11651952, -0.06103888,  0.13869096,  0.16502142,  0.1459436 ,\n",
       "       -0.07613458, -0.04947934,  0.17209159, -0.09343198,  0.02270422,\n",
       "       -0.28556883,  0.31155792,  0.3428456 , -0.201098  , -0.4841159 ,\n",
       "       -0.05176089,  0.46577904, -0.00898925,  0.09238164,  0.08177429,\n",
       "        0.0058013 ,  0.0459238 ,  0.11496423,  0.06417272,  0.16104391,\n",
       "        0.0023216 ,  0.95133054, -0.055487  , -0.10426263, -0.14830385,\n",
       "        0.35958865, -0.0808351 ,  0.10147125, -0.21803054,  0.71748704,\n",
       "       -0.26600435, -0.05913629, -0.04507977,  0.1975525 , -0.07258221,\n",
       "       -0.5936595 , -0.34366837,  0.06096908,  0.05327378, -0.00617341,\n",
       "       -0.04680731, -0.25966185,  0.21129797,  0.23104085,  0.27325898,\n",
       "       -0.30760804,  0.31851223,  0.04144328, -0.5434055 , -0.11891778,\n",
       "        0.3251494 , -0.19897734, -0.05277116, -0.2605352 , -0.3267747 ,\n",
       "       -0.26801732, -0.51523286,  0.34075817,  0.51897126,  0.34782946,\n",
       "        0.2132899 ,  0.37101293,  0.33201867, -0.4195552 ,  0.22189446,\n",
       "       -0.06429181,  0.24013916,  0.14322504,  0.16670275, -0.3477162 ,\n",
       "       -0.10468467, -0.17927253, -0.2974341 , -0.145291  , -0.08171447,\n",
       "        0.11750571,  0.19303441,  0.32200536,  0.04273227,  0.6621682 ,\n",
       "        0.0998909 ,  0.12086421, -0.51866335, -0.42098066, -0.23624425,\n",
       "       -0.09452292, -0.29513612,  0.47664702, -0.2619097 ,  0.24758463,\n",
       "        0.40965962, -0.40554076, -0.09772153, -0.1790751 , -0.28557009,\n",
       "       -0.0388703 , -0.62065506, -0.11675179, -0.19886006,  0.10488517,\n",
       "       -0.3427169 ,  0.18069199,  0.08633194,  0.10340255, -0.21327853,\n",
       "        0.09165271, -0.2268571 , -0.11084136, -0.23722845,  0.42200482,\n",
       "        0.1997399 ,  0.31082097, -0.29015943,  0.13086036,  0.18098466,\n",
       "       -0.6861844 ,  0.22732732,  0.07844923,  0.3580526 ,  0.2492868 ,\n",
       "       -0.16892207, -0.4139032 , -0.17913972,  0.27234378,  0.14190961,\n",
       "        0.43776014, -0.32169455,  0.07958905,  0.5425125 , -0.10414922,\n",
       "        0.02031552, -0.0151447 , -0.34602123, -0.0352611 , -0.07285418,\n",
       "       -0.28844565,  0.39885852, -0.42078674,  0.26251683, -0.00902781,\n",
       "       -0.1299773 ,  0.18189807, -0.24774666,  0.19221886,  0.3777828 ,\n",
       "       -0.13373213, -0.17513661,  0.39803797, -0.00724339,  0.05655606,\n",
       "       -0.01810635, -0.19274877, -0.03228952, -0.5023519 , -0.37370193,\n",
       "        0.13013962, -0.42371675, -0.14569318, -0.18585658,  0.39866444,\n",
       "        0.6167347 ,  0.2682414 , -0.35058504, -0.17974785, -0.52251005,\n",
       "        0.34403464, -0.25436366,  0.02361165,  0.0139579 ,  0.4464357 ,\n",
       "       -0.4631468 ], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## print an example\n",
    "model.wv['系統']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-31 08:51:31,559: INFO: saving Word2Vec object under part2/word2vec_model/1999_CBOW, separately None\n",
      "2019-05-31 08:51:31,561: INFO: not storing attribute syn0norm\n",
      "2019-05-31 08:51:31,563: INFO: not storing attribute cum_table\n",
      "2019-05-31 08:51:31,592: INFO: saved part2/word2vec_model/1999_CBOW\n"
     ]
    }
   ],
   "source": [
    "## save model\n",
    "model.save('part2/word2vec_model/1999_CBOW')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
