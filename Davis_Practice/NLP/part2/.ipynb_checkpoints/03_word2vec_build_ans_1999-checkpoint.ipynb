{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# how to build words relationship ? word2vec !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jovyan/Davis_Practice/NLP'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "from gensim.models import word2vec\n",
    "import random\n",
    "import logging\n",
    "import os\n",
    "\n",
    "## turn back to main directory\n",
    "os.chdir(\"../\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s: %(levelname)s: %(message)s')\n",
    "logging.root.setLevel(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load 'article_cutted'\n",
    "with open('article_cutted', 'rb') as file:\n",
    "    data = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build word2vec\n",
    "# sg=0 CBOW ; sg=1 skip-gram\n",
    "model = word2vec.Word2Vec(size=256, min_count=5, window=5, sg=0, negative=15, iter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-29 07:26:32,702: INFO: collecting all words and their counts\n",
      "2019-05-29 07:26:32,704: INFO: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-05-29 07:26:32,719: INFO: collected 7225 word types from a corpus of 55629 raw words and 6047 sentences\n",
      "2019-05-29 07:26:32,719: INFO: Loading a fresh vocabulary\n",
      "2019-05-29 07:26:32,724: INFO: min_count=5 retains 1497 unique words (20% of original 7225, drops 5728)\n",
      "2019-05-29 07:26:32,725: INFO: min_count=5 leaves 46249 word corpus (83% of original 55629, drops 9380)\n",
      "2019-05-29 07:26:32,729: INFO: deleting the raw counts dictionary of 7225 items\n",
      "2019-05-29 07:26:32,730: INFO: sample=0.001 downsamples 59 most-common words\n",
      "2019-05-29 07:26:32,731: INFO: downsampling leaves estimated 35217 word corpus (76.1% of prior 46249)\n",
      "2019-05-29 07:26:32,731: INFO: estimated required memory for 1497 words and 256 dimensions: 3814356 bytes\n",
      "2019-05-29 07:26:32,735: INFO: resetting layer weights\n"
     ]
    }
   ],
   "source": [
    "# build vocabulary\n",
    "model.build_vocab(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-29 07:26:37,123: INFO: training model with 3 workers on 1497 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=15 window=5\n",
      "2019-05-29 07:26:37,203: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-29 07:26:37,212: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-29 07:26:37,218: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-29 07:26:37,218: INFO: training on 55629 raw words (35109 effective words) took 0.1s, 395574 effective words/s\n",
      "2019-05-29 07:26:37,219: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-29 07:26:37,225: INFO: training model with 3 workers on 1497 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=15 window=5\n",
      "2019-05-29 07:26:37,310: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-29 07:26:37,314: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-29 07:26:37,319: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-29 07:26:37,320: INFO: training on 55629 raw words (35212 effective words) took 0.1s, 414285 effective words/s\n",
      "2019-05-29 07:26:37,321: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-29 07:26:37,328: INFO: training model with 3 workers on 1497 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=15 window=5\n",
      "2019-05-29 07:26:37,416: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-29 07:26:37,418: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-29 07:26:37,420: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-29 07:26:37,421: INFO: training on 55629 raw words (35263 effective words) took 0.1s, 419558 effective words/s\n",
      "2019-05-29 07:26:37,421: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-29 07:26:37,427: INFO: training model with 3 workers on 1497 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=15 window=5\n",
      "2019-05-29 07:26:37,505: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-29 07:26:37,514: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-29 07:26:37,521: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-29 07:26:37,521: INFO: training on 55629 raw words (35223 effective words) took 0.1s, 429740 effective words/s\n",
      "2019-05-29 07:26:37,522: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-29 07:26:37,527: INFO: training model with 3 workers on 1497 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=15 window=5\n",
      "2019-05-29 07:26:37,606: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-29 07:26:37,615: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-29 07:26:37,622: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-29 07:26:37,623: INFO: training on 55629 raw words (35349 effective words) took 0.1s, 418552 effective words/s\n",
      "2019-05-29 07:26:37,623: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-29 07:26:37,631: INFO: training model with 3 workers on 1497 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=15 window=5\n",
      "2019-05-29 07:26:37,709: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-29 07:26:37,723: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-29 07:26:37,729: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-29 07:26:37,730: INFO: training on 55629 raw words (35163 effective words) took 0.1s, 385998 effective words/s\n",
      "2019-05-29 07:26:37,731: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-29 07:26:37,736: INFO: training model with 3 workers on 1497 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=15 window=5\n",
      "2019-05-29 07:26:37,819: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-29 07:26:37,827: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-29 07:26:37,832: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-29 07:26:37,832: INFO: training on 55629 raw words (35061 effective words) took 0.1s, 414248 effective words/s\n",
      "2019-05-29 07:26:37,833: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-29 07:26:37,838: INFO: training model with 3 workers on 1497 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=15 window=5\n",
      "2019-05-29 07:26:37,921: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-29 07:26:37,922: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-29 07:26:37,929: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-29 07:26:37,929: INFO: training on 55629 raw words (35144 effective words) took 0.1s, 421420 effective words/s\n",
      "2019-05-29 07:26:37,930: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-29 07:26:37,934: INFO: training model with 3 workers on 1497 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=15 window=5\n",
      "2019-05-29 07:26:38,011: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-29 07:26:38,021: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-29 07:26:38,027: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-29 07:26:38,027: INFO: training on 55629 raw words (35150 effective words) took 0.1s, 419570 effective words/s\n",
      "2019-05-29 07:26:38,028: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-29 07:26:38,036: INFO: training model with 3 workers on 1497 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=15 window=5\n",
      "2019-05-29 07:26:38,113: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-29 07:26:38,125: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-29 07:26:38,132: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-29 07:26:38,132: INFO: training on 55629 raw words (35156 effective words) took 0.1s, 401488 effective words/s\n",
      "2019-05-29 07:26:38,132: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-29 07:26:38,137: INFO: training model with 3 workers on 1497 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=15 window=5\n",
      "2019-05-29 07:26:38,231: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-29 07:26:38,242: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-29 07:26:38,247: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-29 07:26:38,247: INFO: training on 55629 raw words (35196 effective words) took 0.1s, 367342 effective words/s\n",
      "2019-05-29 07:26:38,248: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-29 07:26:38,253: INFO: training model with 3 workers on 1497 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=15 window=5\n",
      "2019-05-29 07:26:38,325: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-29 07:26:38,338: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-29 07:26:38,344: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-29 07:26:38,345: INFO: training on 55629 raw words (35232 effective words) took 0.1s, 436839 effective words/s\n",
      "2019-05-29 07:26:38,345: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-29 07:26:38,350: INFO: training model with 3 workers on 1497 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=15 window=5\n",
      "2019-05-29 07:26:38,434: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-29 07:26:38,434: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-29 07:26:38,441: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-29 07:26:38,442: INFO: training on 55629 raw words (35181 effective words) took 0.1s, 418786 effective words/s\n",
      "2019-05-29 07:26:38,442: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-29 07:26:38,447: INFO: training model with 3 workers on 1497 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=15 window=5\n",
      "2019-05-29 07:26:38,537: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-29 07:26:38,540: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-29 07:26:38,544: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-29 07:26:38,544: INFO: training on 55629 raw words (35217 effective words) took 0.1s, 397410 effective words/s\n",
      "2019-05-29 07:26:38,544: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-29 07:26:38,549: INFO: training model with 3 workers on 1497 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=15 window=5\n",
      "2019-05-29 07:26:38,634: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-29 07:26:38,647: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-29 07:26:38,650: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-29 07:26:38,651: INFO: training on 55629 raw words (35176 effective words) took 0.1s, 379914 effective words/s\n",
      "2019-05-29 07:26:38,652: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-29 07:26:38,659: INFO: training model with 3 workers on 1497 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=15 window=5\n",
      "2019-05-29 07:26:38,736: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-29 07:26:38,757: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-29 07:26:38,764: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-29 07:26:38,765: INFO: training on 55629 raw words (35193 effective words) took 0.1s, 356074 effective words/s\n",
      "2019-05-29 07:26:38,766: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-29 07:26:38,773: INFO: training model with 3 workers on 1497 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=15 window=5\n",
      "2019-05-29 07:26:38,856: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-29 07:26:38,867: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-29 07:26:38,870: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-29 07:26:38,871: INFO: training on 55629 raw words (35293 effective words) took 0.1s, 389218 effective words/s\n",
      "2019-05-29 07:26:38,872: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-29 07:26:38,879: INFO: training model with 3 workers on 1497 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=15 window=5\n",
      "2019-05-29 07:26:38,959: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-29 07:26:38,965: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-29 07:26:38,971: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-29 07:26:38,972: INFO: training on 55629 raw words (35142 effective words) took 0.1s, 411478 effective words/s\n",
      "2019-05-29 07:26:38,973: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-29 07:26:38,978: INFO: training model with 3 workers on 1497 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=15 window=5\n",
      "2019-05-29 07:26:39,067: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-29 07:26:39,069: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-29 07:26:39,074: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-29 07:26:39,074: INFO: training on 55629 raw words (35282 effective words) took 0.1s, 412621 effective words/s\n",
      "2019-05-29 07:26:39,075: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-29 07:26:39,080: INFO: training model with 3 workers on 1497 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=15 window=5\n",
      "2019-05-29 07:26:39,161: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-29 07:26:39,170: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-29 07:26:39,178: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-29 07:26:39,179: INFO: training on 55629 raw words (35126 effective words) took 0.1s, 409274 effective words/s\n",
      "2019-05-29 07:26:39,179: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    }
   ],
   "source": [
    "# train word2vec model ; shuffle data every epoch\n",
    "for i in range(20):\n",
    "    random.shuffle(data)\n",
    "    model.train(data, total_examples=len(data), epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.31391782e-01, -9.05091688e-03, -1.10812396e-01, -7.81158209e-02,\n",
       "        2.52538156e-02,  2.33316913e-01,  4.60198343e-01,  1.33788899e-01,\n",
       "       -2.04606339e-01,  1.02522641e-01,  4.85710166e-02,  2.52316557e-02,\n",
       "       -2.01531082e-01, -4.41562891e-01,  1.84239093e-02,  1.72054265e-02,\n",
       "        9.65175033e-02,  9.41811502e-02, -3.71063083e-01,  2.54801005e-01,\n",
       "        4.56681639e-01, -3.79199386e-01,  1.53125718e-01,  1.93214670e-01,\n",
       "       -8.84598792e-02,  1.83179706e-01, -7.04239830e-02, -1.77268326e-01,\n",
       "        1.61232561e-01,  1.24864496e-01,  3.44789289e-02, -2.29162484e-01,\n",
       "       -2.40623236e-01,  1.33307558e-02,  3.32709253e-01,  2.35342026e-01,\n",
       "       -2.67805606e-01,  6.12302776e-03,  5.74571677e-02,  2.95528591e-01,\n",
       "       -5.48082054e-01,  2.85235316e-01, -8.30586776e-02,  1.10463358e-01,\n",
       "       -1.13173844e-02,  2.53503501e-01,  1.13027291e-02, -1.51793152e-01,\n",
       "       -7.15910569e-02,  1.53353304e-01, -1.29099905e-01, -1.04501925e-01,\n",
       "        7.31165856e-02,  2.28105947e-01, -6.58454299e-01,  2.80214064e-02,\n",
       "       -1.97967783e-01, -1.59366786e-01,  1.50170267e-01,  4.44861241e-02,\n",
       "       -3.70467603e-01,  2.25066900e-01, -2.11923435e-01,  3.89321856e-02,\n",
       "        3.87865528e-02,  1.29802048e-01,  1.30168628e-02, -3.15636873e-01,\n",
       "        3.92048389e-01,  5.55258282e-02,  8.08179528e-02, -4.62669297e-04,\n",
       "       -1.29153002e-02, -4.04739287e-03,  2.31354646e-02, -2.13417679e-01,\n",
       "        1.14748426e-01,  2.63873309e-01,  2.64232337e-01,  1.85627732e-02,\n",
       "       -1.87405661e-01, -5.26544414e-02, -4.53806341e-01, -1.22650452e-01,\n",
       "        2.18401596e-01,  1.41739413e-01, -6.87563792e-02,  3.89690734e-02,\n",
       "       -3.74686778e-01, -5.37709408e-02, -4.53742802e-01,  8.59148055e-02,\n",
       "        1.60149783e-01, -4.30619642e-02, -5.15974760e-01,  2.35873222e-01,\n",
       "       -4.58438993e-01, -8.71717408e-02,  2.37944070e-02,  3.16041820e-02,\n",
       "       -8.98099765e-02,  3.30334276e-01,  3.98834527e-01,  2.77426690e-02,\n",
       "        1.22954130e-01, -4.34699804e-02, -1.07212991e-01,  3.96530747e-01,\n",
       "        3.59235406e-02, -5.06110191e-01, -9.48461741e-02,  5.84097207e-02,\n",
       "       -2.42119908e-01,  1.22579694e-01, -5.84684350e-02,  1.93520710e-01,\n",
       "        1.67263165e-01, -2.64832824e-01,  1.43992633e-01, -5.37659824e-01,\n",
       "       -1.78491935e-01,  1.78859551e-02,  2.48330027e-01,  6.27081394e-02,\n",
       "       -5.48933148e-01, -2.96345681e-01, -9.34716761e-02,  6.05466664e-02,\n",
       "        8.02461922e-01, -2.13523939e-01,  1.46549776e-01, -9.62840319e-01,\n",
       "        2.69116890e-02, -1.35185674e-01,  2.02968255e-01, -1.89816996e-01,\n",
       "        1.69851467e-01, -1.22190565e-01, -4.72236603e-01, -4.24938619e-01,\n",
       "        1.73172981e-01,  2.76342273e-01,  1.53563857e-01,  7.38642141e-02,\n",
       "       -2.26854905e-01,  1.78520560e-01, -3.31495196e-01,  1.97875891e-02,\n",
       "        3.71156663e-01,  1.09719001e-01,  1.92961544e-01, -8.36512893e-02,\n",
       "       -8.74565318e-02, -4.15137470e-01, -1.17918581e-01,  1.68469027e-01,\n",
       "        2.33588349e-02,  1.26070932e-01,  2.96945572e-01, -5.12134552e-01,\n",
       "       -4.60758775e-01, -3.06925803e-01, -1.48463324e-01,  1.23548418e-01,\n",
       "       -2.30713040e-01,  2.64682353e-01, -1.47807881e-01, -9.41393375e-02,\n",
       "        6.75100505e-01, -8.24152008e-02, -6.20851636e-01,  3.68498228e-02,\n",
       "        3.92004639e-01, -1.83778226e-01, -4.41137373e-01, -1.27148163e-02,\n",
       "       -2.02861384e-01,  1.88293800e-01,  1.97646990e-01,  9.02537629e-02,\n",
       "        1.71128765e-01, -1.63571194e-01, -9.39101279e-02, -5.84387481e-01,\n",
       "       -2.31516227e-01, -3.56873162e-02,  3.55870605e-01, -6.43170178e-02,\n",
       "        3.85925025e-01, -6.80582225e-01,  1.71202555e-01, -4.57260460e-01,\n",
       "       -1.71802819e-01,  3.54553849e-01, -4.14236933e-02, -7.76777044e-02,\n",
       "       -3.64418656e-01, -1.29015550e-01,  3.98315117e-02,  2.53983766e-01,\n",
       "        8.97771940e-02, -4.49906081e-01,  8.15280676e-02, -4.81162518e-02,\n",
       "        1.46193087e-01,  1.10522665e-01,  4.64545935e-01,  6.97883010e-01,\n",
       "       -8.40144157e-01, -3.63634586e-01, -2.77481943e-01,  2.22437158e-01,\n",
       "       -2.30575409e-02,  9.25970599e-02,  2.49769948e-02, -3.04072231e-01,\n",
       "       -8.48152954e-03, -1.06360033e-01, -2.11200163e-01,  3.59767079e-01,\n",
       "        3.01239014e-01,  1.39826402e-01,  4.26937014e-01,  2.93083280e-01,\n",
       "       -4.78776246e-02,  8.64953473e-02, -3.94197047e-01, -2.17635632e-01,\n",
       "       -4.77196574e-02,  7.24155270e-03,  2.16397643e-02, -2.05770750e-02,\n",
       "        2.88038909e-01,  2.85265148e-01, -2.16963217e-01, -1.64396018e-01,\n",
       "        3.41502786e-01, -1.27039850e-01, -5.57802953e-02,  4.33797657e-01,\n",
       "        2.97657371e-01, -2.23366261e-01,  1.64659187e-01,  7.98089579e-02,\n",
       "       -3.05191487e-01,  1.79656208e-01, -6.08954765e-03, -1.23662770e-01,\n",
       "        3.04282099e-01, -8.88608918e-02, -1.22523075e-03, -4.35030490e-01,\n",
       "       -1.86157852e-01, -1.76068291e-01, -4.74580497e-01, -1.33720110e-04],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## print an example\n",
    "model.wv['系統']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-29 07:27:02,138: INFO: saving Word2Vec object under part2/word2vec_model/1999_CBOW, separately None\n",
      "2019-05-29 07:27:02,139: INFO: not storing attribute syn0norm\n",
      "2019-05-29 07:27:02,140: INFO: not storing attribute cum_table\n",
      "2019-05-29 07:27:02,167: INFO: saved part2/word2vec_model/1999_CBOW\n"
     ]
    }
   ],
   "source": [
    "## save model\n",
    "model.save('part2/word2vec_model/1999_CBOW')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
