{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# how to generate document vector ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jovyan/Davis_Practice'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "from gensim.models import Doc2Vec, doc2vec\n",
    "from gensim.models import word2vec\n",
    "import random\n",
    "import numpy as np\n",
    "import logging\n",
    "import os\n",
    "\n",
    "## turn back to main directory\n",
    "os.chdir(\"../\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s: %(levelname)s: %(message)s')\n",
    "logging.root.setLevel(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'article_cutted'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-b0da853a1149>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## load 'article_cutted'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'article_cutted'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'article_cutted'"
     ]
    }
   ],
   "source": [
    "## load 'article_cutted'\n",
    "with open('article_cutted', 'rb') as file:\n",
    "    data = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## average word vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-29 07:30:51,698: INFO: loading Word2Vec object from part2/word2vec_model/1999_CBOW\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'part2/word2vec_model/1999_CBOW'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-4a4774bcf989>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## load word2vec model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword2vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'part2/word2vec_model/1999_CBOW'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1520\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m         \u001b[0;31m# update older models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'table'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, fname, mmap)\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0mcompress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSaveLoad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_adapt_by_suffix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m         \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_specials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loaded %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36munpickle\u001b[0;34m(fname)\u001b[0m\n\u001b[1;32m    928\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0munpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m     \u001b[0;34m\"\"\"Load pickled object from `fname`\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0msmart_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m         \u001b[0;31m# Because of loading from S3 load can't be used (missing readline in smart_open)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36msmart_open\u001b[0;34m(uri, mode, **kw)\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mode should be a string'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m     \u001b[0mfobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_shortcut_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36m_shortcut_open\u001b[0;34m(uri, mode, **kw)\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPY3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_uri\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muri_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mopen_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopen_kwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_uri\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muri_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'part2/word2vec_model/1999_CBOW'"
     ]
    }
   ],
   "source": [
    "## load word2vec model\n",
    "model = word2vec.Word2Vec.load('part2/word2vec_model/1999_CBOW')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## filter words that not in word2vec's vocab\n",
    "data_filtered = [[w for w in l if w in model.wv] for l in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## compute average word vector\n",
    "avg_vector = []\n",
    "\n",
    "for l in data_filtered:\n",
    "    if len(l)==0:\n",
    "        avg_vector.append(np.array([0]*256))\n",
    "    else:\n",
    "        avg_vector.append(np.mean([model.wv[w] for w in l], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.0139285 , -0.39187518,  0.30997524, -0.02639583, -0.05615464,\n",
       "       -0.17006838,  0.09093032,  0.12481024, -0.11513121,  0.69859374,\n",
       "        0.05435597, -0.17942967,  0.19397376, -0.21798687, -0.19564486,\n",
       "       -0.23448524,  0.6245148 ,  0.30544153,  0.12568992, -0.17878732,\n",
       "        0.19276644, -0.0021207 ,  0.03991944,  0.00747062,  0.02577071,\n",
       "       -0.2516992 , -0.32358325, -0.2869339 ,  0.19776559,  0.08177391,\n",
       "       -0.04599835,  0.07135937, -0.00313927,  0.14666186,  0.19478728,\n",
       "        0.18374243, -0.04065815,  0.10906093,  0.46180308, -0.00594633,\n",
       "       -0.00173552,  0.16704375, -0.07027641,  0.07253345, -0.21797003,\n",
       "        0.19466257,  0.04524614,  0.1277492 ,  0.25559604,  0.08097415,\n",
       "        0.06736798,  0.00274889,  0.01478393, -0.01976258,  0.24934207,\n",
       "       -0.07983894,  0.00828975,  0.07848642, -0.08747084, -0.21230389,\n",
       "       -0.05773079, -0.42753103, -0.25437108, -0.35709623,  0.20307775,\n",
       "       -0.1370518 , -0.25532243,  0.08523082, -0.18767005,  0.16497818,\n",
       "        0.09665003,  0.01676647,  0.33797845, -0.44859478, -0.08978231,\n",
       "       -0.06323998,  0.21876478,  0.00303048, -0.19141841,  0.00730798,\n",
       "        0.01342099, -0.07333665, -0.14218563, -0.16132015,  0.48679602,\n",
       "        0.33931816,  0.172494  , -0.05806891,  0.13746792,  0.13782431,\n",
       "        0.051902  , -0.41191453, -0.2344234 , -0.03277351,  0.08055201,\n",
       "        0.17353159,  0.06764568,  0.05551548, -0.00288566,  0.01276947,\n",
       "       -0.2699756 , -0.03327787, -0.24586032, -0.2154724 ,  0.0768112 ,\n",
       "        0.07308732,  0.2451478 , -0.04945047, -0.15978386,  0.1802429 ,\n",
       "        0.26189682,  0.13400544,  0.13439907, -0.06424995,  0.3467588 ,\n",
       "        0.25281864,  0.00210852,  0.09388843, -0.2135952 , -0.07467007,\n",
       "       -0.08618179,  0.18870936, -0.22266318,  0.07133804, -0.19679007,\n",
       "        0.26460794,  0.0314558 , -0.17019169, -0.01408651, -0.09841104,\n",
       "       -0.24004455,  0.08695529,  0.43058822,  0.03712847,  0.122347  ,\n",
       "        0.29103956, -0.00348623, -0.00795252, -0.2827381 , -0.00172234,\n",
       "       -0.06303199,  0.17776494, -0.08859134,  0.2475779 ,  0.15499744,\n",
       "        0.4142296 , -0.05062487,  0.14295346,  0.10679779,  0.12079652,\n",
       "        0.00186654,  0.43019265,  0.06343298,  0.30667534,  0.15212344,\n",
       "       -0.02131412, -0.16673161, -0.0696182 ,  0.09845272,  0.05829411,\n",
       "        0.15438218, -0.11558788, -0.09648986,  0.21339308, -0.44750825,\n",
       "       -0.00341605,  0.12941256,  0.10731692, -0.23074365,  0.30556828,\n",
       "       -0.43521416, -0.14938475,  0.23212615,  0.04509592,  0.06611696,\n",
       "        0.07910047, -0.3689596 ,  0.08601427,  0.13296425, -0.08205783,\n",
       "       -0.13901938, -0.20299573,  0.00468242,  0.03083801,  0.11234017,\n",
       "       -0.19493309, -0.24861024,  0.09219632, -0.0304965 ,  0.05355639,\n",
       "       -0.16915381,  0.09658608,  0.16117649,  0.06603459, -0.02662434,\n",
       "       -0.05253616,  0.19610977,  0.00736556, -0.24326545,  0.3116794 ,\n",
       "       -0.3083243 , -0.30017644, -0.09872904, -0.01658627, -0.08955172,\n",
       "        0.05625769, -0.20591159, -0.04439213,  0.17664415, -0.04723593,\n",
       "       -0.06791607, -0.27083945, -0.35153487, -0.06394786,  0.3125339 ,\n",
       "        0.18010409,  0.00535459,  0.19723625, -0.10636905,  0.01831433,\n",
       "        0.15564089, -0.13982928,  0.31593212, -0.13104598, -0.04937408,\n",
       "        0.14191668, -0.18706731,  0.13311733,  0.01212184, -0.12680946,\n",
       "        0.01613708, -0.18410645, -0.15716274, -0.52195185,  0.01414667,\n",
       "        0.00350433, -0.27643937, -0.17284687, -0.11879332, -0.08764336,\n",
       "       -0.12565614, -0.02148012,  0.26237917, -0.14391242,  0.10111001,\n",
       "        0.3726557 , -0.03299749, -0.0723274 ,  0.03131708, -0.30534813,\n",
       "       -0.11363037, -0.00585137,  0.05985809, -0.29359487,  0.10206308,\n",
       "        0.04137683], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## print result\n",
    "avg_vector[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save result\n",
    "with open('avg_article_vector', 'wb') as file:\n",
    "    pickle.dump(avg_vector, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a document id map\n",
    "sentence_list = []\n",
    "\n",
    "for i, l in enumerate(data):\n",
    "    sentence_list.append(doc2vec.LabeledSentence(words=l, tags=[str(i)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabeledSentence(words=['無法', '透過', 'CAMP', 'SIT', '系統', 'icon', '連接', '登入', '可否', '協助', '一下', '問題'], tags=['0'])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## print result\n",
    "sentence_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define 轉換器\n",
    "model = Doc2Vec(size=256, min_count=5, window=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-29 07:30:54,032: INFO: collecting all words and their counts\n",
      "2019-05-29 07:30:54,033: INFO: PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "2019-05-29 07:30:54,060: INFO: collected 7225 word types and 6047 unique tags from a corpus of 6047 examples and 55629 words\n",
      "2019-05-29 07:30:54,061: INFO: Loading a fresh vocabulary\n",
      "2019-05-29 07:30:54,066: INFO: min_count=5 retains 1497 unique words (20% of original 7225, drops 5728)\n",
      "2019-05-29 07:30:54,066: INFO: min_count=5 leaves 46249 word corpus (83% of original 55629, drops 9380)\n",
      "2019-05-29 07:30:54,070: INFO: deleting the raw counts dictionary of 7225 items\n",
      "2019-05-29 07:30:54,070: INFO: sample=0.001 downsamples 59 most-common words\n",
      "2019-05-29 07:30:54,071: INFO: downsampling leaves estimated 35217 word corpus (76.1% of prior 46249)\n",
      "2019-05-29 07:30:54,071: INFO: estimated required memory for 1497 words and 256 dimensions: 11215884 bytes\n",
      "2019-05-29 07:30:54,074: INFO: resetting layer weights\n"
     ]
    }
   ],
   "source": [
    "## build vocabulary\n",
    "model.build_vocab(sentence_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-29 07:30:54,276: INFO: training model with 3 workers on 1497 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=5 window=4\n",
      "2019-05-29 07:30:54,482: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-29 07:30:54,504: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-29 07:30:54,507: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-29 07:30:54,507: INFO: training on 55629 raw words (41204 effective words) took 0.2s, 185232 effective words/s\n",
      "2019-05-29 07:30:54,508: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-29 07:30:54,513: INFO: training model with 3 workers on 1497 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=5 window=4\n",
      "2019-05-29 07:30:54,707: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-29 07:30:54,735: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-29 07:30:54,738: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-29 07:30:54,739: INFO: training on 55629 raw words (41262 effective words) took 0.2s, 188344 effective words/s\n",
      "2019-05-29 07:30:54,739: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-29 07:30:54,744: INFO: training model with 3 workers on 1497 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=5 window=4\n",
      "2019-05-29 07:30:54,930: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-29 07:30:54,952: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-29 07:30:54,956: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-29 07:30:54,957: INFO: training on 55629 raw words (41192 effective words) took 0.2s, 200445 effective words/s\n",
      "2019-05-29 07:30:54,957: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-29 07:30:54,962: INFO: training model with 3 workers on 1497 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=5 window=4\n",
      "2019-05-29 07:30:55,147: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-29 07:30:55,175: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-29 07:30:55,177: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-29 07:30:55,177: INFO: training on 55629 raw words (41262 effective words) took 0.2s, 197819 effective words/s\n",
      "2019-05-29 07:30:55,177: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-29 07:30:55,182: INFO: training model with 3 workers on 1497 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=5 window=4\n",
      "2019-05-29 07:30:55,422: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-29 07:30:55,443: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-29 07:30:55,454: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-29 07:30:55,455: INFO: training on 55629 raw words (41258 effective words) took 0.3s, 156190 effective words/s\n",
      "2019-05-29 07:30:55,455: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-29 07:30:55,460: INFO: training model with 3 workers on 1497 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=5 window=4\n",
      "2019-05-29 07:30:55,660: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-29 07:30:55,686: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-29 07:30:55,687: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-29 07:30:55,688: INFO: training on 55629 raw words (41401 effective words) took 0.2s, 188634 effective words/s\n",
      "2019-05-29 07:30:55,688: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-29 07:30:55,694: INFO: training model with 3 workers on 1497 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=5 window=4\n",
      "2019-05-29 07:30:55,899: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-29 07:30:55,926: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-29 07:30:55,928: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-29 07:30:55,929: INFO: training on 55629 raw words (41260 effective words) took 0.2s, 181888 effective words/s\n",
      "2019-05-29 07:30:55,929: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-29 07:30:55,934: INFO: training model with 3 workers on 1497 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=5 window=4\n",
      "2019-05-29 07:30:56,133: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-29 07:30:56,160: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-29 07:30:56,164: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-29 07:30:56,164: INFO: training on 55629 raw words (41313 effective words) took 0.2s, 186127 effective words/s\n",
      "2019-05-29 07:30:56,164: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-29 07:30:56,169: INFO: training model with 3 workers on 1497 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=5 window=4\n",
      "2019-05-29 07:30:56,379: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-29 07:30:56,400: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-29 07:30:56,403: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-29 07:30:56,403: INFO: training on 55629 raw words (41223 effective words) took 0.2s, 182288 effective words/s\n",
      "2019-05-29 07:30:56,404: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-29 07:30:56,409: INFO: training model with 3 workers on 1497 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=5 window=4\n",
      "2019-05-29 07:30:56,616: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-29 07:30:56,636: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-29 07:30:56,640: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-29 07:30:56,640: INFO: training on 55629 raw words (41235 effective words) took 0.2s, 183290 effective words/s\n",
      "2019-05-29 07:30:56,640: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-29 07:30:56,645: INFO: training model with 3 workers on 1497 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=5 window=4\n",
      "2019-05-29 07:30:56,839: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-29 07:30:56,869: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-29 07:30:56,870: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-29 07:30:56,871: INFO: training on 55629 raw words (41312 effective words) took 0.2s, 189721 effective words/s\n",
      "2019-05-29 07:30:56,871: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-29 07:30:56,876: INFO: training model with 3 workers on 1497 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=5 window=4\n",
      "2019-05-29 07:30:57,074: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-29 07:30:57,096: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-29 07:30:57,099: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-29 07:30:57,099: INFO: training on 55629 raw words (41296 effective words) took 0.2s, 192232 effective words/s\n",
      "2019-05-29 07:30:57,099: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-29 07:30:57,104: INFO: training model with 3 workers on 1497 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=5 window=4\n",
      "2019-05-29 07:30:57,299: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-29 07:30:57,320: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-29 07:30:57,323: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-29 07:30:57,324: INFO: training on 55629 raw words (41252 effective words) took 0.2s, 193967 effective words/s\n",
      "2019-05-29 07:30:57,324: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-29 07:30:57,329: INFO: training model with 3 workers on 1497 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=5 window=4\n",
      "2019-05-29 07:30:57,511: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-29 07:30:57,535: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-29 07:30:57,540: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-29 07:30:57,540: INFO: training on 55629 raw words (41186 effective words) took 0.2s, 202073 effective words/s\n",
      "2019-05-29 07:30:57,541: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-29 07:30:57,546: INFO: training model with 3 workers on 1497 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=5 window=4\n",
      "2019-05-29 07:30:57,730: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-29 07:30:57,758: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-29 07:30:57,759: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-29 07:30:57,760: INFO: training on 55629 raw words (41243 effective words) took 0.2s, 198348 effective words/s\n",
      "2019-05-29 07:30:57,760: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-29 07:30:57,765: INFO: training model with 3 workers on 1497 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=5 window=4\n",
      "2019-05-29 07:30:57,955: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-29 07:30:57,980: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-29 07:30:57,982: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-29 07:30:57,982: INFO: training on 55629 raw words (41122 effective words) took 0.2s, 195612 effective words/s\n",
      "2019-05-29 07:30:57,983: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-29 07:30:57,988: INFO: training model with 3 workers on 1497 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=5 window=4\n",
      "2019-05-29 07:30:58,180: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-29 07:30:58,212: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-29 07:30:58,215: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-29 07:30:58,215: INFO: training on 55629 raw words (41202 effective words) took 0.2s, 190838 effective words/s\n",
      "2019-05-29 07:30:58,215: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-29 07:30:58,221: INFO: training model with 3 workers on 1497 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=5 window=4\n",
      "2019-05-29 07:30:58,414: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-29 07:30:58,444: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-29 07:30:58,445: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-29 07:30:58,445: INFO: training on 55629 raw words (41272 effective words) took 0.2s, 189957 effective words/s\n",
      "2019-05-29 07:30:58,446: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-29 07:30:58,451: INFO: training model with 3 workers on 1497 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=5 window=4\n",
      "2019-05-29 07:30:58,652: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-29 07:30:58,673: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-29 07:30:58,676: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-29 07:30:58,676: INFO: training on 55629 raw words (41166 effective words) took 0.2s, 189395 effective words/s\n",
      "2019-05-29 07:30:58,676: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-29 07:30:58,682: INFO: training model with 3 workers on 1497 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=5 window=4\n",
      "2019-05-29 07:30:58,874: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-29 07:30:58,896: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-29 07:30:58,901: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-29 07:30:58,902: INFO: training on 55629 raw words (41210 effective words) took 0.2s, 193086 effective words/s\n",
      "2019-05-29 07:30:58,902: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    }
   ],
   "source": [
    "# train word2vec model ; shuffle data every epoch\n",
    "for i in range(20):\n",
    "    random.shuffle(sentence_list)\n",
    "    model.train(sentence_list, total_examples=len(data), epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6.25454704e-05, -1.84322465e-02,  2.59880144e-02,  1.98366144e-03,\n",
       "       -2.34506303e-03, -2.81660166e-02, -1.78552611e-04,  7.02655932e-04,\n",
       "       -7.22423382e-03,  1.56452414e-02, -1.32940030e-02,  4.21170937e-03,\n",
       "        1.22960592e-02, -1.12613561e-02, -1.79241654e-02, -1.38544850e-02,\n",
       "        4.06705774e-02,  2.56714895e-02,  2.75322329e-03,  1.70518421e-02,\n",
       "        4.10514604e-03, -1.81033816e-02,  7.58296717e-03,  6.64237468e-03,\n",
       "        1.56811252e-02, -1.07537024e-02, -1.60534736e-02, -9.27812327e-03,\n",
       "        3.75580997e-03,  1.71560142e-02,  1.56983100e-02,  1.56714600e-02,\n",
       "       -3.75061855e-03,  8.30757525e-03,  2.07202751e-02, -1.31824962e-03,\n",
       "        1.49021135e-03,  1.91826606e-03,  1.52330175e-02,  1.63495739e-03,\n",
       "        5.74622583e-03,  1.75354630e-02,  1.87964458e-02,  5.72441937e-03,\n",
       "       -7.78383715e-03, -1.95784215e-02, -1.00256968e-02,  2.34175976e-02,\n",
       "        1.18687721e-02, -2.08064425e-03,  1.35407355e-02,  1.02025568e-02,\n",
       "        1.32544972e-02, -6.35604886e-03,  1.66913364e-02, -1.27479178e-03,\n",
       "       -5.76658547e-03,  1.48010133e-02, -5.24683995e-03, -1.61519982e-02,\n",
       "       -4.84176772e-03, -2.02401392e-02, -2.44405493e-02, -1.94374193e-02,\n",
       "        1.53443450e-02,  2.33278214e-03, -1.51295690e-02, -9.38884169e-03,\n",
       "       -2.21899003e-02,  5.51563455e-03,  1.43138948e-03,  8.81256443e-03,\n",
       "       -1.09836657e-03, -3.32397334e-02, -1.48777021e-02, -1.62813198e-02,\n",
       "       -1.38985796e-03, -5.14074974e-03, -2.89816153e-03,  3.06217838e-03,\n",
       "        5.42793050e-03,  3.95461824e-03, -4.03079321e-06, -7.57595710e-03,\n",
       "        1.90546792e-02,  2.50934791e-02,  1.10364817e-02, -7.60112144e-03,\n",
       "        1.38723021e-02, -5.57390321e-03,  9.00295097e-03, -2.50710696e-02,\n",
       "       -4.38127015e-03,  1.66504097e-03,  2.08092947e-02,  1.89230498e-02,\n",
       "       -5.24033001e-03,  1.16242915e-02,  1.16953347e-03,  1.10390619e-03,\n",
       "       -1.51221547e-02, -3.79354903e-03, -5.01890108e-03, -1.68575626e-02,\n",
       "       -7.71678379e-03,  5.39269857e-03,  1.68832182e-03, -6.53721159e-03,\n",
       "       -2.09505344e-03,  1.12449052e-02,  1.95287857e-02, -4.14215308e-03,\n",
       "        1.84516162e-02, -2.12721713e-02,  1.02915559e-02,  5.58856642e-03,\n",
       "        1.22263078e-02, -6.94753369e-03, -4.38892841e-03, -3.69903864e-05,\n",
       "       -2.52064457e-03,  7.42854318e-05, -1.26426271e-03, -1.64390206e-02,\n",
       "       -1.17616318e-02,  6.34779874e-03,  7.07126129e-03, -9.64312814e-03,\n",
       "        1.37762148e-02,  1.15995649e-02, -9.41870827e-03, -4.55387775e-03,\n",
       "        2.36697607e-02,  1.35740777e-02, -1.38262305e-02,  4.94442275e-03,\n",
       "       -3.47631122e-03, -6.43140450e-03, -1.02455998e-02, -1.67843755e-02,\n",
       "       -1.63667742e-02, -4.26881394e-04, -1.76212657e-02,  8.31121951e-03,\n",
       "        1.43780550e-02,  1.31169781e-02,  2.12562270e-03, -7.02622114e-03,\n",
       "        1.47652077e-02,  9.53853782e-03, -4.93869418e-04,  1.24442987e-02,\n",
       "        7.20212702e-03,  1.04257818e-02,  1.08508207e-02,  4.20500059e-03,\n",
       "       -2.30433289e-02,  6.54404238e-03,  1.48471240e-02,  1.24166375e-02,\n",
       "        1.79821700e-02, -1.49696972e-02,  1.05207004e-02, -3.76481307e-03,\n",
       "       -1.24076344e-02,  8.72112811e-03,  1.10076498e-02,  5.18828025e-03,\n",
       "       -4.84172069e-03,  7.72948610e-03, -2.58910563e-02, -1.24092875e-02,\n",
       "        7.22767925e-03,  5.10955974e-03, -1.29892100e-02,  5.40837273e-03,\n",
       "       -6.23349892e-03,  9.14390781e-04,  2.64307354e-02,  7.46423984e-03,\n",
       "       -8.44534952e-05, -1.10578761e-02,  4.77691879e-03, -1.74026098e-02,\n",
       "        6.32179342e-03, -1.45991603e-02, -1.81930009e-02,  2.59951083e-03,\n",
       "        1.89334329e-04,  3.89700057e-03, -1.90011319e-02, -7.13319937e-03,\n",
       "        1.62977874e-02, -2.68047769e-03,  1.19432081e-02,  6.37757219e-03,\n",
       "        1.76643115e-02, -8.37894063e-03, -6.62985072e-03,  1.25956684e-02,\n",
       "       -1.07165491e-02, -3.34962853e-03, -9.48585570e-03, -2.37138830e-02,\n",
       "       -4.46719630e-03,  7.66310235e-03,  5.16920863e-03, -1.04417875e-02,\n",
       "        1.67781133e-02, -6.15288876e-03, -1.34952355e-03, -1.93776619e-02,\n",
       "       -2.32662056e-02, -5.53497719e-03,  8.67507048e-03, -1.86204934e-03,\n",
       "       -3.06414440e-05,  2.95773079e-03, -1.07299872e-02,  1.04317870e-02,\n",
       "       -1.35218892e-02, -1.29451696e-02,  2.14938819e-02, -2.09061801e-03,\n",
       "        6.48574485e-03,  1.84969027e-02, -1.73954256e-02,  1.40605140e-02,\n",
       "       -4.46271675e-04, -4.41009970e-03,  1.27305770e-02, -1.18548581e-02,\n",
       "       -6.05146121e-03, -1.45147350e-02, -2.27032043e-03,  9.89538967e-05,\n",
       "       -2.61218660e-02, -7.45308062e-04, -1.38550820e-02, -3.96096930e-02,\n",
       "       -1.30393319e-02,  1.29440511e-02,  5.68945287e-03, -4.47197491e-03,\n",
       "        9.60401085e-04,  1.73831750e-02,  1.36353401e-03,  5.52556431e-03,\n",
       "       -5.63918147e-04,  2.48630112e-03, -9.53096058e-03, -2.41368241e-03,\n",
       "        5.33606671e-03, -2.88237352e-02,  5.03264973e-03, -9.28208325e-03],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## print result\n",
    "model.docvecs['0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-29 07:32:34,647: INFO: saving Doc2Vec object under word2vec_model/1999_doc2vec, separately None\n",
      "2019-05-29 07:32:34,648: INFO: not storing attribute syn0norm\n",
      "2019-05-29 07:32:34,649: INFO: not storing attribute cum_table\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'word2vec_model/1999_doc2vec'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, fname_or_handle, separately, sep_limit, ignore, pickle_protocol)\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 495\u001b[0;31m             \u001b[0m_pickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname_or_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    496\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"saved %s object\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: file must have a 'write' attribute",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-be7bbb7951bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## save result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#model.save('1999_doc2vec')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'word2vec_model/1999_doc2vec'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1512\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'syn0norm'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'table'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cum_table'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1514\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[0msave\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSaveLoad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, fname_or_handle, separately, sep_limit, ignore, pickle_protocol)\u001b[0m\n\u001b[1;32m    496\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"saved %s object\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# `fname_or_handle` does not have write attribute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_smart_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname_or_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseparately\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep_limit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36m_smart_save\u001b[0;34m(self, fname, separately, sep_limit, ignore, pickle_protocol)\u001b[0m\n\u001b[1;32m    373\u001b[0m                                        compress, subname)\n\u001b[1;32m    374\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 375\u001b[0;31m             \u001b[0mpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    376\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m             \u001b[0;31m# restore attribs handled specially\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36mpickle\u001b[0;34m(obj, fname, protocol)\u001b[0m\n\u001b[1;32m    922\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m     \"\"\"\n\u001b[0;32m--> 924\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0msmart_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfout\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# 'b' for binary, needed on Windows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    925\u001b[0m         \u001b[0m_pickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36msmart_open\u001b[0;34m(uri, mode, **kw)\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mode should be a string'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m     \u001b[0mfobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_shortcut_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36m_shortcut_open\u001b[0;34m(uri, mode, **kw)\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPY3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_uri\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muri_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mopen_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopen_kwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_uri\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muri_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'word2vec_model/1999_doc2vec'"
     ]
    }
   ],
   "source": [
    "## save result\n",
    "#model.save('1999_doc2vec')\n",
    "model.save('word2vec_model/1999_doc2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
