{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization, GlobalAveragePooling2D\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from utils import load_data, load_test_data\n",
    "from utils import num_classes, epochs, batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_valid, y_valid = load_data(test_size=0.1, img_size=224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(484, 224, 224, 1)\n",
      "(484, 15)\n",
      "(54, 224, 224, 1)\n",
      "(54, 15)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_valid.shape)\n",
    "print(y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_14 (Conv2D)           (None, 224, 224, 32)      320       \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 224, 224, 32)      9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 112, 112, 32)      0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 112, 112, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 112, 112, 64)      18496     \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 112, 112, 64)      36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 112, 112, 64)      256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 56, 56, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 56, 56, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 56, 56, 128)       73856     \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 56, 56, 128)       147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 56, 56, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_3 ( (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 512)               66048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 15)                7695      \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 15)                60        \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 15)                0         \n",
      "=================================================================\n",
      "Total params: 627,755\n",
      "Trainable params: 625,293\n",
      "Non-trainable params: 2,462\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# a classic CNN model\n",
    "model_name = 'classic_CNN'\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=X_train.shape[1:]))\n",
    "model.add(Conv2D(32, (3, 3), padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
    "model.add(Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(GlobalAveragePooling2D())\n",
    "\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(num_classes))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(activation='softmax'))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.9473 - acc: 0.6295\n",
      "Epoch 00001: val_loss improved from inf to 2.02378, saving model to ./saved_models/cnn_0516a.h5\n",
      "30/30 [==============================] - 6s 210ms/step - loss: 0.9450 - acc: 0.6337 - val_loss: 2.0238 - val_acc: 0.4444\n",
      "Epoch 2/200\n",
      "29/30 [============================>.] - ETA: 0s - loss: 0.9533 - acc: 0.6379\n",
      "Epoch 00002: val_loss improved from 2.02378 to 1.78240, saving model to ./saved_models/cnn_0516a.h5\n",
      "30/30 [==============================] - 2s 52ms/step - loss: 0.9432 - acc: 0.6439 - val_loss: 1.7824 - val_acc: 0.3148\n",
      "Epoch 3/200\n",
      "29/30 [============================>.] - ETA: 0s - loss: 0.9491 - acc: 0.6315\n",
      "Epoch 00003: val_loss improved from 1.78240 to 1.35330, saving model to ./saved_models/cnn_0516a.h5\n",
      "30/30 [==============================] - 2s 51ms/step - loss: 0.9553 - acc: 0.6272 - val_loss: 1.3533 - val_acc: 0.5185\n",
      "Epoch 4/200\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 1.0082 - acc: 0.6049\n",
      "Epoch 00004: val_loss did not improve from 1.35330\n",
      "30/30 [==============================] - 1s 48ms/step - loss: 1.0272 - acc: 0.5982 - val_loss: 1.7084 - val_acc: 0.3519\n",
      "Epoch 5/200\n",
      "29/30 [============================>.] - ETA: 0s - loss: 0.8967 - acc: 0.6487\n",
      "Epoch 00005: val_loss did not improve from 1.35330\n",
      "30/30 [==============================] - 1s 50ms/step - loss: 0.9135 - acc: 0.6438 - val_loss: 3.2910 - val_acc: 0.3519\n",
      "Epoch 6/200\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 1.0938 - acc: 0.5647\n",
      "Epoch 00006: val_loss did not improve from 1.35330\n",
      "30/30 [==============================] - 1s 46ms/step - loss: 1.0795 - acc: 0.5739 - val_loss: 1.8715 - val_acc: 0.3704\n",
      "Epoch 7/200\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.9107 - acc: 0.6585\n",
      "Epoch 00007: val_loss did not improve from 1.35330\n",
      "30/30 [==============================] - 1s 47ms/step - loss: 0.9095 - acc: 0.6562 - val_loss: 1.5545 - val_acc: 0.5185\n",
      "Epoch 8/200\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 1.0300 - acc: 0.6027\n",
      "Epoch 00008: val_loss did not improve from 1.35330\n",
      "30/30 [==============================] - 1s 49ms/step - loss: 1.0380 - acc: 0.5958 - val_loss: 2.1782 - val_acc: 0.3519\n",
      "Epoch 9/200\n",
      "29/30 [============================>.] - ETA: 0s - loss: 0.9948 - acc: 0.5991\n",
      "Epoch 00009: val_loss did not improve from 1.35330\n",
      "30/30 [==============================] - 1s 47ms/step - loss: 0.9896 - acc: 0.6020 - val_loss: 1.7040 - val_acc: 0.3889\n",
      "Epoch 10/200\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.9161 - acc: 0.6406\n",
      "Epoch 00010: val_loss did not improve from 1.35330\n",
      "30/30 [==============================] - 1s 49ms/step - loss: 0.9312 - acc: 0.6293 - val_loss: 1.7357 - val_acc: 0.4074\n",
      "Epoch 11/200\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.9302 - acc: 0.6295\n",
      "Epoch 00011: val_loss did not improve from 1.35330\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "30/30 [==============================] - 2s 64ms/step - loss: 0.9256 - acc: 0.6292 - val_loss: 1.5997 - val_acc: 0.4444\n",
      "Epoch 12/200\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.9543 - acc: 0.5938\n",
      "Epoch 00012: val_loss did not improve from 1.35330\n",
      "30/30 [==============================] - 1s 48ms/step - loss: 0.9409 - acc: 0.6027 - val_loss: 1.4959 - val_acc: 0.4630\n",
      "Epoch 13/200\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.9454 - acc: 0.6205\n",
      "Epoch 00013: val_loss did not improve from 1.35330\n",
      "30/30 [==============================] - 1s 48ms/step - loss: 0.9360 - acc: 0.6272 - val_loss: 1.5422 - val_acc: 0.4259\n",
      "Epoch 14/200\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.8943 - acc: 0.6473\n",
      "Epoch 00014: val_loss did not improve from 1.35330\n",
      "30/30 [==============================] - 1s 47ms/step - loss: 0.9028 - acc: 0.6439 - val_loss: 1.4751 - val_acc: 0.4074\n",
      "Epoch 15/200\n",
      "29/30 [============================>.] - ETA: 0s - loss: 0.9270 - acc: 0.6401\n",
      "Epoch 00015: val_loss did not improve from 1.35330\n",
      "30/30 [==============================] - 1s 48ms/step - loss: 0.9334 - acc: 0.6274 - val_loss: 1.4792 - val_acc: 0.3889\n",
      "Epoch 16/200\n",
      "29/30 [============================>.] - ETA: 0s - loss: 0.8343 - acc: 0.6659\n",
      "Epoch 00016: val_loss did not improve from 1.35330\n",
      "30/30 [==============================] - 1s 50ms/step - loss: 0.8358 - acc: 0.6687 - val_loss: 1.4292 - val_acc: 0.4259\n",
      "Epoch 17/200\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.9237 - acc: 0.6496\n",
      "Epoch 00017: val_loss did not improve from 1.35330\n",
      "30/30 [==============================] - 1s 47ms/step - loss: 0.9108 - acc: 0.6521 - val_loss: 1.4453 - val_acc: 0.4259\n",
      "Epoch 18/200\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.8580 - acc: 0.6853\n",
      "Epoch 00018: val_loss did not improve from 1.35330\n",
      "30/30 [==============================] - 1s 48ms/step - loss: 0.8672 - acc: 0.6812 - val_loss: 1.3740 - val_acc: 0.4074\n",
      "Epoch 19/200\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.9189 - acc: 0.6317\n",
      "Epoch 00019: val_loss did not improve from 1.35330\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "30/30 [==============================] - 1s 48ms/step - loss: 0.9443 - acc: 0.6256 - val_loss: 1.5024 - val_acc: 0.4074\n",
      "Epoch 00019: early stopping\n"
     ]
    }
   ],
   "source": [
    "# Data generator with augmentation\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='constant',\n",
    "    cval=0)\n",
    "\n",
    "optimizer = keras.optimizers.Adam(lr=10e-4)\n",
    "\n",
    "model_path = './saved_models/cnn_0516a.h5'.format(model_name)\n",
    "checkpoint = ModelCheckpoint(model_path, monitor='val_loss', save_best_only=True, verbose=1)\n",
    "earlystop = EarlyStopping(monitor='val_loss', patience=16, verbose=1)\n",
    "\n",
    "\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_loss', patience=8, verbose=1, factor=0.1, min_lr=0.000001, cooldown=1)\n",
    "\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer, metrics=['accuracy'])\n",
    "# Fit the model on the batches generated by datagen.flow().\n",
    "batch_size = 16\n",
    "aug_ratio = 1\n",
    "epochs = 200\n",
    "steps_per_epoch = int(aug_ratio * X_train.shape[0] / batch_size)\n",
    "validation_steps = int(aug_ratio * X_valid.shape[0] / batch_size)\n",
    "model_history = model.fit_generator(datagen.flow(X_train, y_train, batch_size = batch_size),\n",
    "                                    epochs = epochs,\n",
    "                                    validation_data = (X_valid, y_valid),\n",
    "                                    callbacks = [checkpoint,learning_rate_reduction, earlystop],\n",
    "                                    steps_per_epoch=steps_per_epoch,\n",
    "                                    validation_steps=validation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loss = model_history.history['loss']\n",
    "val_loss = model_history.history['val_loss']\n",
    "\n",
    "plt.plot(training_loss, label=\"training_loss\")\n",
    "plt.plot(val_loss, label=\"validation_loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Learning Curve\")\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_acc = model_history.history['acc']\n",
    "val_acc = model_history.history['val_acc']\n",
    "\n",
    "plt.plot(training_acc, label=\"training_acc\")\n",
    "plt.plot(val_acc, label=\"validation_acc\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Acc\")\n",
    "plt.title(\"Learning Curve\")\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, X_id = load_test_data()\n",
    "\n",
    "model_path = './saved_models/cnn_0510a.h5'.format(model_name)\n",
    "model = load_model(model_path)\n",
    "\n",
    "scores = model.evaluate(X_valid, y_valid, verbose=1)\n",
    "print('Validation loss:', scores[0])\n",
    "print('Validation accuracy:', scores[1])\n",
    "\n",
    "y_test_pred = model.predict_classes(X_test)\n",
    "y_test_pred_df = pd.DataFrame({'id': np.array(X_id), 'class':y_test_pred}).sort_values(by='id')\n",
    "y_test_pred_df.to_csv('./cnn_0510a.csv'.format(model_name), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a classic CNN model\n",
    "model_name = 'classic_CNN_GlobalAveragePooling2D'\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same', input_shape=X_train.shape[1:]))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(Conv2D(256, (3, 3), padding='same', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(Conv2D(512, (3, 3), padding='same', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(Conv2D(512, (3, 3), padding='same', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(GlobalAveragePooling2D())\n",
    "\n",
    "model.add(Dense(128))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(activation='relu'))\n",
    "\n",
    "model.add(Dense(num_classes))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(activation='softmax'))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data generator with augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=10,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='constant',\n",
    "    cval=0)\n",
    "\n",
    "optimizer = keras.optimizers.Adam(lr=10e-4)\n",
    "\n",
    "model_path = './saved_models/cnn_0510001a.h5'.format(model_name)\n",
    "checkpoint = ModelCheckpoint(model_path, monitor='val_loss', save_best_only=True, verbose=1)\n",
    "earlystop = EarlyStopping(monitor='val_loss', patience=10, verbose=1)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer, metrics=['accuracy'])\n",
    "# Fit the model on the batches generated by datagen.flow().\n",
    "batch_size = 16\n",
    "aug_ratio = 1\n",
    "epochs = 200\n",
    "steps_per_epoch = int(aug_ratio * X_train.shape[0] / batch_size)\n",
    "validation_steps = int(aug_ratio * X_valid.shape[0] / batch_size)\n",
    "model_history = model.fit_generator(datagen.flow(X_train, y_train, batch_size = batch_size),\n",
    "                                    epochs = epochs,\n",
    "                                    validation_data = (X_valid, y_valid),\n",
    "                                    callbacks = [checkpoint, earlystop],\n",
    "                                    steps_per_epoch=steps_per_epoch,\n",
    "                                    validation_steps=validation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loss = model_history.history['loss']\n",
    "val_loss = model_history.history['val_loss']\n",
    "\n",
    "plt.plot(training_loss, label=\"training_loss\")\n",
    "plt.plot(val_loss, label=\"validation_loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Learning Curve\")\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_acc = model_history.history['acc']\n",
    "val_acc = model_history.history['val_acc']\n",
    "\n",
    "plt.plot(training_acc, label=\"training_acc\")\n",
    "plt.plot(val_acc, label=\"validation_acc\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Acc\")\n",
    "plt.title(\"Learning Curve\")\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, X_id = load_test_data()\n",
    "\n",
    "model_path = './saved_models/cnn_0510001a.h5'.format(model_name)\n",
    "model = load_model(model_path)\n",
    "\n",
    "scores = model.evaluate(X_valid, y_valid, verbose=1)\n",
    "print('Validation loss:', scores[0])\n",
    "print('Validation accuracy:', scores[1])\n",
    "\n",
    "y_test_pred = model.predict_classes(X_test)\n",
    "y_test_pred_df = pd.DataFrame({'id': np.array(X_id), 'class':y_test_pred}).sort_values(by='id')\n",
    "y_test_pred_df.to_csv('./submissions/cnn_0510001a.csv'.format(model_name), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
