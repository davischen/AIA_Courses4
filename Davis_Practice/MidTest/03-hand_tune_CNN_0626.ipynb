{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization, GlobalAveragePooling2D\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from utils import load_data, load_test_data\n",
    "from utils import num_classes, epochs, batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_valid, y_valid = load_data(test_size=0.1, img_size=224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(484, 224, 224, 1)\n",
      "(484, 15)\n",
      "(54, 224, 224, 1)\n",
      "(54, 15)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_valid.shape)\n",
    "print(y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#https://github.com/LB-Yu/TextCNN-keras/blob/master/TextCNN-keras.ipynb\n",
    "import keras.layers as L\n",
    "import keras\n",
    "def text_cnn(num_classes, vocab_size, sequence_length, embedding_size, \n",
    "               drop_out=0.5 ,l2_reg_lambda=0.0):\n",
    "    input_x = L.Input(shape=(sequence_length,), name='input_x')\n",
    "    filter_sizes = [3,4,5]\n",
    "    num_filters=128\n",
    "    # embedding layer\n",
    "    #if embedding_matrix is None:\n",
    "    embedding = L.Embedding(vocab_size, embedding_size, name='embedding')(input_x)\n",
    "    #else:\n",
    "    #    embedding = L.Embedding(vocab_size, embedding_size, weights=[embedding_matrix], name='embedding')(input_x)\n",
    "    expend_shape = [embedding.get_shape().as_list()[1], embedding.get_shape().as_list()[2], 1]\n",
    "    # embedding_chars = K.expand_dims(embedding, -1)    # 4D tensor [batch_size, seq_len, embeding_size, 1] seems like a gray picture\n",
    "    embedding_chars = L.Reshape(expend_shape)(embedding)\n",
    "    \n",
    "    # conv->max pool\n",
    "    pooled_outputs = []\n",
    "    for i, filter_size in enumerate(filter_sizes):\n",
    "        conv = L.Conv2D(filters=num_filters, \n",
    "                        kernel_size=[filter_size, embedding_size],\n",
    "                        strides=1,\n",
    "                        padding='valid',\n",
    "                        activation='relu',\n",
    "                        kernel_initializer=keras.initializers.TruncatedNormal(mean=0.0, stddev=0.1),\n",
    "                        bias_initializer=keras.initializers.constant(value=0.1),\n",
    "                        name=('conv_%d' % filter_size))(embedding_chars)\n",
    "        # print(\"conv-%d: \" % i, conv)\n",
    "        max_pool = L.MaxPool2D(pool_size=[sequence_length - filter_size + 1, 1],\n",
    "                               strides=(1, 1),\n",
    "                               padding='valid',\n",
    "                               name=('max_pool_%d' % filter_size))(conv)\n",
    "        pooled_outputs.append(max_pool)\n",
    "        # print(\"max_pool-%d: \" % i, max_pool)\n",
    "    \n",
    "    # combine all the pooled features\n",
    "    num_filters_total = num_filters * len(filter_sizes)\n",
    "    h_pool = L.Concatenate(axis=3)(pooled_outputs)\n",
    "    h_pool_flat = L.Reshape([num_filters_total])(h_pool)\n",
    "    # add dropout\n",
    "    dropout = L.Dropout(drop_out)(h_pool_flat)\n",
    "    \n",
    "    # output layer\n",
    "    output = L.Dense(num_classes,\n",
    "                     kernel_initializer='glorot_normal',\n",
    "                     bias_initializer=keras.initializers.constant(0.1),\n",
    "                     activation='softmax',\n",
    "                     name='output')(dropout)\n",
    "    model = keras.models.Model(inputs=input_x, outputs=output)\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'NUM_WORDS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-60476a3c5eb2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_cnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_WORDS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAX_LEN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m model.compile(optimizer='Adam', \n\u001b[1;32m      3\u001b[0m               \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m               metrics=['accuracy'])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'NUM_WORDS' is not defined"
     ]
    }
   ],
   "source": [
    "model = text_cnn(num_classes, NUM_WORDS, MAX_LEN,256)\n",
    "model.compile(optimizer='Adam', \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.9473 - acc: 0.6295\n",
      "Epoch 00001: val_loss improved from inf to 2.02378, saving model to ./saved_models/cnn_0516a.h5\n",
      "30/30 [==============================] - 6s 210ms/step - loss: 0.9450 - acc: 0.6337 - val_loss: 2.0238 - val_acc: 0.4444\n",
      "Epoch 2/200\n",
      "29/30 [============================>.] - ETA: 0s - loss: 0.9533 - acc: 0.6379\n",
      "Epoch 00002: val_loss improved from 2.02378 to 1.78240, saving model to ./saved_models/cnn_0516a.h5\n",
      "30/30 [==============================] - 2s 52ms/step - loss: 0.9432 - acc: 0.6439 - val_loss: 1.7824 - val_acc: 0.3148\n",
      "Epoch 3/200\n",
      "29/30 [============================>.] - ETA: 0s - loss: 0.9491 - acc: 0.6315\n",
      "Epoch 00003: val_loss improved from 1.78240 to 1.35330, saving model to ./saved_models/cnn_0516a.h5\n",
      "30/30 [==============================] - 2s 51ms/step - loss: 0.9553 - acc: 0.6272 - val_loss: 1.3533 - val_acc: 0.5185\n",
      "Epoch 4/200\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 1.0082 - acc: 0.6049\n",
      "Epoch 00004: val_loss did not improve from 1.35330\n",
      "30/30 [==============================] - 1s 48ms/step - loss: 1.0272 - acc: 0.5982 - val_loss: 1.7084 - val_acc: 0.3519\n",
      "Epoch 5/200\n",
      "29/30 [============================>.] - ETA: 0s - loss: 0.8967 - acc: 0.6487\n",
      "Epoch 00005: val_loss did not improve from 1.35330\n",
      "30/30 [==============================] - 1s 50ms/step - loss: 0.9135 - acc: 0.6438 - val_loss: 3.2910 - val_acc: 0.3519\n",
      "Epoch 6/200\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 1.0938 - acc: 0.5647\n",
      "Epoch 00006: val_loss did not improve from 1.35330\n",
      "30/30 [==============================] - 1s 46ms/step - loss: 1.0795 - acc: 0.5739 - val_loss: 1.8715 - val_acc: 0.3704\n",
      "Epoch 7/200\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.9107 - acc: 0.6585\n",
      "Epoch 00007: val_loss did not improve from 1.35330\n",
      "30/30 [==============================] - 1s 47ms/step - loss: 0.9095 - acc: 0.6562 - val_loss: 1.5545 - val_acc: 0.5185\n",
      "Epoch 8/200\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 1.0300 - acc: 0.6027\n",
      "Epoch 00008: val_loss did not improve from 1.35330\n",
      "30/30 [==============================] - 1s 49ms/step - loss: 1.0380 - acc: 0.5958 - val_loss: 2.1782 - val_acc: 0.3519\n",
      "Epoch 9/200\n",
      "29/30 [============================>.] - ETA: 0s - loss: 0.9948 - acc: 0.5991\n",
      "Epoch 00009: val_loss did not improve from 1.35330\n",
      "30/30 [==============================] - 1s 47ms/step - loss: 0.9896 - acc: 0.6020 - val_loss: 1.7040 - val_acc: 0.3889\n",
      "Epoch 10/200\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.9161 - acc: 0.6406\n",
      "Epoch 00010: val_loss did not improve from 1.35330\n",
      "30/30 [==============================] - 1s 49ms/step - loss: 0.9312 - acc: 0.6293 - val_loss: 1.7357 - val_acc: 0.4074\n",
      "Epoch 11/200\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.9302 - acc: 0.6295\n",
      "Epoch 00011: val_loss did not improve from 1.35330\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "30/30 [==============================] - 2s 64ms/step - loss: 0.9256 - acc: 0.6292 - val_loss: 1.5997 - val_acc: 0.4444\n",
      "Epoch 12/200\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.9543 - acc: 0.5938\n",
      "Epoch 00012: val_loss did not improve from 1.35330\n",
      "30/30 [==============================] - 1s 48ms/step - loss: 0.9409 - acc: 0.6027 - val_loss: 1.4959 - val_acc: 0.4630\n",
      "Epoch 13/200\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.9454 - acc: 0.6205\n",
      "Epoch 00013: val_loss did not improve from 1.35330\n",
      "30/30 [==============================] - 1s 48ms/step - loss: 0.9360 - acc: 0.6272 - val_loss: 1.5422 - val_acc: 0.4259\n",
      "Epoch 14/200\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.8943 - acc: 0.6473\n",
      "Epoch 00014: val_loss did not improve from 1.35330\n",
      "30/30 [==============================] - 1s 47ms/step - loss: 0.9028 - acc: 0.6439 - val_loss: 1.4751 - val_acc: 0.4074\n",
      "Epoch 15/200\n",
      "29/30 [============================>.] - ETA: 0s - loss: 0.9270 - acc: 0.6401\n",
      "Epoch 00015: val_loss did not improve from 1.35330\n",
      "30/30 [==============================] - 1s 48ms/step - loss: 0.9334 - acc: 0.6274 - val_loss: 1.4792 - val_acc: 0.3889\n",
      "Epoch 16/200\n",
      "29/30 [============================>.] - ETA: 0s - loss: 0.8343 - acc: 0.6659\n",
      "Epoch 00016: val_loss did not improve from 1.35330\n",
      "30/30 [==============================] - 1s 50ms/step - loss: 0.8358 - acc: 0.6687 - val_loss: 1.4292 - val_acc: 0.4259\n",
      "Epoch 17/200\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.9237 - acc: 0.6496\n",
      "Epoch 00017: val_loss did not improve from 1.35330\n",
      "30/30 [==============================] - 1s 47ms/step - loss: 0.9108 - acc: 0.6521 - val_loss: 1.4453 - val_acc: 0.4259\n",
      "Epoch 18/200\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.8580 - acc: 0.6853\n",
      "Epoch 00018: val_loss did not improve from 1.35330\n",
      "30/30 [==============================] - 1s 48ms/step - loss: 0.8672 - acc: 0.6812 - val_loss: 1.3740 - val_acc: 0.4074\n",
      "Epoch 19/200\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.9189 - acc: 0.6317\n",
      "Epoch 00019: val_loss did not improve from 1.35330\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "30/30 [==============================] - 1s 48ms/step - loss: 0.9443 - acc: 0.6256 - val_loss: 1.5024 - val_acc: 0.4074\n",
      "Epoch 00019: early stopping\n"
     ]
    }
   ],
   "source": [
    "# Data generator with augmentation\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='constant',\n",
    "    cval=0)\n",
    "\n",
    "optimizer = keras.optimizers.Adam(lr=10e-4)\n",
    "\n",
    "model_path = './saved_models/cnn_0516a.h5'.format(model_name)\n",
    "checkpoint = ModelCheckpoint(model_path, monitor='val_loss', save_best_only=True, verbose=1)\n",
    "earlystop = EarlyStopping(monitor='val_loss', patience=16, verbose=1)\n",
    "\n",
    "\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_loss', patience=8, verbose=1, factor=0.1, min_lr=0.000001, cooldown=1)\n",
    "\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer, metrics=['accuracy'])\n",
    "# Fit the model on the batches generated by datagen.flow().\n",
    "batch_size = 16\n",
    "aug_ratio = 1\n",
    "epochs = 200\n",
    "steps_per_epoch = int(aug_ratio * X_train.shape[0] / batch_size)\n",
    "validation_steps = int(aug_ratio * X_valid.shape[0] / batch_size)\n",
    "model_history = model.fit_generator(datagen.flow(X_train, y_train, batch_size = batch_size),\n",
    "                                    epochs = epochs,\n",
    "                                    validation_data = (X_valid, y_valid),\n",
    "                                    callbacks = [checkpoint,learning_rate_reduction, earlystop],\n",
    "                                    steps_per_epoch=steps_per_epoch,\n",
    "                                    validation_steps=validation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loss = model_history.history['loss']\n",
    "val_loss = model_history.history['val_loss']\n",
    "\n",
    "plt.plot(training_loss, label=\"training_loss\")\n",
    "plt.plot(val_loss, label=\"validation_loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Learning Curve\")\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_acc = model_history.history['acc']\n",
    "val_acc = model_history.history['val_acc']\n",
    "\n",
    "plt.plot(training_acc, label=\"training_acc\")\n",
    "plt.plot(val_acc, label=\"validation_acc\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Acc\")\n",
    "plt.title(\"Learning Curve\")\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, X_id = load_test_data()\n",
    "\n",
    "model_path = './saved_models/cnn_0510a.h5'.format(model_name)\n",
    "model = load_model(model_path)\n",
    "\n",
    "scores = model.evaluate(X_valid, y_valid, verbose=1)\n",
    "print('Validation loss:', scores[0])\n",
    "print('Validation accuracy:', scores[1])\n",
    "\n",
    "y_test_pred = model.predict_classes(X_test)\n",
    "y_test_pred_df = pd.DataFrame({'id': np.array(X_id), 'class':y_test_pred}).sort_values(by='id')\n",
    "y_test_pred_df.to_csv('./cnn_0510a.csv'.format(model_name), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a classic CNN model\n",
    "model_name = 'classic_CNN_GlobalAveragePooling2D'\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same', input_shape=X_train.shape[1:]))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(Conv2D(256, (3, 3), padding='same', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(Conv2D(512, (3, 3), padding='same', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(Conv2D(512, (3, 3), padding='same', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(GlobalAveragePooling2D())\n",
    "\n",
    "model.add(Dense(128))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(activation='relu'))\n",
    "\n",
    "model.add(Dense(num_classes))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(activation='softmax'))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data generator with augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=10,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='constant',\n",
    "    cval=0)\n",
    "\n",
    "optimizer = keras.optimizers.Adam(lr=10e-4)\n",
    "\n",
    "model_path = './saved_models/cnn_0510001a.h5'.format(model_name)\n",
    "checkpoint = ModelCheckpoint(model_path, monitor='val_loss', save_best_only=True, verbose=1)\n",
    "earlystop = EarlyStopping(monitor='val_loss', patience=10, verbose=1)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer, metrics=['accuracy'])\n",
    "# Fit the model on the batches generated by datagen.flow().\n",
    "batch_size = 16\n",
    "aug_ratio = 1\n",
    "epochs = 200\n",
    "steps_per_epoch = int(aug_ratio * X_train.shape[0] / batch_size)\n",
    "validation_steps = int(aug_ratio * X_valid.shape[0] / batch_size)\n",
    "model_history = model.fit_generator(datagen.flow(X_train, y_train, batch_size = batch_size),\n",
    "                                    epochs = epochs,\n",
    "                                    validation_data = (X_valid, y_valid),\n",
    "                                    callbacks = [checkpoint, earlystop],\n",
    "                                    steps_per_epoch=steps_per_epoch,\n",
    "                                    validation_steps=validation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loss = model_history.history['loss']\n",
    "val_loss = model_history.history['val_loss']\n",
    "\n",
    "plt.plot(training_loss, label=\"training_loss\")\n",
    "plt.plot(val_loss, label=\"validation_loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Learning Curve\")\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_acc = model_history.history['acc']\n",
    "val_acc = model_history.history['val_acc']\n",
    "\n",
    "plt.plot(training_acc, label=\"training_acc\")\n",
    "plt.plot(val_acc, label=\"validation_acc\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Acc\")\n",
    "plt.title(\"Learning Curve\")\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, X_id = load_test_data()\n",
    "\n",
    "model_path = './saved_models/cnn_0510001a.h5'.format(model_name)\n",
    "model = load_model(model_path)\n",
    "\n",
    "scores = model.evaluate(X_valid, y_valid, verbose=1)\n",
    "print('Validation loss:', scores[0])\n",
    "print('Validation accuracy:', scores[1])\n",
    "\n",
    "y_test_pred = model.predict_classes(X_test)\n",
    "y_test_pred_df = pd.DataFrame({'id': np.array(X_id), 'class':y_test_pred}).sort_values(by='id')\n",
    "y_test_pred_df.to_csv('./submissions/cnn_0510001a.csv'.format(model_name), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
