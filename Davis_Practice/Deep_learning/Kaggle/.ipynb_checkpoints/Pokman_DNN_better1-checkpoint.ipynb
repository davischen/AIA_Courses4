{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here, we decide to learn tensorflow from a simple example directly!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General writing flow\n",
    "1. import required libraries\n",
    "2. load data and do some data pre-processing\n",
    "3. split your data into training and validation set\n",
    "4. build the network\n",
    "5. train the model and record/monitoring the performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import required libries and set some parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm_notebook\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load data and do some pre-processing\n",
    "We use MNIST HERE (with sklearn 8x8 version rather than use tensorflow 28x28 version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "train = pd.read_csv(\"train-3.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "testID = test['id']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop Null Feature(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_all = pd.concat( [ train.drop( [ 'Id', 'SalePrice' ], axis = 1 ),\n",
    "#                      test.drop( [ 'Id' ], axis = 1 ) ],\n",
    "#                      axis = 0, ignore_index = False )\n",
    "#train.drop_duplicates(keep='first', inplace=False) \n",
    "#test.drop_duplicates(keep='first', inplace=False) \n",
    "#train_data=train.drop('class', axis=1)\n",
    "df_all = pd.concat([train, test], keys=['train', 'test'], sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_all.drop(['Id'], axis=1, inplace=True)\n",
    "#df_all.drop(['id'], axis=1, inplace=True)\n",
    "#df_all.drop(['city'],axis=1,inplace=True)\n",
    "#df_all.drop(['appearedHour'], axis=1, inplace=True)\n",
    "#df_all.drop(['appearedMinute'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8951, 184)\n",
      "class    1791\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# check null status\n",
    "print( df_all.shape )\n",
    "df_na = df_all.select_dtypes( exclude = [ 'object' ] ).isnull().sum()\n",
    "print( df_na[ df_na > 100 ].sort_values(ascending=False) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train = train.drop( labels = train[ train['class'].isna().any() ].index, axis = 0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['class'] = df_all['class'].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def FunctionalToInt(x):\n",
    "    if(x==False):\n",
    "        r = 0\n",
    "    else:\n",
    "        r = 1\n",
    "    return r\n",
    "\n",
    "df_all['closeToWater'] = df_all['closeToWater'].apply(FunctionalToInt)\n",
    "df_all['urban'] = df_all['urban'].apply(FunctionalToInt)\n",
    "df_all['suburban'] = df_all['suburban'].apply(FunctionalToInt)\n",
    "df_all['midurban'] = df_all['midurban'].apply(FunctionalToInt)\n",
    "df_all['rural'] = df_all['rural'].apply(FunctionalToInt)\n",
    "df_all['gymIn100m'] = df_all['gymIn100m'].apply(FunctionalToInt)\n",
    "df_all['gymIn250m'] = df_all['gymIn250m'].apply(FunctionalToInt)\n",
    "df_all['gymIn500m'] = df_all['gymIn500m'].apply(FunctionalToInt)\n",
    "df_all['gymIn1000m'] = df_all['gymIn1000m'].apply(FunctionalToInt)\n",
    "df_all['gymIn2500m'] = df_all['gymIn2500m'].apply(FunctionalToInt)\n",
    "df_all['gymIn5000m'] = df_all['gymIn5000m'].apply(FunctionalToInt)\n",
    "df_all['pokestopIn100m'] = df_all['pokestopIn100m'].apply(FunctionalToInt)\n",
    "df_all['pokestopIn250m'] = df_all['pokestopIn250m'].apply(FunctionalToInt)\n",
    "df_all['pokestopIn500m'] = df_all['pokestopIn500m'].apply(FunctionalToInt)\n",
    "df_all['pokestopIn1000m'] = df_all['pokestopIn1000m'].apply(FunctionalToInt)\n",
    "\n",
    "df_all['pokestopIn2500m'] = df_all['pokestopIn2500m'].apply(FunctionalToInt)\n",
    "df_all['pokestopIn5000m'] = df_all['pokestopIn5000m'].apply(FunctionalToInt)\n",
    "\n",
    "\n",
    "for num in range(1,152):\n",
    "    df_all['cooc_'+str(num)] = df_all['cooc_'+str(num)].apply(FunctionalToInt)\n",
    "\n",
    "\n",
    "\n",
    "def FunctionalToDay(x):\n",
    "    if(x=='morning'):\n",
    "        r = 0\n",
    "    elif(x=='afternoon'):\n",
    "        r = 1\n",
    "    elif(x=='evening'):\n",
    "        r = 2\n",
    "    elif(x=='night'):\n",
    "        r = 3\n",
    "    else:\n",
    "        r = 4\n",
    "    return r\n",
    "\n",
    "df_all['appearedTimeOfDay'] = df_all['appearedTimeOfDay'].apply(FunctionalToDay)\n",
    "\n",
    "\n",
    "def FunctionalToWeather(x):\n",
    "    if(x=='Clear'):\n",
    "        r = 0\n",
    "    elif(x=='PartlyCloudy'):\n",
    "        r = 1\n",
    "    elif(x=='Overcast'):\n",
    "        r = 2\n",
    "    elif(x=='MostlyCloudy'):\n",
    "        r = 3\n",
    "    elif(x=='PartlyCloudy'):\n",
    "        r = 4\n",
    "    elif(x=='BreezyandPartlyCloudy'):\n",
    "        r = 5\n",
    "    elif(x=='Dry'):\n",
    "        r = 6\n",
    "    elif(x=='LightRain'):\n",
    "        r = 7\n",
    "    elif(x=='Rain'):\n",
    "        r = 8\n",
    "    else:\n",
    "        r = 9\n",
    "    return r\n",
    "\n",
    "df_all['weather'] = df_all['weather'].apply(FunctionalToWeather)\n",
    "\n",
    "\n",
    "\n",
    "def FunctionalToDay(x):\n",
    "    if(x=='morning'):\n",
    "        r = 0\n",
    "    elif(x=='afternoon'):\n",
    "        r = 1\n",
    "    elif(x=='evening'):\n",
    "        r = 2\n",
    "    elif(x=='night'):\n",
    "        r = 3\n",
    "    else:\n",
    "        r = 4\n",
    "    return r\n",
    "\n",
    "df_all['weatherIcon'] = df_all['weather'].apply(FunctionalToWeather)\n",
    "\n",
    "\n",
    "def FunctionalToClss(x):\n",
    "    #print(x)\n",
    "    if(x=='0.0'):\n",
    "        r = '0'\n",
    "    elif(x=='1.0'):\n",
    "        r = '1'\n",
    "    elif(x=='2.0'):\n",
    "        r = '2'\n",
    "    elif(x=='3.0'):\n",
    "        r = '3'\n",
    "    elif(x=='4.0'):\n",
    "        r = '4'\n",
    "    elif(x=='5.0'):\n",
    "        r = '5'\n",
    "    elif(x=='6.0'):\n",
    "        r = '6'\n",
    "    else:\n",
    "        r = '7'\n",
    "    return r\n",
    "\n",
    "#df_all['class'] = df_all['class'].apply(FunctionalToClss)\n",
    "\n",
    "def FunctionalToLocation(x):\n",
    "    if(x=='Asia'):\n",
    "        r = 0\n",
    "    elif(x=='America'):\n",
    "        r = 1\n",
    "    elif(x=='America/Argentina'):\n",
    "        r = 2\n",
    "    elif(x=='Europe'):\n",
    "        r = 3\n",
    "    elif(x=='Pacific'):\n",
    "        r = 4\n",
    "    elif(x=='Africa'):\n",
    "        r = 5\n",
    "    else:\n",
    "        r = 6\n",
    "    return r\n",
    "\n",
    "df_all['continent'] = df_all['continent'].apply(FunctionalToLocation)\n",
    "\n",
    "df_all.drop(['population_density'], axis=1, inplace=True)\n",
    "\n",
    "df_all.drop(['pressure'], axis=1, inplace=True)\n",
    "\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "MultiIndex: 8951 entries, (train, 0) to (test, 1790)\n",
      "Columns: 182 entries, appearedHour to windSpeed\n",
      "dtypes: float64(4), int64(175), object(3)\n",
      "memory usage: 12.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df_all.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一些右偏分佈的 feature，可透過取 log 將其轉為常態分佈\n",
    "#df_all['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>appearedHour</th>\n",
       "      <th>appearedMinute</th>\n",
       "      <th>appearedTimeOfDay</th>\n",
       "      <th>closeToWater</th>\n",
       "      <th>continent</th>\n",
       "      <th>cooc_1</th>\n",
       "      <th>cooc_10</th>\n",
       "      <th>cooc_100</th>\n",
       "      <th>cooc_101</th>\n",
       "      <th>cooc_102</th>\n",
       "      <th>...</th>\n",
       "      <th>city_Tokyo</th>\n",
       "      <th>city_Toronto</th>\n",
       "      <th>city_Tripoli</th>\n",
       "      <th>city_Tunis</th>\n",
       "      <th>city_Vancouver</th>\n",
       "      <th>city_Vienna</th>\n",
       "      <th>city_Vilnius</th>\n",
       "      <th>city_Warsaw</th>\n",
       "      <th>city_Zagreb</th>\n",
       "      <th>city_Zurich</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">train</th>\n",
       "      <th>0</th>\n",
       "      <td>19</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19</td>\n",
       "      <td>46</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 254 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         appearedHour  appearedMinute  appearedTimeOfDay  closeToWater  \\\n",
       "train 0            19              10                  2             0   \n",
       "      1             5              19                  3             1   \n",
       "      2            19              46                  2             1   \n",
       "      3            11              10                  0             1   \n",
       "      4            18              32                  2             1   \n",
       "\n",
       "         continent  cooc_1  cooc_10  cooc_100  cooc_101  cooc_102  \\\n",
       "train 0          0       0        1         0         0         0   \n",
       "      1          1       0        0         0         0         0   \n",
       "      2          1       0        0         0         0         0   \n",
       "      3          6       0        0         0         0         0   \n",
       "      4          1       0        0         0         0         0   \n",
       "\n",
       "            ...       city_Tokyo  city_Toronto  city_Tripoli  city_Tunis  \\\n",
       "train 0     ...                0             0             0           0   \n",
       "      1     ...                0             0             0           0   \n",
       "      2     ...                0             0             0           0   \n",
       "      3     ...                0             0             0           0   \n",
       "      4     ...                0             0             0           0   \n",
       "\n",
       "         city_Vancouver  city_Vienna  city_Vilnius  city_Warsaw  city_Zagreb  \\\n",
       "train 0               0            0             0            0            0   \n",
       "      1               0            0             0            0            0   \n",
       "      2               0            0             0            0            0   \n",
       "      3               0            0             0            0            0   \n",
       "      4               0            0             0            0            0   \n",
       "\n",
       "         city_Zurich  \n",
       "train 0            0  \n",
       "      1            0  \n",
       "      2            0  \n",
       "      3            0  \n",
       "      4            0  \n",
       "\n",
       "[5 rows x 254 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.drop(['id'], axis=1, inplace=True)\n",
    "df_all.drop(['class'], axis=1, inplace=True)\n",
    "df_all_dum = pd.get_dummies(df_all); df_all_dum.head()\n",
    "df_all_dum.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>appearedHour</th>\n",
       "      <th>appearedMinute</th>\n",
       "      <th>appearedTimeOfDay</th>\n",
       "      <th>closeToWater</th>\n",
       "      <th>continent</th>\n",
       "      <th>cooc_1</th>\n",
       "      <th>cooc_10</th>\n",
       "      <th>cooc_100</th>\n",
       "      <th>cooc_101</th>\n",
       "      <th>cooc_102</th>\n",
       "      <th>...</th>\n",
       "      <th>city_Tokyo</th>\n",
       "      <th>city_Toronto</th>\n",
       "      <th>city_Tripoli</th>\n",
       "      <th>city_Tunis</th>\n",
       "      <th>city_Vancouver</th>\n",
       "      <th>city_Vienna</th>\n",
       "      <th>city_Vilnius</th>\n",
       "      <th>city_Warsaw</th>\n",
       "      <th>city_Zagreb</th>\n",
       "      <th>city_Zurich</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">train</th>\n",
       "      <th>0</th>\n",
       "      <td>19</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19</td>\n",
       "      <td>46</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 254 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         appearedHour  appearedMinute  appearedTimeOfDay  closeToWater  \\\n",
       "train 0            19              10                  2             0   \n",
       "      1             5              19                  3             1   \n",
       "      2            19              46                  2             1   \n",
       "      3            11              10                  0             1   \n",
       "      4            18              32                  2             1   \n",
       "\n",
       "         continent  cooc_1  cooc_10  cooc_100  cooc_101  cooc_102  \\\n",
       "train 0          0       0        1         0         0         0   \n",
       "      1          1       0        0         0         0         0   \n",
       "      2          1       0        0         0         0         0   \n",
       "      3          6       0        0         0         0         0   \n",
       "      4          1       0        0         0         0         0   \n",
       "\n",
       "            ...       city_Tokyo  city_Toronto  city_Tripoli  city_Tunis  \\\n",
       "train 0     ...                0             0             0           0   \n",
       "      1     ...                0             0             0           0   \n",
       "      2     ...                0             0             0           0   \n",
       "      3     ...                0             0             0           0   \n",
       "      4     ...                0             0             0           0   \n",
       "\n",
       "         city_Vancouver  city_Vienna  city_Vilnius  city_Warsaw  city_Zagreb  \\\n",
       "train 0               0            0             0            0            0   \n",
       "      1               0            0             0            0            0   \n",
       "      2               0            0             0            0            0   \n",
       "      3               0            0             0            0            0   \n",
       "      4               0            0             0            0            0   \n",
       "\n",
       "         city_Zurich  \n",
       "train 0            0  \n",
       "      1            0  \n",
       "      2            0  \n",
       "      3            0  \n",
       "      4            0  \n",
       "\n",
       "[5 rows x 254 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# min-max normalization\n",
    "#x_ = x_ / x_.max() \n",
    "# 將NA的值填平均值\n",
    "df_all_dum = df_all_dum.fillna( df_all_dum.mean() ); df_all_dum.head()\n",
    "# one hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import rcParams\n",
    "#x_ = df_all_dum[:train.shape[0]].values\n",
    "#y_ = train['class'].values\n",
    "\n",
    "\n",
    "#X_test = df_all_dum[ train.shape[0]: ]\n",
    "\n",
    "X_train_temp=df_all_dum[:train.shape[0]]\n",
    "\n",
    "\n",
    "\n",
    "X = X_train_temp.values\n",
    "y = train['class'].values\n",
    "#把train data與test分開\n",
    "#X, X_val, y, y_val = train_test_split(X, y, test_size = 0.1, random_state = 40)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train['class'].isna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# setting hyperparameter\n",
    "batch_size = 64\n",
    "epochs = 1000\n",
    "#lr = 0.8\n",
    "train_ratio = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7160, 254)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# min-max normalization\n",
    "X = X / X.max() \n",
    "#f_all_dum.dtypes\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test = df_all_dum[ train.shape[0]: ]\n",
    "#y_one_hot=y_\n",
    "\n",
    "y_one_hot = np.zeros((len(y), 10))  \n",
    "y_one_hot[np.arange(len(y)), y] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7160, 10)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_one_hot.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Split your data into training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y_one_hot, \n",
    "                                                    test_size=0.05, \n",
    "                                                    stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Dropout, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 64)                16320     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 16,970\n",
      "Trainable params: 16,970\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(Dense(64, activation='relu',  input_shape=(254,)))\n",
    "#model.add(Dropout(0.5))\n",
    "#model.add(Dense(64, activation='relu', input_dim=20))\n",
    "#model.add(Dropout(0.5))\n",
    "#model.add(Dense(64, activation='relu'))\n",
    "#model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax')) \n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=tf.keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6121 samples, validate on 681 samples\n",
      "Epoch 1/1000\n",
      "6121/6121 [==============================] - 1s 98us/step - loss: 2.1745 - acc: 0.2090 - val_loss: 2.0182 - val_acc: 0.1762\n",
      "Epoch 2/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.8826 - acc: 0.2158 - val_loss: 1.8378 - val_acc: 0.2261\n",
      "Epoch 3/1000\n",
      "6121/6121 [==============================] - 0s 33us/step - loss: 1.7923 - acc: 0.2246 - val_loss: 1.8037 - val_acc: 0.2261\n",
      "Epoch 4/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.7731 - acc: 0.2246 - val_loss: 1.7927 - val_acc: 0.2261\n",
      "Epoch 5/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.7654 - acc: 0.2248 - val_loss: 1.7852 - val_acc: 0.2261\n",
      "Epoch 6/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.7619 - acc: 0.2255 - val_loss: 1.7840 - val_acc: 0.2746\n",
      "Epoch 7/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.7601 - acc: 0.2263 - val_loss: 1.7829 - val_acc: 0.2261\n",
      "Epoch 8/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.7582 - acc: 0.2248 - val_loss: 1.7815 - val_acc: 0.2261\n",
      "Epoch 9/1000\n",
      "6121/6121 [==============================] - 0s 33us/step - loss: 1.7575 - acc: 0.2248 - val_loss: 1.7786 - val_acc: 0.2261\n",
      "Epoch 10/1000\n",
      "6121/6121 [==============================] - 0s 33us/step - loss: 1.7566 - acc: 0.2248 - val_loss: 1.7776 - val_acc: 0.2261\n",
      "Epoch 11/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.7564 - acc: 0.2222 - val_loss: 1.7798 - val_acc: 0.2261\n",
      "Epoch 12/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.7557 - acc: 0.2253 - val_loss: 1.7753 - val_acc: 0.2261\n",
      "Epoch 13/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.7553 - acc: 0.2258 - val_loss: 1.7746 - val_acc: 0.2261\n",
      "Epoch 14/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.7552 - acc: 0.2248 - val_loss: 1.7767 - val_acc: 0.2261\n",
      "Epoch 15/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.7548 - acc: 0.2289 - val_loss: 1.7762 - val_acc: 0.2261\n",
      "Epoch 16/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.7538 - acc: 0.2385 - val_loss: 1.7724 - val_acc: 0.2261\n",
      "Epoch 17/1000\n",
      "6121/6121 [==============================] - 0s 33us/step - loss: 1.7540 - acc: 0.2349 - val_loss: 1.7770 - val_acc: 0.2731\n",
      "Epoch 18/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.7535 - acc: 0.2286 - val_loss: 1.7737 - val_acc: 0.2261\n",
      "Epoch 19/1000\n",
      "6121/6121 [==============================] - 0s 34us/step - loss: 1.7536 - acc: 0.2315 - val_loss: 1.7782 - val_acc: 0.1777\n",
      "Epoch 20/1000\n",
      "6121/6121 [==============================] - 0s 33us/step - loss: 1.7529 - acc: 0.2317 - val_loss: 1.7738 - val_acc: 0.2261\n",
      "Epoch 21/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.7522 - acc: 0.2354 - val_loss: 1.7708 - val_acc: 0.2261\n",
      "Epoch 22/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.7521 - acc: 0.2331 - val_loss: 1.7731 - val_acc: 0.2261\n",
      "Epoch 23/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.7510 - acc: 0.2369 - val_loss: 1.7728 - val_acc: 0.2261\n",
      "Epoch 24/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.7508 - acc: 0.2421 - val_loss: 1.7695 - val_acc: 0.2261\n",
      "Epoch 25/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.7507 - acc: 0.2449 - val_loss: 1.7713 - val_acc: 0.2261\n",
      "Epoch 26/1000\n",
      "6121/6121 [==============================] - 0s 33us/step - loss: 1.7499 - acc: 0.2357 - val_loss: 1.7720 - val_acc: 0.2628\n",
      "Epoch 27/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.7498 - acc: 0.2331 - val_loss: 1.7680 - val_acc: 0.2261\n",
      "Epoch 28/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.7484 - acc: 0.2446 - val_loss: 1.7702 - val_acc: 0.2261\n",
      "Epoch 29/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.7481 - acc: 0.2415 - val_loss: 1.7711 - val_acc: 0.2276\n",
      "Epoch 30/1000\n",
      "6121/6121 [==============================] - 0s 33us/step - loss: 1.7475 - acc: 0.2504 - val_loss: 1.7683 - val_acc: 0.2261\n",
      "Epoch 31/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.7466 - acc: 0.2472 - val_loss: 1.7666 - val_acc: 0.2261\n",
      "Epoch 32/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.7464 - acc: 0.2522 - val_loss: 1.7671 - val_acc: 0.2261\n",
      "Epoch 33/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.7459 - acc: 0.2460 - val_loss: 1.7628 - val_acc: 0.2261\n",
      "Epoch 34/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.7446 - acc: 0.2446 - val_loss: 1.7627 - val_acc: 0.2276\n",
      "Epoch 35/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.7447 - acc: 0.2424 - val_loss: 1.7648 - val_acc: 0.2276\n",
      "Epoch 36/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.7427 - acc: 0.2617 - val_loss: 1.7609 - val_acc: 0.2261\n",
      "Epoch 37/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.7424 - acc: 0.2500 - val_loss: 1.7642 - val_acc: 0.2717\n",
      "Epoch 38/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.7414 - acc: 0.2583 - val_loss: 1.7616 - val_acc: 0.2335\n",
      "Epoch 39/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.7401 - acc: 0.2488 - val_loss: 1.7625 - val_acc: 0.2628\n",
      "Epoch 40/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.7397 - acc: 0.2678 - val_loss: 1.7611 - val_acc: 0.2584\n",
      "Epoch 41/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.7388 - acc: 0.2487 - val_loss: 1.7608 - val_acc: 0.2570\n",
      "Epoch 42/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.7381 - acc: 0.2603 - val_loss: 1.7611 - val_acc: 0.2907\n",
      "Epoch 43/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.7372 - acc: 0.2712 - val_loss: 1.7574 - val_acc: 0.2790\n",
      "Epoch 44/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.7357 - acc: 0.2754 - val_loss: 1.7558 - val_acc: 0.2276\n",
      "Epoch 45/1000\n",
      "6121/6121 [==============================] - 0s 33us/step - loss: 1.7346 - acc: 0.2524 - val_loss: 1.7550 - val_acc: 0.3069\n",
      "Epoch 46/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.7337 - acc: 0.2740 - val_loss: 1.7552 - val_acc: 0.2834\n",
      "Epoch 47/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.7327 - acc: 0.2679 - val_loss: 1.7513 - val_acc: 0.3069\n",
      "Epoch 48/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.7311 - acc: 0.2872 - val_loss: 1.7485 - val_acc: 0.2276\n",
      "Epoch 49/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.7303 - acc: 0.2620 - val_loss: 1.7482 - val_acc: 0.3040\n",
      "Epoch 50/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.7287 - acc: 0.2705 - val_loss: 1.7493 - val_acc: 0.3025\n",
      "Epoch 51/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.7274 - acc: 0.2879 - val_loss: 1.7439 - val_acc: 0.2717\n",
      "Epoch 52/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.7259 - acc: 0.2715 - val_loss: 1.7467 - val_acc: 0.2687\n",
      "Epoch 53/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.7244 - acc: 0.2754 - val_loss: 1.7424 - val_acc: 0.2819\n",
      "Epoch 54/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.7238 - acc: 0.2717 - val_loss: 1.7434 - val_acc: 0.2981\n",
      "Epoch 55/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.7221 - acc: 0.2838 - val_loss: 1.7406 - val_acc: 0.2907\n",
      "Epoch 56/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.7206 - acc: 0.2802 - val_loss: 1.7402 - val_acc: 0.2981\n",
      "Epoch 57/1000\n",
      "6121/6121 [==============================] - 0s 33us/step - loss: 1.7189 - acc: 0.2867 - val_loss: 1.7361 - val_acc: 0.2907\n",
      "Epoch 58/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.7179 - acc: 0.2802 - val_loss: 1.7356 - val_acc: 0.2291\n",
      "Epoch 59/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.7163 - acc: 0.2748 - val_loss: 1.7348 - val_acc: 0.3040\n",
      "Epoch 60/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.7151 - acc: 0.2872 - val_loss: 1.7339 - val_acc: 0.2438\n",
      "Epoch 61/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.7137 - acc: 0.2835 - val_loss: 1.7290 - val_acc: 0.2922\n",
      "Epoch 62/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.7118 - acc: 0.2823 - val_loss: 1.7290 - val_acc: 0.2907\n",
      "Epoch 63/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.7101 - acc: 0.2851 - val_loss: 1.7270 - val_acc: 0.2819\n",
      "Epoch 64/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.7083 - acc: 0.2852 - val_loss: 1.7238 - val_acc: 0.2937\n",
      "Epoch 65/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.7066 - acc: 0.2890 - val_loss: 1.7234 - val_acc: 0.2937\n",
      "Epoch 66/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.7049 - acc: 0.2921 - val_loss: 1.7192 - val_acc: 0.2893\n",
      "Epoch 67/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.7030 - acc: 0.2846 - val_loss: 1.7198 - val_acc: 0.2731\n",
      "Epoch 68/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.7017 - acc: 0.2884 - val_loss: 1.7171 - val_acc: 0.2996\n",
      "Epoch 69/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.7006 - acc: 0.2870 - val_loss: 1.7133 - val_acc: 0.2937\n",
      "Epoch 70/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.6985 - acc: 0.2897 - val_loss: 1.7141 - val_acc: 0.2937\n",
      "Epoch 71/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.6962 - acc: 0.2885 - val_loss: 1.7095 - val_acc: 0.2937\n",
      "Epoch 72/1000\n",
      "6121/6121 [==============================] - 0s 33us/step - loss: 1.6946 - acc: 0.2906 - val_loss: 1.7095 - val_acc: 0.2952\n",
      "Epoch 73/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.6923 - acc: 0.2910 - val_loss: 1.7115 - val_acc: 0.2966\n",
      "Epoch 74/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.6914 - acc: 0.2942 - val_loss: 1.7059 - val_acc: 0.2952\n",
      "Epoch 75/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.6890 - acc: 0.2960 - val_loss: 1.7038 - val_acc: 0.2849\n",
      "Epoch 76/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.6874 - acc: 0.2911 - val_loss: 1.7034 - val_acc: 0.2643\n",
      "Epoch 77/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.6851 - acc: 0.2941 - val_loss: 1.6999 - val_acc: 0.2834\n",
      "Epoch 78/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.6833 - acc: 0.2929 - val_loss: 1.6964 - val_acc: 0.2937\n",
      "Epoch 79/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.6813 - acc: 0.2990 - val_loss: 1.6963 - val_acc: 0.2996\n",
      "Epoch 80/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.6791 - acc: 0.2949 - val_loss: 1.6912 - val_acc: 0.2922\n",
      "Epoch 81/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.6774 - acc: 0.2962 - val_loss: 1.6880 - val_acc: 0.2966\n",
      "Epoch 82/1000\n",
      "6121/6121 [==============================] - 0s 34us/step - loss: 1.6757 - acc: 0.2965 - val_loss: 1.6856 - val_acc: 0.3172\n",
      "Epoch 83/1000\n",
      "6121/6121 [==============================] - 0s 34us/step - loss: 1.6739 - acc: 0.2973 - val_loss: 1.6859 - val_acc: 0.3069\n",
      "Epoch 84/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.6719 - acc: 0.3014 - val_loss: 1.6834 - val_acc: 0.2981\n",
      "Epoch 85/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.6690 - acc: 0.2967 - val_loss: 1.6797 - val_acc: 0.3231\n",
      "Epoch 86/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.6680 - acc: 0.3017 - val_loss: 1.6770 - val_acc: 0.2966\n",
      "Epoch 87/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.6657 - acc: 0.2975 - val_loss: 1.6743 - val_acc: 0.2966\n",
      "Epoch 88/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.6638 - acc: 0.3034 - val_loss: 1.6720 - val_acc: 0.2966\n",
      "Epoch 89/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.6620 - acc: 0.2957 - val_loss: 1.6712 - val_acc: 0.2966\n",
      "Epoch 90/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.6602 - acc: 0.3021 - val_loss: 1.6713 - val_acc: 0.2775\n",
      "Epoch 91/1000\n",
      "6121/6121 [==============================] - 0s 33us/step - loss: 1.6586 - acc: 0.3037 - val_loss: 1.6641 - val_acc: 0.2996\n",
      "Epoch 92/1000\n",
      "6121/6121 [==============================] - 0s 33us/step - loss: 1.6560 - acc: 0.3044 - val_loss: 1.6658 - val_acc: 0.2966\n",
      "Epoch 93/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.6545 - acc: 0.2985 - val_loss: 1.6645 - val_acc: 0.3172\n",
      "Epoch 94/1000\n",
      "6121/6121 [==============================] - 0s 30us/step - loss: 1.6528 - acc: 0.3000 - val_loss: 1.6621 - val_acc: 0.2996\n",
      "Epoch 95/1000\n",
      "6121/6121 [==============================] - 0s 33us/step - loss: 1.6510 - acc: 0.3014 - val_loss: 1.6610 - val_acc: 0.2952\n",
      "Epoch 96/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.6485 - acc: 0.3065 - val_loss: 1.6530 - val_acc: 0.2981\n",
      "Epoch 97/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.6469 - acc: 0.3070 - val_loss: 1.6549 - val_acc: 0.3010\n",
      "Epoch 98/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.6447 - acc: 0.3107 - val_loss: 1.6487 - val_acc: 0.3054\n",
      "Epoch 99/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.6431 - acc: 0.3075 - val_loss: 1.6510 - val_acc: 0.2966\n",
      "Epoch 100/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.6428 - acc: 0.3112 - val_loss: 1.6473 - val_acc: 0.2981\n",
      "Epoch 101/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.6399 - acc: 0.3034 - val_loss: 1.6442 - val_acc: 0.2966\n",
      "Epoch 102/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.6378 - acc: 0.3058 - val_loss: 1.6418 - val_acc: 0.3157\n",
      "Epoch 103/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.6366 - acc: 0.3102 - val_loss: 1.6394 - val_acc: 0.3084\n",
      "Epoch 104/1000\n",
      "6121/6121 [==============================] - 0s 33us/step - loss: 1.6341 - acc: 0.3129 - val_loss: 1.6404 - val_acc: 0.2981\n",
      "Epoch 105/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.6323 - acc: 0.3171 - val_loss: 1.6341 - val_acc: 0.3069\n",
      "Epoch 106/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.6302 - acc: 0.3138 - val_loss: 1.6357 - val_acc: 0.3319\n",
      "Epoch 107/1000\n",
      "6121/6121 [==============================] - 0s 33us/step - loss: 1.6284 - acc: 0.3161 - val_loss: 1.6358 - val_acc: 0.3304\n",
      "Epoch 108/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.6273 - acc: 0.3236 - val_loss: 1.6314 - val_acc: 0.3040\n",
      "Epoch 109/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.6254 - acc: 0.3174 - val_loss: 1.6262 - val_acc: 0.3319\n",
      "Epoch 110/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.6236 - acc: 0.3181 - val_loss: 1.6242 - val_acc: 0.3128\n",
      "Epoch 111/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.6221 - acc: 0.3168 - val_loss: 1.6258 - val_acc: 0.3172\n",
      "Epoch 112/1000\n",
      "6121/6121 [==============================] - 0s 30us/step - loss: 1.6207 - acc: 0.3290 - val_loss: 1.6198 - val_acc: 0.3040\n",
      "Epoch 113/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.6177 - acc: 0.3228 - val_loss: 1.6181 - val_acc: 0.3201\n",
      "Epoch 114/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.6168 - acc: 0.3192 - val_loss: 1.6160 - val_acc: 0.3084\n",
      "Epoch 115/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.6150 - acc: 0.3215 - val_loss: 1.6162 - val_acc: 0.3348\n",
      "Epoch 116/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.6125 - acc: 0.3254 - val_loss: 1.6179 - val_acc: 0.3142\n",
      "Epoch 117/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.6115 - acc: 0.3338 - val_loss: 1.6133 - val_acc: 0.3142\n",
      "Epoch 118/1000\n",
      "6121/6121 [==============================] - 0s 33us/step - loss: 1.6094 - acc: 0.3256 - val_loss: 1.6093 - val_acc: 0.3333\n",
      "Epoch 119/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.6078 - acc: 0.3271 - val_loss: 1.6118 - val_acc: 0.3436\n",
      "Epoch 120/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.6067 - acc: 0.3277 - val_loss: 1.6055 - val_acc: 0.3304\n",
      "Epoch 121/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.6061 - acc: 0.3334 - val_loss: 1.6059 - val_acc: 0.3186\n",
      "Epoch 122/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.6032 - acc: 0.3297 - val_loss: 1.6095 - val_acc: 0.3495\n",
      "Epoch 123/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.6017 - acc: 0.3308 - val_loss: 1.5992 - val_acc: 0.3451\n",
      "Epoch 124/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.6002 - acc: 0.3339 - val_loss: 1.6030 - val_acc: 0.3260\n",
      "Epoch 125/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.5984 - acc: 0.3387 - val_loss: 1.5935 - val_acc: 0.3495\n",
      "Epoch 126/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.5980 - acc: 0.3377 - val_loss: 1.5933 - val_acc: 0.3465\n",
      "Epoch 127/1000\n",
      "6121/6121 [==============================] - 0s 30us/step - loss: 1.5951 - acc: 0.3328 - val_loss: 1.5950 - val_acc: 0.3421\n",
      "Epoch 128/1000\n",
      "6121/6121 [==============================] - 0s 33us/step - loss: 1.5937 - acc: 0.3361 - val_loss: 1.5918 - val_acc: 0.3465\n",
      "Epoch 129/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.5931 - acc: 0.3437 - val_loss: 1.5904 - val_acc: 0.3304\n",
      "Epoch 130/1000\n",
      "6121/6121 [==============================] - 0s 30us/step - loss: 1.5907 - acc: 0.3429 - val_loss: 1.5911 - val_acc: 0.3363\n",
      "Epoch 131/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.5887 - acc: 0.3462 - val_loss: 1.5883 - val_acc: 0.3216\n",
      "Epoch 132/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.5875 - acc: 0.3416 - val_loss: 1.5872 - val_acc: 0.3539\n",
      "Epoch 133/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.5862 - acc: 0.3491 - val_loss: 1.5844 - val_acc: 0.3304\n",
      "Epoch 134/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.5854 - acc: 0.3477 - val_loss: 1.5845 - val_acc: 0.3348\n",
      "Epoch 135/1000\n",
      "6121/6121 [==============================] - 0s 30us/step - loss: 1.5833 - acc: 0.3535 - val_loss: 1.5778 - val_acc: 0.3510\n",
      "Epoch 136/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.5815 - acc: 0.3490 - val_loss: 1.5788 - val_acc: 0.3627\n",
      "Epoch 137/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.5811 - acc: 0.3555 - val_loss: 1.5778 - val_acc: 0.3465\n",
      "Epoch 138/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.5783 - acc: 0.3547 - val_loss: 1.5768 - val_acc: 0.3480\n",
      "Epoch 139/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.5766 - acc: 0.3571 - val_loss: 1.5790 - val_acc: 0.3289\n",
      "Epoch 140/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.5753 - acc: 0.3517 - val_loss: 1.5748 - val_acc: 0.3583\n",
      "Epoch 141/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.5738 - acc: 0.3524 - val_loss: 1.5693 - val_acc: 0.3744\n",
      "Epoch 142/1000\n",
      "6121/6121 [==============================] - 0s 30us/step - loss: 1.5724 - acc: 0.3604 - val_loss: 1.5720 - val_acc: 0.3524\n",
      "Epoch 143/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.5718 - acc: 0.3521 - val_loss: 1.5677 - val_acc: 0.3510\n",
      "Epoch 144/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.5696 - acc: 0.3596 - val_loss: 1.5712 - val_acc: 0.3465\n",
      "Epoch 145/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.5681 - acc: 0.3627 - val_loss: 1.5731 - val_acc: 0.3583\n",
      "Epoch 146/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.5683 - acc: 0.3622 - val_loss: 1.5683 - val_acc: 0.3583\n",
      "Epoch 147/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.5664 - acc: 0.3607 - val_loss: 1.5652 - val_acc: 0.3700\n",
      "Epoch 148/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.5646 - acc: 0.3673 - val_loss: 1.5709 - val_acc: 0.3348\n",
      "Epoch 149/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.5630 - acc: 0.3650 - val_loss: 1.5596 - val_acc: 0.3656\n",
      "Epoch 150/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.5625 - acc: 0.3678 - val_loss: 1.5573 - val_acc: 0.3700\n",
      "Epoch 151/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.5607 - acc: 0.3642 - val_loss: 1.5597 - val_acc: 0.3656\n",
      "Epoch 152/1000\n",
      "6121/6121 [==============================] - 0s 33us/step - loss: 1.5600 - acc: 0.3669 - val_loss: 1.5546 - val_acc: 0.3524\n",
      "Epoch 153/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.5577 - acc: 0.3694 - val_loss: 1.5508 - val_acc: 0.3818\n",
      "Epoch 154/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.5569 - acc: 0.3678 - val_loss: 1.5512 - val_acc: 0.3833\n",
      "Epoch 155/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.5554 - acc: 0.3668 - val_loss: 1.5526 - val_acc: 0.3686\n",
      "Epoch 156/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.5538 - acc: 0.3746 - val_loss: 1.5562 - val_acc: 0.3656\n",
      "Epoch 157/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.5542 - acc: 0.3707 - val_loss: 1.5478 - val_acc: 0.3847\n",
      "Epoch 158/1000\n",
      "6121/6121 [==============================] - 0s 33us/step - loss: 1.5507 - acc: 0.3749 - val_loss: 1.5446 - val_acc: 0.3715\n",
      "Epoch 159/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.5494 - acc: 0.3802 - val_loss: 1.5515 - val_acc: 0.3480\n",
      "Epoch 160/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.5488 - acc: 0.3718 - val_loss: 1.5434 - val_acc: 0.3759\n",
      "Epoch 161/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.5469 - acc: 0.3787 - val_loss: 1.5434 - val_acc: 0.3715\n",
      "Epoch 162/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.5459 - acc: 0.3728 - val_loss: 1.5400 - val_acc: 0.3833\n",
      "Epoch 163/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.5448 - acc: 0.3795 - val_loss: 1.5388 - val_acc: 0.3789\n",
      "Epoch 164/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.5428 - acc: 0.3797 - val_loss: 1.5390 - val_acc: 0.3730\n",
      "Epoch 165/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.5417 - acc: 0.3813 - val_loss: 1.5386 - val_acc: 0.3744\n",
      "Epoch 166/1000\n",
      "6121/6121 [==============================] - 0s 33us/step - loss: 1.5415 - acc: 0.3802 - val_loss: 1.5389 - val_acc: 0.3656\n",
      "Epoch 167/1000\n",
      "6121/6121 [==============================] - 0s 30us/step - loss: 1.5400 - acc: 0.3844 - val_loss: 1.5334 - val_acc: 0.3686\n",
      "Epoch 168/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.5395 - acc: 0.3838 - val_loss: 1.5367 - val_acc: 0.3759\n",
      "Epoch 169/1000\n",
      "6121/6121 [==============================] - 0s 30us/step - loss: 1.5375 - acc: 0.3849 - val_loss: 1.5315 - val_acc: 0.3730\n",
      "Epoch 170/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.5365 - acc: 0.3761 - val_loss: 1.5334 - val_acc: 0.3671\n",
      "Epoch 171/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.5352 - acc: 0.3841 - val_loss: 1.5301 - val_acc: 0.3833\n",
      "Epoch 172/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.5333 - acc: 0.3882 - val_loss: 1.5247 - val_acc: 0.3862\n",
      "Epoch 173/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.5328 - acc: 0.3856 - val_loss: 1.5281 - val_acc: 0.3744\n",
      "Epoch 174/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.5311 - acc: 0.3857 - val_loss: 1.5331 - val_acc: 0.3789\n",
      "Epoch 175/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.5300 - acc: 0.3911 - val_loss: 1.5229 - val_acc: 0.3627\n",
      "Epoch 176/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.5287 - acc: 0.3865 - val_loss: 1.5266 - val_acc: 0.3862\n",
      "Epoch 177/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.5290 - acc: 0.3908 - val_loss: 1.5250 - val_acc: 0.3759\n",
      "Epoch 178/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.5254 - acc: 0.3885 - val_loss: 1.5175 - val_acc: 0.3891\n",
      "Epoch 179/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.5258 - acc: 0.3929 - val_loss: 1.5190 - val_acc: 0.3891\n",
      "Epoch 180/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.5244 - acc: 0.3923 - val_loss: 1.5155 - val_acc: 0.3789\n",
      "Epoch 181/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.5231 - acc: 0.3999 - val_loss: 1.5207 - val_acc: 0.3730\n",
      "Epoch 182/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.5210 - acc: 0.3913 - val_loss: 1.5166 - val_acc: 0.3818\n",
      "Epoch 183/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.5199 - acc: 0.3909 - val_loss: 1.5183 - val_acc: 0.3921\n",
      "Epoch 184/1000\n",
      "6121/6121 [==============================] - 0s 30us/step - loss: 1.5190 - acc: 0.3980 - val_loss: 1.5172 - val_acc: 0.3965\n",
      "Epoch 185/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.5179 - acc: 0.3993 - val_loss: 1.5149 - val_acc: 0.3862\n",
      "Epoch 186/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.5163 - acc: 0.3947 - val_loss: 1.5147 - val_acc: 0.3833\n",
      "Epoch 187/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.5153 - acc: 0.3988 - val_loss: 1.5162 - val_acc: 0.3891\n",
      "Epoch 188/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.5137 - acc: 0.3999 - val_loss: 1.5063 - val_acc: 0.3921\n",
      "Epoch 189/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.5130 - acc: 0.3976 - val_loss: 1.5090 - val_acc: 0.3700\n",
      "Epoch 190/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.5131 - acc: 0.3914 - val_loss: 1.5145 - val_acc: 0.3847\n",
      "Epoch 191/1000\n",
      "6121/6121 [==============================] - 0s 30us/step - loss: 1.5118 - acc: 0.3983 - val_loss: 1.5068 - val_acc: 0.3833\n",
      "Epoch 192/1000\n",
      "6121/6121 [==============================] - 0s 33us/step - loss: 1.5104 - acc: 0.4039 - val_loss: 1.5068 - val_acc: 0.3877\n",
      "Epoch 193/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.5083 - acc: 0.4037 - val_loss: 1.5030 - val_acc: 0.3891\n",
      "Epoch 194/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.5079 - acc: 0.4042 - val_loss: 1.5019 - val_acc: 0.3965\n",
      "Epoch 195/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.5066 - acc: 0.4021 - val_loss: 1.5002 - val_acc: 0.3818\n",
      "Epoch 196/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.5055 - acc: 0.4086 - val_loss: 1.5008 - val_acc: 0.4009\n",
      "Epoch 197/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.5043 - acc: 0.4024 - val_loss: 1.5007 - val_acc: 0.3965\n",
      "Epoch 198/1000\n",
      "6121/6121 [==============================] - 0s 33us/step - loss: 1.5025 - acc: 0.4074 - val_loss: 1.4969 - val_acc: 0.3891\n",
      "Epoch 199/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.5012 - acc: 0.4027 - val_loss: 1.4963 - val_acc: 0.3891\n",
      "Epoch 200/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.5006 - acc: 0.4094 - val_loss: 1.4979 - val_acc: 0.3906\n",
      "Epoch 201/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.4996 - acc: 0.4052 - val_loss: 1.4971 - val_acc: 0.3994\n",
      "Epoch 202/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.4980 - acc: 0.4102 - val_loss: 1.4942 - val_acc: 0.3906\n",
      "Epoch 203/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.4983 - acc: 0.4022 - val_loss: 1.4909 - val_acc: 0.3994\n",
      "Epoch 204/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.4957 - acc: 0.4045 - val_loss: 1.5041 - val_acc: 0.3803\n",
      "Epoch 205/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.4953 - acc: 0.4122 - val_loss: 1.4891 - val_acc: 0.4068\n",
      "Epoch 206/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.4934 - acc: 0.4128 - val_loss: 1.4891 - val_acc: 0.4053\n",
      "Epoch 207/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.4936 - acc: 0.4128 - val_loss: 1.4881 - val_acc: 0.3906\n",
      "Epoch 208/1000\n",
      "6121/6121 [==============================] - 0s 33us/step - loss: 1.4917 - acc: 0.4146 - val_loss: 1.4861 - val_acc: 0.3877\n",
      "Epoch 209/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.4910 - acc: 0.4117 - val_loss: 1.4872 - val_acc: 0.3950\n",
      "Epoch 210/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.4898 - acc: 0.4122 - val_loss: 1.4905 - val_acc: 0.4009\n",
      "Epoch 211/1000\n",
      "6121/6121 [==============================] - 0s 33us/step - loss: 1.4889 - acc: 0.4143 - val_loss: 1.4896 - val_acc: 0.4097\n",
      "Epoch 212/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.4884 - acc: 0.4125 - val_loss: 1.4848 - val_acc: 0.3965\n",
      "Epoch 213/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.4869 - acc: 0.4106 - val_loss: 1.4861 - val_acc: 0.4009\n",
      "Epoch 214/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.4867 - acc: 0.4171 - val_loss: 1.4896 - val_acc: 0.3921\n",
      "Epoch 215/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.4841 - acc: 0.4148 - val_loss: 1.4822 - val_acc: 0.4009\n",
      "Epoch 216/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.4840 - acc: 0.4168 - val_loss: 1.4804 - val_acc: 0.3891\n",
      "Epoch 217/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.4826 - acc: 0.4159 - val_loss: 1.4785 - val_acc: 0.3994\n",
      "Epoch 218/1000\n",
      "6121/6121 [==============================] - 0s 30us/step - loss: 1.4818 - acc: 0.4184 - val_loss: 1.4790 - val_acc: 0.4038\n",
      "Epoch 219/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.4814 - acc: 0.4156 - val_loss: 1.4782 - val_acc: 0.3979\n",
      "Epoch 220/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.4793 - acc: 0.4248 - val_loss: 1.4804 - val_acc: 0.4038\n",
      "Epoch 221/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.4790 - acc: 0.4143 - val_loss: 1.4750 - val_acc: 0.3935\n",
      "Epoch 222/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.4768 - acc: 0.4226 - val_loss: 1.4728 - val_acc: 0.4023\n",
      "Epoch 223/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.4763 - acc: 0.4262 - val_loss: 1.4781 - val_acc: 0.4023\n",
      "Epoch 224/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.4747 - acc: 0.4205 - val_loss: 1.4719 - val_acc: 0.4009\n",
      "Epoch 225/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.4740 - acc: 0.4222 - val_loss: 1.4718 - val_acc: 0.4097\n",
      "Epoch 226/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.4731 - acc: 0.4213 - val_loss: 1.4720 - val_acc: 0.4126\n",
      "Epoch 227/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.4721 - acc: 0.4181 - val_loss: 1.4782 - val_acc: 0.4023\n",
      "Epoch 228/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.4709 - acc: 0.4282 - val_loss: 1.4735 - val_acc: 0.4097\n",
      "Epoch 229/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.4713 - acc: 0.4208 - val_loss: 1.4662 - val_acc: 0.4038\n",
      "Epoch 230/1000\n",
      "6121/6121 [==============================] - 0s 33us/step - loss: 1.4696 - acc: 0.4295 - val_loss: 1.4757 - val_acc: 0.4112\n",
      "Epoch 231/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.4681 - acc: 0.4305 - val_loss: 1.4651 - val_acc: 0.4009\n",
      "Epoch 232/1000\n",
      "6121/6121 [==============================] - 0s 30us/step - loss: 1.4681 - acc: 0.4292 - val_loss: 1.4639 - val_acc: 0.4068\n",
      "Epoch 233/1000\n",
      "6121/6121 [==============================] - 0s 30us/step - loss: 1.4663 - acc: 0.4297 - val_loss: 1.4666 - val_acc: 0.4112\n",
      "Epoch 234/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.4669 - acc: 0.4236 - val_loss: 1.4657 - val_acc: 0.4038\n",
      "Epoch 235/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.4643 - acc: 0.4262 - val_loss: 1.4645 - val_acc: 0.4068\n",
      "Epoch 236/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.4633 - acc: 0.4344 - val_loss: 1.4593 - val_acc: 0.4126\n",
      "Epoch 237/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.4624 - acc: 0.4240 - val_loss: 1.4666 - val_acc: 0.4126\n",
      "Epoch 238/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.4609 - acc: 0.4292 - val_loss: 1.4587 - val_acc: 0.4023\n",
      "Epoch 239/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.4598 - acc: 0.4287 - val_loss: 1.4582 - val_acc: 0.4156\n",
      "Epoch 240/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.4589 - acc: 0.4300 - val_loss: 1.4615 - val_acc: 0.4097\n",
      "Epoch 241/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.4587 - acc: 0.4295 - val_loss: 1.4545 - val_acc: 0.4112\n",
      "Epoch 242/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.4573 - acc: 0.4297 - val_loss: 1.4580 - val_acc: 0.4141\n",
      "Epoch 243/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.4572 - acc: 0.4279 - val_loss: 1.4550 - val_acc: 0.4068\n",
      "Epoch 244/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.4555 - acc: 0.4316 - val_loss: 1.4587 - val_acc: 0.4126\n",
      "Epoch 245/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.4539 - acc: 0.4331 - val_loss: 1.4588 - val_acc: 0.4097\n",
      "Epoch 246/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.4545 - acc: 0.4338 - val_loss: 1.4524 - val_acc: 0.4038\n",
      "Epoch 247/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.4522 - acc: 0.4346 - val_loss: 1.4560 - val_acc: 0.4112\n",
      "Epoch 248/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.4520 - acc: 0.4311 - val_loss: 1.4507 - val_acc: 0.4023\n",
      "Epoch 249/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.4520 - acc: 0.4405 - val_loss: 1.4484 - val_acc: 0.4156\n",
      "Epoch 250/1000\n",
      "6121/6121 [==============================] - 0s 33us/step - loss: 1.4503 - acc: 0.4364 - val_loss: 1.4525 - val_acc: 0.4053\n",
      "Epoch 251/1000\n",
      "6121/6121 [==============================] - 0s 33us/step - loss: 1.4489 - acc: 0.4373 - val_loss: 1.4497 - val_acc: 0.4097\n",
      "Epoch 252/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.4476 - acc: 0.4401 - val_loss: 1.4502 - val_acc: 0.4170\n",
      "Epoch 253/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.4461 - acc: 0.4360 - val_loss: 1.4482 - val_acc: 0.4156\n",
      "Epoch 254/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.4458 - acc: 0.4439 - val_loss: 1.4444 - val_acc: 0.4185\n",
      "Epoch 255/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.4454 - acc: 0.4373 - val_loss: 1.4435 - val_acc: 0.4214\n",
      "Epoch 256/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.4447 - acc: 0.4393 - val_loss: 1.4494 - val_acc: 0.4229\n",
      "Epoch 257/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.4419 - acc: 0.4437 - val_loss: 1.4451 - val_acc: 0.4141\n",
      "Epoch 258/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.4421 - acc: 0.4365 - val_loss: 1.4448 - val_acc: 0.4126\n",
      "Epoch 259/1000\n",
      "6121/6121 [==============================] - 0s 30us/step - loss: 1.4416 - acc: 0.4411 - val_loss: 1.4411 - val_acc: 0.4156\n",
      "Epoch 260/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.4403 - acc: 0.4416 - val_loss: 1.4412 - val_acc: 0.4126\n",
      "Epoch 261/1000\n",
      "6121/6121 [==============================] - 0s 33us/step - loss: 1.4387 - acc: 0.4362 - val_loss: 1.4400 - val_acc: 0.4170\n",
      "Epoch 262/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.4379 - acc: 0.4400 - val_loss: 1.4419 - val_acc: 0.4112\n",
      "Epoch 263/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.4378 - acc: 0.4450 - val_loss: 1.4436 - val_acc: 0.4244\n",
      "Epoch 264/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.4374 - acc: 0.4408 - val_loss: 1.4375 - val_acc: 0.4141\n",
      "Epoch 265/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.4361 - acc: 0.4408 - val_loss: 1.4397 - val_acc: 0.4244\n",
      "Epoch 266/1000\n",
      "6121/6121 [==============================] - 0s 34us/step - loss: 1.4349 - acc: 0.4457 - val_loss: 1.4431 - val_acc: 0.4229\n",
      "Epoch 267/1000\n",
      "6121/6121 [==============================] - 0s 35us/step - loss: 1.4340 - acc: 0.4439 - val_loss: 1.4438 - val_acc: 0.4200\n",
      "Epoch 268/1000\n",
      "6121/6121 [==============================] - 0s 34us/step - loss: 1.4321 - acc: 0.4468 - val_loss: 1.4375 - val_acc: 0.4185\n",
      "Epoch 269/1000\n",
      "6121/6121 [==============================] - 0s 35us/step - loss: 1.4322 - acc: 0.4431 - val_loss: 1.4350 - val_acc: 0.4097\n",
      "Epoch 270/1000\n",
      "6121/6121 [==============================] - 0s 36us/step - loss: 1.4314 - acc: 0.4494 - val_loss: 1.4336 - val_acc: 0.4214\n",
      "Epoch 271/1000\n",
      "6121/6121 [==============================] - 0s 33us/step - loss: 1.4311 - acc: 0.4457 - val_loss: 1.4342 - val_acc: 0.4185\n",
      "Epoch 272/1000\n",
      "6121/6121 [==============================] - 0s 30us/step - loss: 1.4289 - acc: 0.4445 - val_loss: 1.4319 - val_acc: 0.4273\n",
      "Epoch 273/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.4307 - acc: 0.4422 - val_loss: 1.4336 - val_acc: 0.4200\n",
      "Epoch 274/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.4268 - acc: 0.4514 - val_loss: 1.4355 - val_acc: 0.4244\n",
      "Epoch 275/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.4264 - acc: 0.4463 - val_loss: 1.4277 - val_acc: 0.4302\n",
      "Epoch 276/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.4265 - acc: 0.4442 - val_loss: 1.4345 - val_acc: 0.4112\n",
      "Epoch 277/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.4246 - acc: 0.4530 - val_loss: 1.4339 - val_acc: 0.4288\n",
      "Epoch 278/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.4240 - acc: 0.4512 - val_loss: 1.4293 - val_acc: 0.4200\n",
      "Epoch 279/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.4235 - acc: 0.4470 - val_loss: 1.4310 - val_acc: 0.4302\n",
      "Epoch 280/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.4219 - acc: 0.4530 - val_loss: 1.4320 - val_acc: 0.4126\n",
      "Epoch 281/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.4212 - acc: 0.4509 - val_loss: 1.4286 - val_acc: 0.4244\n",
      "Epoch 282/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.4199 - acc: 0.4521 - val_loss: 1.4261 - val_acc: 0.4302\n",
      "Epoch 283/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.4183 - acc: 0.4483 - val_loss: 1.4208 - val_acc: 0.4273\n",
      "Epoch 284/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.4181 - acc: 0.4538 - val_loss: 1.4219 - val_acc: 0.4244\n",
      "Epoch 285/1000\n",
      "6121/6121 [==============================] - 0s 30us/step - loss: 1.4169 - acc: 0.4486 - val_loss: 1.4250 - val_acc: 0.4170\n",
      "Epoch 286/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.4164 - acc: 0.4534 - val_loss: 1.4210 - val_acc: 0.4302\n",
      "Epoch 287/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.4164 - acc: 0.4560 - val_loss: 1.4209 - val_acc: 0.4200\n",
      "Epoch 288/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.4135 - acc: 0.4521 - val_loss: 1.4230 - val_acc: 0.4229\n",
      "Epoch 289/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.4131 - acc: 0.4522 - val_loss: 1.4254 - val_acc: 0.4317\n",
      "Epoch 290/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.4131 - acc: 0.4586 - val_loss: 1.4289 - val_acc: 0.4258\n",
      "Epoch 291/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.4130 - acc: 0.4587 - val_loss: 1.4192 - val_acc: 0.4302\n",
      "Epoch 292/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.4109 - acc: 0.4568 - val_loss: 1.4143 - val_acc: 0.4258\n",
      "Epoch 293/1000\n",
      "6121/6121 [==============================] - 0s 33us/step - loss: 1.4105 - acc: 0.4530 - val_loss: 1.4142 - val_acc: 0.4405\n",
      "Epoch 294/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.4094 - acc: 0.4628 - val_loss: 1.4196 - val_acc: 0.4229\n",
      "Epoch 295/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.4080 - acc: 0.4566 - val_loss: 1.4210 - val_acc: 0.4185\n",
      "Epoch 296/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.4083 - acc: 0.4511 - val_loss: 1.4133 - val_acc: 0.4376\n",
      "Epoch 297/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.4073 - acc: 0.4592 - val_loss: 1.4114 - val_acc: 0.4361\n",
      "Epoch 298/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.4059 - acc: 0.4661 - val_loss: 1.4212 - val_acc: 0.4332\n",
      "Epoch 299/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.4045 - acc: 0.4573 - val_loss: 1.4110 - val_acc: 0.4347\n",
      "Epoch 300/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.4052 - acc: 0.4511 - val_loss: 1.4112 - val_acc: 0.4376\n",
      "Epoch 301/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.4034 - acc: 0.4625 - val_loss: 1.4146 - val_acc: 0.4317\n",
      "Epoch 302/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.4015 - acc: 0.4648 - val_loss: 1.4094 - val_acc: 0.4273\n",
      "Epoch 303/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.4009 - acc: 0.4633 - val_loss: 1.4074 - val_acc: 0.4479\n",
      "Epoch 304/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.4007 - acc: 0.4619 - val_loss: 1.4052 - val_acc: 0.4391\n",
      "Epoch 305/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.3988 - acc: 0.4620 - val_loss: 1.4086 - val_acc: 0.4317\n",
      "Epoch 306/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.3989 - acc: 0.4677 - val_loss: 1.4077 - val_acc: 0.4361\n",
      "Epoch 307/1000\n",
      "6121/6121 [==============================] - 0s 33us/step - loss: 1.3988 - acc: 0.4641 - val_loss: 1.4032 - val_acc: 0.4391\n",
      "Epoch 308/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.3974 - acc: 0.4599 - val_loss: 1.4129 - val_acc: 0.4273\n",
      "Epoch 309/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.3973 - acc: 0.4710 - val_loss: 1.4036 - val_acc: 0.4273\n",
      "Epoch 310/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.3955 - acc: 0.4632 - val_loss: 1.4096 - val_acc: 0.4244\n",
      "Epoch 311/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.3950 - acc: 0.4612 - val_loss: 1.3987 - val_acc: 0.4449\n",
      "Epoch 312/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.3936 - acc: 0.4625 - val_loss: 1.4031 - val_acc: 0.4361\n",
      "Epoch 313/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.3937 - acc: 0.4659 - val_loss: 1.4030 - val_acc: 0.4288\n",
      "Epoch 314/1000\n",
      "6121/6121 [==============================] - 0s 30us/step - loss: 1.3925 - acc: 0.4645 - val_loss: 1.3978 - val_acc: 0.4361\n",
      "Epoch 315/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.3917 - acc: 0.4687 - val_loss: 1.3985 - val_acc: 0.4391\n",
      "Epoch 316/1000\n",
      "6121/6121 [==============================] - 0s 33us/step - loss: 1.3904 - acc: 0.4741 - val_loss: 1.4065 - val_acc: 0.4288\n",
      "Epoch 317/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.3889 - acc: 0.4686 - val_loss: 1.3975 - val_acc: 0.4479\n",
      "Epoch 318/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.3896 - acc: 0.4679 - val_loss: 1.3968 - val_acc: 0.4347\n",
      "Epoch 319/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.3884 - acc: 0.4686 - val_loss: 1.3972 - val_acc: 0.4347\n",
      "Epoch 320/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.3872 - acc: 0.4710 - val_loss: 1.3980 - val_acc: 0.4376\n",
      "Epoch 321/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.3861 - acc: 0.4717 - val_loss: 1.4012 - val_acc: 0.4361\n",
      "Epoch 322/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.3851 - acc: 0.4684 - val_loss: 1.3936 - val_acc: 0.4347\n",
      "Epoch 323/1000\n",
      "6121/6121 [==============================] - 0s 33us/step - loss: 1.3853 - acc: 0.4736 - val_loss: 1.3984 - val_acc: 0.4288\n",
      "Epoch 324/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.3831 - acc: 0.4746 - val_loss: 1.3967 - val_acc: 0.4391\n",
      "Epoch 325/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.3827 - acc: 0.4736 - val_loss: 1.3932 - val_acc: 0.4391\n",
      "Epoch 326/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.3826 - acc: 0.4718 - val_loss: 1.3938 - val_acc: 0.4347\n",
      "Epoch 327/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.3805 - acc: 0.4712 - val_loss: 1.3916 - val_acc: 0.4347\n",
      "Epoch 328/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.3801 - acc: 0.4692 - val_loss: 1.3892 - val_acc: 0.4449\n",
      "Epoch 329/1000\n",
      "6121/6121 [==============================] - 0s 33us/step - loss: 1.3804 - acc: 0.4713 - val_loss: 1.3934 - val_acc: 0.4347\n",
      "Epoch 330/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.3783 - acc: 0.4774 - val_loss: 1.3887 - val_acc: 0.4405\n",
      "Epoch 331/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.3778 - acc: 0.4782 - val_loss: 1.3970 - val_acc: 0.4376\n",
      "Epoch 332/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.3774 - acc: 0.4725 - val_loss: 1.3880 - val_acc: 0.4347\n",
      "Epoch 333/1000\n",
      "6121/6121 [==============================] - 0s 33us/step - loss: 1.3766 - acc: 0.4689 - val_loss: 1.3922 - val_acc: 0.4391\n",
      "Epoch 334/1000\n",
      "6121/6121 [==============================] - 0s 34us/step - loss: 1.3763 - acc: 0.4748 - val_loss: 1.3917 - val_acc: 0.4288\n",
      "Epoch 335/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.3755 - acc: 0.4741 - val_loss: 1.3865 - val_acc: 0.4420\n",
      "Epoch 336/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.3743 - acc: 0.4752 - val_loss: 1.3851 - val_acc: 0.4332\n",
      "Epoch 337/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.3749 - acc: 0.4731 - val_loss: 1.3857 - val_acc: 0.4347\n",
      "Epoch 338/1000\n",
      "6121/6121 [==============================] - 0s 30us/step - loss: 1.3716 - acc: 0.4720 - val_loss: 1.3870 - val_acc: 0.4332\n",
      "Epoch 339/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.3714 - acc: 0.4797 - val_loss: 1.3841 - val_acc: 0.4391\n",
      "Epoch 340/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.3708 - acc: 0.4762 - val_loss: 1.3814 - val_acc: 0.4449\n",
      "Epoch 341/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.3693 - acc: 0.4797 - val_loss: 1.3853 - val_acc: 0.4479\n",
      "Epoch 342/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.3681 - acc: 0.4800 - val_loss: 1.3848 - val_acc: 0.4479\n",
      "Epoch 343/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.3683 - acc: 0.4829 - val_loss: 1.3819 - val_acc: 0.4435\n",
      "Epoch 344/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.3679 - acc: 0.4780 - val_loss: 1.3787 - val_acc: 0.4317\n",
      "Epoch 345/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.3670 - acc: 0.4779 - val_loss: 1.3892 - val_acc: 0.4435\n",
      "Epoch 346/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.3665 - acc: 0.4806 - val_loss: 1.3765 - val_acc: 0.4449\n",
      "Epoch 347/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.3654 - acc: 0.4857 - val_loss: 1.3785 - val_acc: 0.4420\n",
      "Epoch 348/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.3665 - acc: 0.4805 - val_loss: 1.3771 - val_acc: 0.4449\n",
      "Epoch 349/1000\n",
      "6121/6121 [==============================] - 0s 33us/step - loss: 1.3635 - acc: 0.4826 - val_loss: 1.3794 - val_acc: 0.4435\n",
      "Epoch 350/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.3620 - acc: 0.4779 - val_loss: 1.3763 - val_acc: 0.4537\n",
      "Epoch 351/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.3614 - acc: 0.4844 - val_loss: 1.3765 - val_acc: 0.4493\n",
      "Epoch 352/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.3614 - acc: 0.4831 - val_loss: 1.3761 - val_acc: 0.4391\n",
      "Epoch 353/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.3618 - acc: 0.4829 - val_loss: 1.3707 - val_acc: 0.4567\n",
      "Epoch 354/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.3597 - acc: 0.4837 - val_loss: 1.3757 - val_acc: 0.4361\n",
      "Epoch 355/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.3590 - acc: 0.4857 - val_loss: 1.3796 - val_acc: 0.4464\n",
      "Epoch 356/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.3586 - acc: 0.4851 - val_loss: 1.3704 - val_acc: 0.4449\n",
      "Epoch 357/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.3593 - acc: 0.4834 - val_loss: 1.3689 - val_acc: 0.4479\n",
      "Epoch 358/1000\n",
      "6121/6121 [==============================] - 0s 33us/step - loss: 1.3570 - acc: 0.4864 - val_loss: 1.3701 - val_acc: 0.4493\n",
      "Epoch 359/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.3571 - acc: 0.4847 - val_loss: 1.3720 - val_acc: 0.4464\n",
      "Epoch 360/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.3550 - acc: 0.4839 - val_loss: 1.3708 - val_acc: 0.4449\n",
      "Epoch 361/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.3544 - acc: 0.4852 - val_loss: 1.3713 - val_acc: 0.4596\n",
      "Epoch 362/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.3538 - acc: 0.4855 - val_loss: 1.3657 - val_acc: 0.4479\n",
      "Epoch 363/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.3542 - acc: 0.4851 - val_loss: 1.3777 - val_acc: 0.4552\n",
      "Epoch 364/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.3531 - acc: 0.4836 - val_loss: 1.3678 - val_acc: 0.4537\n",
      "Epoch 365/1000\n",
      "6121/6121 [==============================] - 0s 30us/step - loss: 1.3510 - acc: 0.4931 - val_loss: 1.3722 - val_acc: 0.4391\n",
      "Epoch 366/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.3516 - acc: 0.4865 - val_loss: 1.3715 - val_acc: 0.4405\n",
      "Epoch 367/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.3511 - acc: 0.4888 - val_loss: 1.3662 - val_acc: 0.4523\n",
      "Epoch 368/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.3486 - acc: 0.4880 - val_loss: 1.3663 - val_acc: 0.4508\n",
      "Epoch 369/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.3493 - acc: 0.4877 - val_loss: 1.3633 - val_acc: 0.4479\n",
      "Epoch 370/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.3477 - acc: 0.4890 - val_loss: 1.3622 - val_acc: 0.4508\n",
      "Epoch 371/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.3467 - acc: 0.4868 - val_loss: 1.3610 - val_acc: 0.4567\n",
      "Epoch 372/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.3458 - acc: 0.4914 - val_loss: 1.3606 - val_acc: 0.4537\n",
      "Epoch 373/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.3462 - acc: 0.4947 - val_loss: 1.3687 - val_acc: 0.4464\n",
      "Epoch 374/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.3450 - acc: 0.4940 - val_loss: 1.3638 - val_acc: 0.4391\n",
      "Epoch 375/1000\n",
      "6121/6121 [==============================] - 0s 33us/step - loss: 1.3449 - acc: 0.4888 - val_loss: 1.3626 - val_acc: 0.4464\n",
      "Epoch 376/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.3430 - acc: 0.4868 - val_loss: 1.3612 - val_acc: 0.4523\n",
      "Epoch 377/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.3434 - acc: 0.4896 - val_loss: 1.3598 - val_acc: 0.4464\n",
      "Epoch 378/1000\n",
      "6121/6121 [==============================] - 0s 30us/step - loss: 1.3423 - acc: 0.4942 - val_loss: 1.3589 - val_acc: 0.4596\n",
      "Epoch 379/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.3404 - acc: 0.4931 - val_loss: 1.3633 - val_acc: 0.4420\n",
      "Epoch 380/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.3411 - acc: 0.4911 - val_loss: 1.3635 - val_acc: 0.4479\n",
      "Epoch 381/1000\n",
      "6121/6121 [==============================] - 0s 33us/step - loss: 1.3410 - acc: 0.4919 - val_loss: 1.3584 - val_acc: 0.4567\n",
      "Epoch 382/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.3393 - acc: 0.4965 - val_loss: 1.3572 - val_acc: 0.4552\n",
      "Epoch 383/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.3383 - acc: 0.4882 - val_loss: 1.3567 - val_acc: 0.4611\n",
      "Epoch 384/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.3390 - acc: 0.4940 - val_loss: 1.3590 - val_acc: 0.4523\n",
      "Epoch 385/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.3364 - acc: 0.4917 - val_loss: 1.3526 - val_acc: 0.4552\n",
      "Epoch 386/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.3372 - acc: 0.4942 - val_loss: 1.3515 - val_acc: 0.4523\n",
      "Epoch 387/1000\n",
      "6121/6121 [==============================] - 0s 33us/step - loss: 1.3353 - acc: 0.4957 - val_loss: 1.3581 - val_acc: 0.4567\n",
      "Epoch 388/1000\n",
      "6121/6121 [==============================] - 0s 33us/step - loss: 1.3376 - acc: 0.4913 - val_loss: 1.3516 - val_acc: 0.4611\n",
      "Epoch 389/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.3345 - acc: 0.4962 - val_loss: 1.3572 - val_acc: 0.4464\n",
      "Epoch 390/1000\n",
      "6121/6121 [==============================] - 0s 30us/step - loss: 1.3339 - acc: 0.4968 - val_loss: 1.3551 - val_acc: 0.4479\n",
      "Epoch 391/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.3343 - acc: 0.4963 - val_loss: 1.3481 - val_acc: 0.4596\n",
      "Epoch 392/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.3322 - acc: 0.4963 - val_loss: 1.3540 - val_acc: 0.4479\n",
      "Epoch 393/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.3323 - acc: 0.4949 - val_loss: 1.3479 - val_acc: 0.4552\n",
      "Epoch 394/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.3307 - acc: 0.4984 - val_loss: 1.3484 - val_acc: 0.4508\n",
      "Epoch 395/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.3298 - acc: 0.5017 - val_loss: 1.3478 - val_acc: 0.4537\n",
      "Epoch 396/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.3287 - acc: 0.4960 - val_loss: 1.3566 - val_acc: 0.4611\n",
      "Epoch 397/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.3298 - acc: 0.4975 - val_loss: 1.3547 - val_acc: 0.4581\n",
      "Epoch 398/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.3298 - acc: 0.4975 - val_loss: 1.3451 - val_acc: 0.4640\n",
      "Epoch 399/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.3278 - acc: 0.5051 - val_loss: 1.3592 - val_acc: 0.4508\n",
      "Epoch 400/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.3272 - acc: 0.4973 - val_loss: 1.3448 - val_acc: 0.4684\n",
      "Epoch 401/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.3274 - acc: 0.4998 - val_loss: 1.3455 - val_acc: 0.4552\n",
      "Epoch 402/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.3250 - acc: 0.4983 - val_loss: 1.3475 - val_acc: 0.4699\n",
      "Epoch 403/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.3271 - acc: 0.5024 - val_loss: 1.3456 - val_acc: 0.4523\n",
      "Epoch 404/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.3245 - acc: 0.5022 - val_loss: 1.3422 - val_acc: 0.4611\n",
      "Epoch 405/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.3236 - acc: 0.4983 - val_loss: 1.3418 - val_acc: 0.4640\n",
      "Epoch 406/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.3240 - acc: 0.5032 - val_loss: 1.3417 - val_acc: 0.4626\n",
      "Epoch 407/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.3233 - acc: 0.5016 - val_loss: 1.3406 - val_acc: 0.4655\n",
      "Epoch 408/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.3213 - acc: 0.5016 - val_loss: 1.3393 - val_acc: 0.4640\n",
      "Epoch 409/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.3236 - acc: 0.5019 - val_loss: 1.3392 - val_acc: 0.4611\n",
      "Epoch 410/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.3207 - acc: 0.4996 - val_loss: 1.3406 - val_acc: 0.4596\n",
      "Epoch 411/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.3204 - acc: 0.5065 - val_loss: 1.3392 - val_acc: 0.4611\n",
      "Epoch 412/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.3194 - acc: 0.5017 - val_loss: 1.3400 - val_acc: 0.4611\n",
      "Epoch 413/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.3208 - acc: 0.5030 - val_loss: 1.3368 - val_acc: 0.4684\n",
      "Epoch 414/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.3199 - acc: 0.5035 - val_loss: 1.3420 - val_acc: 0.4640\n",
      "Epoch 415/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.3182 - acc: 0.5030 - val_loss: 1.3418 - val_acc: 0.4655\n",
      "Epoch 416/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.3167 - acc: 0.5063 - val_loss: 1.3376 - val_acc: 0.4611\n",
      "Epoch 417/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.3157 - acc: 0.5051 - val_loss: 1.3377 - val_acc: 0.4611\n",
      "Epoch 418/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.3149 - acc: 0.5060 - val_loss: 1.3353 - val_acc: 0.4567\n",
      "Epoch 419/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.3148 - acc: 0.5074 - val_loss: 1.3352 - val_acc: 0.4655\n",
      "Epoch 420/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.3129 - acc: 0.5076 - val_loss: 1.3387 - val_acc: 0.4596\n",
      "Epoch 421/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.3130 - acc: 0.5043 - val_loss: 1.3411 - val_acc: 0.4552\n",
      "Epoch 422/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.3138 - acc: 0.5074 - val_loss: 1.3370 - val_acc: 0.4670\n",
      "Epoch 423/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.3133 - acc: 0.5087 - val_loss: 1.3339 - val_acc: 0.4596\n",
      "Epoch 424/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.3126 - acc: 0.5051 - val_loss: 1.3337 - val_acc: 0.4670\n",
      "Epoch 425/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.3121 - acc: 0.5115 - val_loss: 1.3370 - val_acc: 0.4523\n",
      "Epoch 426/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.3115 - acc: 0.5069 - val_loss: 1.3307 - val_acc: 0.4684\n",
      "Epoch 427/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.3090 - acc: 0.5078 - val_loss: 1.3328 - val_acc: 0.4787\n",
      "Epoch 428/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.3109 - acc: 0.5068 - val_loss: 1.3374 - val_acc: 0.4611\n",
      "Epoch 429/1000\n",
      "6121/6121 [==============================] - 0s 30us/step - loss: 1.3105 - acc: 0.5060 - val_loss: 1.3300 - val_acc: 0.4596\n",
      "Epoch 430/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.3081 - acc: 0.5074 - val_loss: 1.3364 - val_acc: 0.4567\n",
      "Epoch 431/1000\n",
      "6121/6121 [==============================] - 0s 30us/step - loss: 1.3095 - acc: 0.5074 - val_loss: 1.3280 - val_acc: 0.4743\n",
      "Epoch 432/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.3078 - acc: 0.5083 - val_loss: 1.3301 - val_acc: 0.4626\n",
      "Epoch 433/1000\n",
      "6121/6121 [==============================] - 0s 33us/step - loss: 1.3056 - acc: 0.5132 - val_loss: 1.3267 - val_acc: 0.4655\n",
      "Epoch 434/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.3051 - acc: 0.5140 - val_loss: 1.3347 - val_acc: 0.4611\n",
      "Epoch 435/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.3046 - acc: 0.5177 - val_loss: 1.3269 - val_acc: 0.4684\n",
      "Epoch 436/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.3041 - acc: 0.5109 - val_loss: 1.3332 - val_acc: 0.4714\n",
      "Epoch 437/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.3037 - acc: 0.5087 - val_loss: 1.3237 - val_acc: 0.4758\n",
      "Epoch 438/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.3034 - acc: 0.5159 - val_loss: 1.3280 - val_acc: 0.4626\n",
      "Epoch 439/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.3042 - acc: 0.5086 - val_loss: 1.3286 - val_acc: 0.4743\n",
      "Epoch 440/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.3022 - acc: 0.5149 - val_loss: 1.3250 - val_acc: 0.4684\n",
      "Epoch 441/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.3024 - acc: 0.5125 - val_loss: 1.3226 - val_acc: 0.4670\n",
      "Epoch 442/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.3009 - acc: 0.5079 - val_loss: 1.3269 - val_acc: 0.4772\n",
      "Epoch 443/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.3012 - acc: 0.5138 - val_loss: 1.3219 - val_acc: 0.4743\n",
      "Epoch 444/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2997 - acc: 0.5133 - val_loss: 1.3195 - val_acc: 0.4787\n",
      "Epoch 445/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2993 - acc: 0.5154 - val_loss: 1.3226 - val_acc: 0.4728\n",
      "Epoch 446/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2985 - acc: 0.5156 - val_loss: 1.3194 - val_acc: 0.4728\n",
      "Epoch 447/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2979 - acc: 0.5130 - val_loss: 1.3297 - val_acc: 0.4699\n",
      "Epoch 448/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2976 - acc: 0.5114 - val_loss: 1.3214 - val_acc: 0.4655\n",
      "Epoch 449/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2972 - acc: 0.5109 - val_loss: 1.3227 - val_acc: 0.4743\n",
      "Epoch 450/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2973 - acc: 0.5141 - val_loss: 1.3204 - val_acc: 0.4684\n",
      "Epoch 451/1000\n",
      "6121/6121 [==============================] - 0s 30us/step - loss: 1.2970 - acc: 0.5145 - val_loss: 1.3182 - val_acc: 0.4802\n",
      "Epoch 452/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2978 - acc: 0.5167 - val_loss: 1.3195 - val_acc: 0.4728\n",
      "Epoch 453/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2952 - acc: 0.5192 - val_loss: 1.3242 - val_acc: 0.4758\n",
      "Epoch 454/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2961 - acc: 0.5135 - val_loss: 1.3179 - val_acc: 0.4772\n",
      "Epoch 455/1000\n",
      "6121/6121 [==============================] - 0s 30us/step - loss: 1.2935 - acc: 0.5215 - val_loss: 1.3221 - val_acc: 0.4787\n",
      "Epoch 456/1000\n",
      "6121/6121 [==============================] - 0s 30us/step - loss: 1.2942 - acc: 0.5153 - val_loss: 1.3234 - val_acc: 0.4684\n",
      "Epoch 457/1000\n",
      "6121/6121 [==============================] - 0s 30us/step - loss: 1.2936 - acc: 0.5153 - val_loss: 1.3205 - val_acc: 0.4728\n",
      "Epoch 458/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2935 - acc: 0.5140 - val_loss: 1.3151 - val_acc: 0.4816\n",
      "Epoch 459/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2920 - acc: 0.5187 - val_loss: 1.3204 - val_acc: 0.4802\n",
      "Epoch 460/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2914 - acc: 0.5218 - val_loss: 1.3198 - val_acc: 0.4772\n",
      "Epoch 461/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2910 - acc: 0.5130 - val_loss: 1.3164 - val_acc: 0.4758\n",
      "Epoch 462/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2900 - acc: 0.5176 - val_loss: 1.3183 - val_acc: 0.4802\n",
      "Epoch 463/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2898 - acc: 0.5203 - val_loss: 1.3234 - val_acc: 0.4743\n",
      "Epoch 464/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2897 - acc: 0.5192 - val_loss: 1.3206 - val_acc: 0.4728\n",
      "Epoch 465/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2890 - acc: 0.5176 - val_loss: 1.3147 - val_acc: 0.4714\n",
      "Epoch 466/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2886 - acc: 0.5192 - val_loss: 1.3145 - val_acc: 0.4846\n",
      "Epoch 467/1000\n",
      "6121/6121 [==============================] - 0s 30us/step - loss: 1.2912 - acc: 0.5184 - val_loss: 1.3172 - val_acc: 0.4816\n",
      "Epoch 468/1000\n",
      "6121/6121 [==============================] - 0s 33us/step - loss: 1.2881 - acc: 0.5203 - val_loss: 1.3153 - val_acc: 0.4772\n",
      "Epoch 469/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2867 - acc: 0.5207 - val_loss: 1.3158 - val_acc: 0.4772\n",
      "Epoch 470/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2870 - acc: 0.5239 - val_loss: 1.3162 - val_acc: 0.4655\n",
      "Epoch 471/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2863 - acc: 0.5215 - val_loss: 1.3118 - val_acc: 0.4758\n",
      "Epoch 472/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2852 - acc: 0.5184 - val_loss: 1.3105 - val_acc: 0.4802\n",
      "Epoch 473/1000\n",
      "6121/6121 [==============================] - 0s 30us/step - loss: 1.2836 - acc: 0.5225 - val_loss: 1.3091 - val_acc: 0.4772\n",
      "Epoch 474/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2853 - acc: 0.5210 - val_loss: 1.3092 - val_acc: 0.4772\n",
      "Epoch 475/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2852 - acc: 0.5182 - val_loss: 1.3085 - val_acc: 0.4860\n",
      "Epoch 476/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2823 - acc: 0.5202 - val_loss: 1.3130 - val_acc: 0.4699\n",
      "Epoch 477/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2833 - acc: 0.5189 - val_loss: 1.3115 - val_acc: 0.4816\n",
      "Epoch 478/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2824 - acc: 0.5195 - val_loss: 1.3104 - val_acc: 0.4831\n",
      "Epoch 479/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2819 - acc: 0.5213 - val_loss: 1.3070 - val_acc: 0.4802\n",
      "Epoch 480/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2824 - acc: 0.5177 - val_loss: 1.3064 - val_acc: 0.4934\n",
      "Epoch 481/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2801 - acc: 0.5256 - val_loss: 1.3139 - val_acc: 0.4802\n",
      "Epoch 482/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2795 - acc: 0.5249 - val_loss: 1.3107 - val_acc: 0.4758\n",
      "Epoch 483/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2796 - acc: 0.5223 - val_loss: 1.3080 - val_acc: 0.4934\n",
      "Epoch 484/1000\n",
      "6121/6121 [==============================] - 0s 33us/step - loss: 1.2799 - acc: 0.5236 - val_loss: 1.3185 - val_acc: 0.4699\n",
      "Epoch 485/1000\n",
      "6121/6121 [==============================] - 0s 34us/step - loss: 1.2799 - acc: 0.5216 - val_loss: 1.3056 - val_acc: 0.4860\n",
      "Epoch 486/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2788 - acc: 0.5212 - val_loss: 1.3057 - val_acc: 0.4831\n",
      "Epoch 487/1000\n",
      "6121/6121 [==============================] - 0s 30us/step - loss: 1.2783 - acc: 0.5246 - val_loss: 1.3116 - val_acc: 0.4787\n",
      "Epoch 488/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2773 - acc: 0.5216 - val_loss: 1.3036 - val_acc: 0.4802\n",
      "Epoch 489/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2778 - acc: 0.5216 - val_loss: 1.3033 - val_acc: 0.4949\n",
      "Epoch 490/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2767 - acc: 0.5261 - val_loss: 1.3136 - val_acc: 0.4728\n",
      "Epoch 491/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2757 - acc: 0.5275 - val_loss: 1.3038 - val_acc: 0.4919\n",
      "Epoch 492/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2754 - acc: 0.5262 - val_loss: 1.3027 - val_acc: 0.4875\n",
      "Epoch 493/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2749 - acc: 0.5225 - val_loss: 1.3030 - val_acc: 0.4831\n",
      "Epoch 494/1000\n",
      "6121/6121 [==============================] - 0s 30us/step - loss: 1.2744 - acc: 0.5318 - val_loss: 1.3074 - val_acc: 0.4816\n",
      "Epoch 495/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2743 - acc: 0.5248 - val_loss: 1.3060 - val_acc: 0.4875\n",
      "Epoch 496/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2737 - acc: 0.5248 - val_loss: 1.3023 - val_acc: 0.4846\n",
      "Epoch 497/1000\n",
      "6121/6121 [==============================] - 0s 30us/step - loss: 1.2736 - acc: 0.5264 - val_loss: 1.3009 - val_acc: 0.4919\n",
      "Epoch 498/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2744 - acc: 0.5221 - val_loss: 1.3020 - val_acc: 0.4831\n",
      "Epoch 499/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2733 - acc: 0.5275 - val_loss: 1.3011 - val_acc: 0.4949\n",
      "Epoch 500/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2716 - acc: 0.5251 - val_loss: 1.3023 - val_acc: 0.4905\n",
      "Epoch 501/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2727 - acc: 0.5225 - val_loss: 1.2999 - val_acc: 0.4860\n",
      "Epoch 502/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2722 - acc: 0.5244 - val_loss: 1.2999 - val_acc: 0.5037\n",
      "Epoch 503/1000\n",
      "6121/6121 [==============================] - 0s 30us/step - loss: 1.2708 - acc: 0.5310 - val_loss: 1.2992 - val_acc: 0.4860\n",
      "Epoch 504/1000\n",
      "6121/6121 [==============================] - 0s 30us/step - loss: 1.2704 - acc: 0.5295 - val_loss: 1.3021 - val_acc: 0.4919\n",
      "Epoch 505/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2728 - acc: 0.5241 - val_loss: 1.2987 - val_acc: 0.4934\n",
      "Epoch 506/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2688 - acc: 0.5285 - val_loss: 1.2983 - val_acc: 0.4772\n",
      "Epoch 507/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2693 - acc: 0.5310 - val_loss: 1.3003 - val_acc: 0.4905\n",
      "Epoch 508/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2688 - acc: 0.5283 - val_loss: 1.3007 - val_acc: 0.4934\n",
      "Epoch 509/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2678 - acc: 0.5274 - val_loss: 1.2980 - val_acc: 0.5007\n",
      "Epoch 510/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2688 - acc: 0.5321 - val_loss: 1.2976 - val_acc: 0.4890\n",
      "Epoch 511/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2671 - acc: 0.5310 - val_loss: 1.2979 - val_acc: 0.4816\n",
      "Epoch 512/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2677 - acc: 0.5244 - val_loss: 1.2944 - val_acc: 0.4978\n",
      "Epoch 513/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2675 - acc: 0.5293 - val_loss: 1.2963 - val_acc: 0.4978\n",
      "Epoch 514/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2663 - acc: 0.5310 - val_loss: 1.2983 - val_acc: 0.4905\n",
      "Epoch 515/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2659 - acc: 0.5272 - val_loss: 1.2983 - val_acc: 0.4846\n",
      "Epoch 516/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2653 - acc: 0.5297 - val_loss: 1.2960 - val_acc: 0.5022\n",
      "Epoch 517/1000\n",
      "6121/6121 [==============================] - 0s 33us/step - loss: 1.2666 - acc: 0.5293 - val_loss: 1.2993 - val_acc: 0.4831\n",
      "Epoch 518/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2654 - acc: 0.5272 - val_loss: 1.3033 - val_acc: 0.4816\n",
      "Epoch 519/1000\n",
      "6121/6121 [==============================] - 0s 30us/step - loss: 1.2644 - acc: 0.5319 - val_loss: 1.2960 - val_acc: 0.5007\n",
      "Epoch 520/1000\n",
      "6121/6121 [==============================] - 0s 33us/step - loss: 1.2634 - acc: 0.5331 - val_loss: 1.2968 - val_acc: 0.4831\n",
      "Epoch 521/1000\n",
      "6121/6121 [==============================] - 0s 33us/step - loss: 1.2644 - acc: 0.5261 - val_loss: 1.2987 - val_acc: 0.4934\n",
      "Epoch 522/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2646 - acc: 0.5306 - val_loss: 1.2954 - val_acc: 0.4978\n",
      "Epoch 523/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2657 - acc: 0.5282 - val_loss: 1.2915 - val_acc: 0.4919\n",
      "Epoch 524/1000\n",
      "6121/6121 [==============================] - 0s 30us/step - loss: 1.2639 - acc: 0.5314 - val_loss: 1.2945 - val_acc: 0.4905\n",
      "Epoch 525/1000\n",
      "6121/6121 [==============================] - 0s 30us/step - loss: 1.2619 - acc: 0.5298 - val_loss: 1.2940 - val_acc: 0.4875\n",
      "Epoch 526/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2605 - acc: 0.5331 - val_loss: 1.2966 - val_acc: 0.4890\n",
      "Epoch 527/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2611 - acc: 0.5342 - val_loss: 1.2950 - val_acc: 0.4905\n",
      "Epoch 528/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2614 - acc: 0.5272 - val_loss: 1.2924 - val_acc: 0.4846\n",
      "Epoch 529/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2612 - acc: 0.5301 - val_loss: 1.2936 - val_acc: 0.4860\n",
      "Epoch 530/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2605 - acc: 0.5334 - val_loss: 1.2920 - val_acc: 0.4919\n",
      "Epoch 531/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2591 - acc: 0.5319 - val_loss: 1.2910 - val_acc: 0.4919\n",
      "Epoch 532/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2611 - acc: 0.5336 - val_loss: 1.2929 - val_acc: 0.4949\n",
      "Epoch 533/1000\n",
      "6121/6121 [==============================] - 0s 34us/step - loss: 1.2588 - acc: 0.5347 - val_loss: 1.2950 - val_acc: 0.4875\n",
      "Epoch 534/1000\n",
      "6121/6121 [==============================] - 0s 30us/step - loss: 1.2583 - acc: 0.5313 - val_loss: 1.2884 - val_acc: 0.5066\n",
      "Epoch 535/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2574 - acc: 0.5339 - val_loss: 1.2918 - val_acc: 0.4831\n",
      "Epoch 536/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2589 - acc: 0.5329 - val_loss: 1.2993 - val_acc: 0.4905\n",
      "Epoch 537/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2573 - acc: 0.5329 - val_loss: 1.2999 - val_acc: 0.4816\n",
      "Epoch 538/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2578 - acc: 0.5337 - val_loss: 1.2926 - val_acc: 0.4875\n",
      "Epoch 539/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2573 - acc: 0.5364 - val_loss: 1.2961 - val_acc: 0.4905\n",
      "Epoch 540/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2573 - acc: 0.5321 - val_loss: 1.2908 - val_acc: 0.4919\n",
      "Epoch 541/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2559 - acc: 0.5308 - val_loss: 1.2874 - val_acc: 0.5007\n",
      "Epoch 542/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2552 - acc: 0.5352 - val_loss: 1.2892 - val_acc: 0.4934\n",
      "Epoch 543/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2548 - acc: 0.5326 - val_loss: 1.2899 - val_acc: 0.4890\n",
      "Epoch 544/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2550 - acc: 0.5308 - val_loss: 1.2858 - val_acc: 0.5037\n",
      "Epoch 545/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2552 - acc: 0.5368 - val_loss: 1.2890 - val_acc: 0.4993\n",
      "Epoch 546/1000\n",
      "6121/6121 [==============================] - 0s 33us/step - loss: 1.2544 - acc: 0.5350 - val_loss: 1.2860 - val_acc: 0.4949\n",
      "Epoch 547/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2544 - acc: 0.5341 - val_loss: 1.2864 - val_acc: 0.5007\n",
      "Epoch 548/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2531 - acc: 0.5318 - val_loss: 1.2889 - val_acc: 0.4963\n",
      "Epoch 549/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2539 - acc: 0.5332 - val_loss: 1.2897 - val_acc: 0.4963\n",
      "Epoch 550/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2531 - acc: 0.5360 - val_loss: 1.2889 - val_acc: 0.5007\n",
      "Epoch 551/1000\n",
      "6121/6121 [==============================] - 0s 30us/step - loss: 1.2518 - acc: 0.5342 - val_loss: 1.2899 - val_acc: 0.4875\n",
      "Epoch 552/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2530 - acc: 0.5385 - val_loss: 1.2841 - val_acc: 0.4949\n",
      "Epoch 553/1000\n",
      "6121/6121 [==============================] - 0s 33us/step - loss: 1.2511 - acc: 0.5395 - val_loss: 1.2890 - val_acc: 0.4831\n",
      "Epoch 554/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2507 - acc: 0.5347 - val_loss: 1.2875 - val_acc: 0.4934\n",
      "Epoch 555/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2519 - acc: 0.5360 - val_loss: 1.2862 - val_acc: 0.4949\n",
      "Epoch 556/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2501 - acc: 0.5354 - val_loss: 1.2863 - val_acc: 0.4993\n",
      "Epoch 557/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2503 - acc: 0.5360 - val_loss: 1.2892 - val_acc: 0.4816\n",
      "Epoch 558/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2499 - acc: 0.5350 - val_loss: 1.2874 - val_acc: 0.4905\n",
      "Epoch 559/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2491 - acc: 0.5373 - val_loss: 1.2847 - val_acc: 0.5051\n",
      "Epoch 560/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2491 - acc: 0.5381 - val_loss: 1.2811 - val_acc: 0.4963\n",
      "Epoch 561/1000\n",
      "6121/6121 [==============================] - 0s 30us/step - loss: 1.2499 - acc: 0.5342 - val_loss: 1.2910 - val_acc: 0.4831\n",
      "Epoch 562/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2493 - acc: 0.5393 - val_loss: 1.2857 - val_acc: 0.4905\n",
      "Epoch 563/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2484 - acc: 0.5375 - val_loss: 1.2843 - val_acc: 0.5022\n",
      "Epoch 564/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2495 - acc: 0.5390 - val_loss: 1.2809 - val_acc: 0.5007\n",
      "Epoch 565/1000\n",
      "6121/6121 [==============================] - 0s 30us/step - loss: 1.2490 - acc: 0.5362 - val_loss: 1.2805 - val_acc: 0.4949\n",
      "Epoch 566/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2468 - acc: 0.5390 - val_loss: 1.2833 - val_acc: 0.4890\n",
      "Epoch 567/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2461 - acc: 0.5378 - val_loss: 1.2874 - val_acc: 0.4846\n",
      "Epoch 568/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2474 - acc: 0.5375 - val_loss: 1.2816 - val_acc: 0.4934\n",
      "Epoch 569/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2468 - acc: 0.5350 - val_loss: 1.2830 - val_acc: 0.4978\n",
      "Epoch 570/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2470 - acc: 0.5393 - val_loss: 1.2818 - val_acc: 0.4905\n",
      "Epoch 571/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2463 - acc: 0.5367 - val_loss: 1.2847 - val_acc: 0.5066\n",
      "Epoch 572/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2466 - acc: 0.5368 - val_loss: 1.2848 - val_acc: 0.4949\n",
      "Epoch 573/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2453 - acc: 0.5388 - val_loss: 1.2860 - val_acc: 0.4875\n",
      "Epoch 574/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2468 - acc: 0.5386 - val_loss: 1.2825 - val_acc: 0.4875\n",
      "Epoch 575/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2447 - acc: 0.5381 - val_loss: 1.2808 - val_acc: 0.5051\n",
      "Epoch 576/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2445 - acc: 0.5383 - val_loss: 1.2813 - val_acc: 0.4919\n",
      "Epoch 577/1000\n",
      "6121/6121 [==============================] - 0s 30us/step - loss: 1.2433 - acc: 0.5368 - val_loss: 1.2821 - val_acc: 0.4978\n",
      "Epoch 578/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2445 - acc: 0.5408 - val_loss: 1.2797 - val_acc: 0.4978\n",
      "Epoch 579/1000\n",
      "6121/6121 [==============================] - 0s 30us/step - loss: 1.2427 - acc: 0.5381 - val_loss: 1.2804 - val_acc: 0.4978\n",
      "Epoch 580/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2434 - acc: 0.5347 - val_loss: 1.2764 - val_acc: 0.5081\n",
      "Epoch 581/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2440 - acc: 0.5386 - val_loss: 1.2756 - val_acc: 0.5051\n",
      "Epoch 582/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2437 - acc: 0.5368 - val_loss: 1.2824 - val_acc: 0.4934\n",
      "Epoch 583/1000\n",
      "6121/6121 [==============================] - 0s 33us/step - loss: 1.2422 - acc: 0.5404 - val_loss: 1.2763 - val_acc: 0.5022\n",
      "Epoch 584/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2431 - acc: 0.5381 - val_loss: 1.2875 - val_acc: 0.4831\n",
      "Epoch 585/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2415 - acc: 0.5377 - val_loss: 1.2848 - val_acc: 0.4963\n",
      "Epoch 586/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2418 - acc: 0.5386 - val_loss: 1.2759 - val_acc: 0.4978\n",
      "Epoch 587/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2401 - acc: 0.5386 - val_loss: 1.2846 - val_acc: 0.4993\n",
      "Epoch 588/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2415 - acc: 0.5395 - val_loss: 1.2742 - val_acc: 0.5081\n",
      "Epoch 589/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2423 - acc: 0.5422 - val_loss: 1.2766 - val_acc: 0.5022\n",
      "Epoch 590/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2407 - acc: 0.5381 - val_loss: 1.2746 - val_acc: 0.5022\n",
      "Epoch 591/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2402 - acc: 0.5403 - val_loss: 1.2857 - val_acc: 0.4860\n",
      "Epoch 592/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2391 - acc: 0.5393 - val_loss: 1.2767 - val_acc: 0.5022\n",
      "Epoch 593/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2396 - acc: 0.5424 - val_loss: 1.2754 - val_acc: 0.5081\n",
      "Epoch 594/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2419 - acc: 0.5390 - val_loss: 1.2786 - val_acc: 0.5007\n",
      "Epoch 595/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2389 - acc: 0.5429 - val_loss: 1.2752 - val_acc: 0.4963\n",
      "Epoch 596/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2380 - acc: 0.5399 - val_loss: 1.2749 - val_acc: 0.5037\n",
      "Epoch 597/1000\n",
      "6121/6121 [==============================] - 0s 30us/step - loss: 1.2387 - acc: 0.5375 - val_loss: 1.2757 - val_acc: 0.4875\n",
      "Epoch 598/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2372 - acc: 0.5426 - val_loss: 1.2828 - val_acc: 0.4934\n",
      "Epoch 599/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2371 - acc: 0.5409 - val_loss: 1.2765 - val_acc: 0.4993\n",
      "Epoch 600/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2375 - acc: 0.5398 - val_loss: 1.2725 - val_acc: 0.5095\n",
      "Epoch 601/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2375 - acc: 0.5406 - val_loss: 1.2771 - val_acc: 0.5022\n",
      "Epoch 602/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2370 - acc: 0.5419 - val_loss: 1.2859 - val_acc: 0.4934\n",
      "Epoch 603/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2358 - acc: 0.5417 - val_loss: 1.2777 - val_acc: 0.4993\n",
      "Epoch 604/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2359 - acc: 0.5393 - val_loss: 1.2760 - val_acc: 0.5037\n",
      "Epoch 605/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2366 - acc: 0.5444 - val_loss: 1.2733 - val_acc: 0.5007\n",
      "Epoch 606/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2375 - acc: 0.5413 - val_loss: 1.2768 - val_acc: 0.5051\n",
      "Epoch 607/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2356 - acc: 0.5429 - val_loss: 1.2752 - val_acc: 0.4934\n",
      "Epoch 608/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2350 - acc: 0.5475 - val_loss: 1.2778 - val_acc: 0.5037\n",
      "Epoch 609/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2352 - acc: 0.5445 - val_loss: 1.2841 - val_acc: 0.4993\n",
      "Epoch 610/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2346 - acc: 0.5424 - val_loss: 1.2732 - val_acc: 0.4978\n",
      "Epoch 611/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2346 - acc: 0.5414 - val_loss: 1.2748 - val_acc: 0.5066\n",
      "Epoch 612/1000\n",
      "6121/6121 [==============================] - 0s 30us/step - loss: 1.2329 - acc: 0.5430 - val_loss: 1.2728 - val_acc: 0.5022\n",
      "Epoch 613/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2347 - acc: 0.5399 - val_loss: 1.2724 - val_acc: 0.5095\n",
      "Epoch 614/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2335 - acc: 0.5377 - val_loss: 1.2766 - val_acc: 0.5051\n",
      "Epoch 615/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2345 - acc: 0.5404 - val_loss: 1.2700 - val_acc: 0.5095\n",
      "Epoch 616/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2347 - acc: 0.5403 - val_loss: 1.2768 - val_acc: 0.5095\n",
      "Epoch 617/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2324 - acc: 0.5448 - val_loss: 1.2721 - val_acc: 0.5095\n",
      "Epoch 618/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2323 - acc: 0.5404 - val_loss: 1.2700 - val_acc: 0.5051\n",
      "Epoch 619/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2324 - acc: 0.5450 - val_loss: 1.2780 - val_acc: 0.4978\n",
      "Epoch 620/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2312 - acc: 0.5468 - val_loss: 1.2733 - val_acc: 0.4978\n",
      "Epoch 621/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2312 - acc: 0.5450 - val_loss: 1.2778 - val_acc: 0.5037\n",
      "Epoch 622/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2328 - acc: 0.5417 - val_loss: 1.2804 - val_acc: 0.4993\n",
      "Epoch 623/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2304 - acc: 0.5444 - val_loss: 1.2668 - val_acc: 0.5022\n",
      "Epoch 624/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2313 - acc: 0.5427 - val_loss: 1.2802 - val_acc: 0.5051\n",
      "Epoch 625/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2322 - acc: 0.5432 - val_loss: 1.2766 - val_acc: 0.5051\n",
      "Epoch 626/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2295 - acc: 0.5447 - val_loss: 1.2724 - val_acc: 0.4905\n",
      "Epoch 627/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2293 - acc: 0.5444 - val_loss: 1.2687 - val_acc: 0.5095\n",
      "Epoch 628/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2287 - acc: 0.5432 - val_loss: 1.2734 - val_acc: 0.4978\n",
      "Epoch 629/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2295 - acc: 0.5473 - val_loss: 1.2667 - val_acc: 0.5037\n",
      "Epoch 630/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2310 - acc: 0.5429 - val_loss: 1.2744 - val_acc: 0.5081\n",
      "Epoch 631/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2314 - acc: 0.5380 - val_loss: 1.2859 - val_acc: 0.4949\n",
      "Epoch 632/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2302 - acc: 0.5455 - val_loss: 1.2661 - val_acc: 0.5140\n",
      "Epoch 633/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2311 - acc: 0.5430 - val_loss: 1.2670 - val_acc: 0.5037\n",
      "Epoch 634/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2282 - acc: 0.5445 - val_loss: 1.2714 - val_acc: 0.5169\n",
      "Epoch 635/1000\n",
      "6121/6121 [==============================] - 0s 33us/step - loss: 1.2289 - acc: 0.5416 - val_loss: 1.2670 - val_acc: 0.5110\n",
      "Epoch 636/1000\n",
      "6121/6121 [==============================] - 0s 33us/step - loss: 1.2293 - acc: 0.5426 - val_loss: 1.2696 - val_acc: 0.5022\n",
      "Epoch 637/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2293 - acc: 0.5427 - val_loss: 1.2705 - val_acc: 0.5051\n",
      "Epoch 638/1000\n",
      "6121/6121 [==============================] - 0s 33us/step - loss: 1.2271 - acc: 0.5447 - val_loss: 1.2652 - val_acc: 0.5140\n",
      "Epoch 639/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2275 - acc: 0.5426 - val_loss: 1.2727 - val_acc: 0.4949\n",
      "Epoch 640/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2274 - acc: 0.5427 - val_loss: 1.2693 - val_acc: 0.4963\n",
      "Epoch 641/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2272 - acc: 0.5437 - val_loss: 1.2660 - val_acc: 0.5051\n",
      "Epoch 642/1000\n",
      "6121/6121 [==============================] - 0s 37us/step - loss: 1.2267 - acc: 0.5468 - val_loss: 1.2668 - val_acc: 0.5125\n",
      "Epoch 643/1000\n",
      "6121/6121 [==============================] - 0s 33us/step - loss: 1.2264 - acc: 0.5463 - val_loss: 1.2636 - val_acc: 0.5095\n",
      "Epoch 644/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2258 - acc: 0.5465 - val_loss: 1.2687 - val_acc: 0.5037\n",
      "Epoch 645/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2260 - acc: 0.5448 - val_loss: 1.2768 - val_acc: 0.4993\n",
      "Epoch 646/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2266 - acc: 0.5447 - val_loss: 1.2650 - val_acc: 0.5037\n",
      "Epoch 647/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2251 - acc: 0.5442 - val_loss: 1.2676 - val_acc: 0.5022\n",
      "Epoch 648/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2248 - acc: 0.5479 - val_loss: 1.2704 - val_acc: 0.4993\n",
      "Epoch 649/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2263 - acc: 0.5391 - val_loss: 1.2739 - val_acc: 0.5051\n",
      "Epoch 650/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2242 - acc: 0.5442 - val_loss: 1.2650 - val_acc: 0.5110\n",
      "Epoch 651/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2246 - acc: 0.5424 - val_loss: 1.2693 - val_acc: 0.4993\n",
      "Epoch 652/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2246 - acc: 0.5409 - val_loss: 1.2647 - val_acc: 0.5110\n",
      "Epoch 653/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2240 - acc: 0.5497 - val_loss: 1.2654 - val_acc: 0.5037\n",
      "Epoch 654/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2232 - acc: 0.5440 - val_loss: 1.2639 - val_acc: 0.5037\n",
      "Epoch 655/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2233 - acc: 0.5457 - val_loss: 1.2637 - val_acc: 0.5140\n",
      "Epoch 656/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2244 - acc: 0.5409 - val_loss: 1.2637 - val_acc: 0.5095\n",
      "Epoch 657/1000\n",
      "6121/6121 [==============================] - 0s 30us/step - loss: 1.2232 - acc: 0.5455 - val_loss: 1.2655 - val_acc: 0.5037\n",
      "Epoch 658/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2227 - acc: 0.5432 - val_loss: 1.2634 - val_acc: 0.5095\n",
      "Epoch 659/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2232 - acc: 0.5439 - val_loss: 1.2697 - val_acc: 0.5037\n",
      "Epoch 660/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2231 - acc: 0.5466 - val_loss: 1.2632 - val_acc: 0.5110\n",
      "Epoch 661/1000\n",
      "6121/6121 [==============================] - 0s 30us/step - loss: 1.2245 - acc: 0.5444 - val_loss: 1.2624 - val_acc: 0.5140\n",
      "Epoch 662/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2225 - acc: 0.5463 - val_loss: 1.2745 - val_acc: 0.4949\n",
      "Epoch 663/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2212 - acc: 0.5448 - val_loss: 1.2697 - val_acc: 0.5022\n",
      "Epoch 664/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2213 - acc: 0.5445 - val_loss: 1.2733 - val_acc: 0.5007\n",
      "Epoch 665/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2208 - acc: 0.5496 - val_loss: 1.2663 - val_acc: 0.5184\n",
      "Epoch 666/1000\n",
      "6121/6121 [==============================] - 0s 30us/step - loss: 1.2205 - acc: 0.5466 - val_loss: 1.2690 - val_acc: 0.5066\n",
      "Epoch 667/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2213 - acc: 0.5468 - val_loss: 1.2614 - val_acc: 0.5066\n",
      "Epoch 668/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2202 - acc: 0.5489 - val_loss: 1.2717 - val_acc: 0.5037\n",
      "Epoch 669/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2235 - acc: 0.5462 - val_loss: 1.2677 - val_acc: 0.5051\n",
      "Epoch 670/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2216 - acc: 0.5484 - val_loss: 1.2701 - val_acc: 0.5066\n",
      "Epoch 671/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2200 - acc: 0.5466 - val_loss: 1.2628 - val_acc: 0.4993\n",
      "Epoch 672/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2203 - acc: 0.5484 - val_loss: 1.2628 - val_acc: 0.5140\n",
      "Epoch 673/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2203 - acc: 0.5481 - val_loss: 1.2613 - val_acc: 0.5022\n",
      "Epoch 674/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2189 - acc: 0.5486 - val_loss: 1.2634 - val_acc: 0.5037\n",
      "Epoch 675/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2199 - acc: 0.5476 - val_loss: 1.2594 - val_acc: 0.5140\n",
      "Epoch 676/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2198 - acc: 0.5471 - val_loss: 1.2639 - val_acc: 0.4993\n",
      "Epoch 677/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2185 - acc: 0.5458 - val_loss: 1.2605 - val_acc: 0.5110\n",
      "Epoch 678/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2183 - acc: 0.5463 - val_loss: 1.2650 - val_acc: 0.5037\n",
      "Epoch 679/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2184 - acc: 0.5483 - val_loss: 1.2605 - val_acc: 0.5095\n",
      "Epoch 680/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2179 - acc: 0.5489 - val_loss: 1.2637 - val_acc: 0.5037\n",
      "Epoch 681/1000\n",
      "6121/6121 [==============================] - 0s 33us/step - loss: 1.2174 - acc: 0.5473 - val_loss: 1.2608 - val_acc: 0.5140\n",
      "Epoch 682/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2186 - acc: 0.5473 - val_loss: 1.2615 - val_acc: 0.4978\n",
      "Epoch 683/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2184 - acc: 0.5481 - val_loss: 1.2616 - val_acc: 0.5022\n",
      "Epoch 684/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2193 - acc: 0.5499 - val_loss: 1.2580 - val_acc: 0.5169\n",
      "Epoch 685/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2176 - acc: 0.5488 - val_loss: 1.2604 - val_acc: 0.5154\n",
      "Epoch 686/1000\n",
      "6121/6121 [==============================] - 0s 33us/step - loss: 1.2182 - acc: 0.5511 - val_loss: 1.2618 - val_acc: 0.5066\n",
      "Epoch 687/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2159 - acc: 0.5476 - val_loss: 1.2634 - val_acc: 0.4949\n",
      "Epoch 688/1000\n",
      "6121/6121 [==============================] - 0s 33us/step - loss: 1.2161 - acc: 0.5481 - val_loss: 1.2723 - val_acc: 0.4963\n",
      "Epoch 689/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2154 - acc: 0.5494 - val_loss: 1.2661 - val_acc: 0.5154\n",
      "Epoch 690/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2162 - acc: 0.5488 - val_loss: 1.2589 - val_acc: 0.5125\n",
      "Epoch 691/1000\n",
      "6121/6121 [==============================] - 0s 33us/step - loss: 1.2153 - acc: 0.5506 - val_loss: 1.2688 - val_acc: 0.5007\n",
      "Epoch 692/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2163 - acc: 0.5479 - val_loss: 1.2611 - val_acc: 0.5022\n",
      "Epoch 693/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2160 - acc: 0.5493 - val_loss: 1.2639 - val_acc: 0.5007\n",
      "Epoch 694/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2154 - acc: 0.5507 - val_loss: 1.2602 - val_acc: 0.5140\n",
      "Epoch 695/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2143 - acc: 0.5481 - val_loss: 1.2625 - val_acc: 0.5022\n",
      "Epoch 696/1000\n",
      "6121/6121 [==============================] - 0s 30us/step - loss: 1.2148 - acc: 0.5468 - val_loss: 1.2789 - val_acc: 0.4993\n",
      "Epoch 697/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2159 - acc: 0.5462 - val_loss: 1.2615 - val_acc: 0.5140\n",
      "Epoch 698/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2154 - acc: 0.5463 - val_loss: 1.2574 - val_acc: 0.5110\n",
      "Epoch 699/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2154 - acc: 0.5491 - val_loss: 1.2606 - val_acc: 0.5198\n",
      "Epoch 700/1000\n",
      "6121/6121 [==============================] - 0s 33us/step - loss: 1.2146 - acc: 0.5478 - val_loss: 1.2592 - val_acc: 0.5095\n",
      "Epoch 701/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2133 - acc: 0.5504 - val_loss: 1.2613 - val_acc: 0.5066\n",
      "Epoch 702/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2118 - acc: 0.5481 - val_loss: 1.2686 - val_acc: 0.5051\n",
      "Epoch 703/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2156 - acc: 0.5468 - val_loss: 1.2664 - val_acc: 0.4934\n",
      "Epoch 704/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2136 - acc: 0.5514 - val_loss: 1.2638 - val_acc: 0.5051\n",
      "Epoch 705/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2133 - acc: 0.5457 - val_loss: 1.2582 - val_acc: 0.5110\n",
      "Epoch 706/1000\n",
      "6121/6121 [==============================] - 0s 33us/step - loss: 1.2127 - acc: 0.5493 - val_loss: 1.2596 - val_acc: 0.5037\n",
      "Epoch 707/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2132 - acc: 0.5493 - val_loss: 1.2621 - val_acc: 0.4978\n",
      "Epoch 708/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2110 - acc: 0.5512 - val_loss: 1.2630 - val_acc: 0.5051\n",
      "Epoch 709/1000\n",
      "6121/6121 [==============================] - 0s 33us/step - loss: 1.2129 - acc: 0.5488 - val_loss: 1.2585 - val_acc: 0.5125\n",
      "Epoch 710/1000\n",
      "6121/6121 [==============================] - 0s 33us/step - loss: 1.2133 - acc: 0.5462 - val_loss: 1.2572 - val_acc: 0.5022\n",
      "Epoch 711/1000\n",
      "6121/6121 [==============================] - 0s 33us/step - loss: 1.2113 - acc: 0.5486 - val_loss: 1.2620 - val_acc: 0.5081\n",
      "Epoch 712/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2112 - acc: 0.5471 - val_loss: 1.2572 - val_acc: 0.5095\n",
      "Epoch 713/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2120 - acc: 0.5494 - val_loss: 1.2552 - val_acc: 0.5198\n",
      "Epoch 714/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2107 - acc: 0.5506 - val_loss: 1.2584 - val_acc: 0.5110\n",
      "Epoch 715/1000\n",
      "6121/6121 [==============================] - 0s 33us/step - loss: 1.2106 - acc: 0.5514 - val_loss: 1.2568 - val_acc: 0.5081\n",
      "Epoch 716/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2112 - acc: 0.5471 - val_loss: 1.2544 - val_acc: 0.5022\n",
      "Epoch 717/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2108 - acc: 0.5494 - val_loss: 1.2624 - val_acc: 0.5140\n",
      "Epoch 718/1000\n",
      "6121/6121 [==============================] - 0s 30us/step - loss: 1.2116 - acc: 0.5491 - val_loss: 1.2537 - val_acc: 0.5184\n",
      "Epoch 719/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2094 - acc: 0.5529 - val_loss: 1.2624 - val_acc: 0.5066\n",
      "Epoch 720/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2106 - acc: 0.5483 - val_loss: 1.2544 - val_acc: 0.5140\n",
      "Epoch 721/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2089 - acc: 0.5497 - val_loss: 1.2586 - val_acc: 0.5213\n",
      "Epoch 722/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2095 - acc: 0.5507 - val_loss: 1.2559 - val_acc: 0.5125\n",
      "Epoch 723/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2080 - acc: 0.5507 - val_loss: 1.2581 - val_acc: 0.5110\n",
      "Epoch 724/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2090 - acc: 0.5501 - val_loss: 1.2619 - val_acc: 0.5081\n",
      "Epoch 725/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2094 - acc: 0.5489 - val_loss: 1.2628 - val_acc: 0.5051\n",
      "Epoch 726/1000\n",
      "6121/6121 [==============================] - 0s 33us/step - loss: 1.2108 - acc: 0.5504 - val_loss: 1.2571 - val_acc: 0.5110\n",
      "Epoch 727/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2087 - acc: 0.5514 - val_loss: 1.2554 - val_acc: 0.5051\n",
      "Epoch 728/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2091 - acc: 0.5511 - val_loss: 1.2564 - val_acc: 0.5110\n",
      "Epoch 729/1000\n",
      "6121/6121 [==============================] - 0s 33us/step - loss: 1.2085 - acc: 0.5506 - val_loss: 1.2559 - val_acc: 0.5184\n",
      "Epoch 730/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2080 - acc: 0.5514 - val_loss: 1.2582 - val_acc: 0.5198\n",
      "Epoch 731/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2073 - acc: 0.5525 - val_loss: 1.2592 - val_acc: 0.4919\n",
      "Epoch 732/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2081 - acc: 0.5486 - val_loss: 1.2551 - val_acc: 0.5022\n",
      "Epoch 733/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2081 - acc: 0.5476 - val_loss: 1.2566 - val_acc: 0.5066\n",
      "Epoch 734/1000\n",
      "6121/6121 [==============================] - 0s 33us/step - loss: 1.2083 - acc: 0.5471 - val_loss: 1.2611 - val_acc: 0.5228\n",
      "Epoch 735/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2085 - acc: 0.5509 - val_loss: 1.2570 - val_acc: 0.5110\n",
      "Epoch 736/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2065 - acc: 0.5507 - val_loss: 1.2553 - val_acc: 0.4978\n",
      "Epoch 737/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2061 - acc: 0.5553 - val_loss: 1.2564 - val_acc: 0.5154\n",
      "Epoch 738/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2070 - acc: 0.5517 - val_loss: 1.2547 - val_acc: 0.5140\n",
      "Epoch 739/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2058 - acc: 0.5527 - val_loss: 1.2578 - val_acc: 0.4993\n",
      "Epoch 740/1000\n",
      "6121/6121 [==============================] - 0s 33us/step - loss: 1.2065 - acc: 0.5502 - val_loss: 1.2610 - val_acc: 0.5051\n",
      "Epoch 741/1000\n",
      "6121/6121 [==============================] - 0s 33us/step - loss: 1.2059 - acc: 0.5509 - val_loss: 1.2538 - val_acc: 0.5051\n",
      "Epoch 742/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2056 - acc: 0.5525 - val_loss: 1.2537 - val_acc: 0.5169\n",
      "Epoch 743/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2060 - acc: 0.5517 - val_loss: 1.2546 - val_acc: 0.5066\n",
      "Epoch 744/1000\n",
      "6121/6121 [==============================] - 0s 33us/step - loss: 1.2053 - acc: 0.5540 - val_loss: 1.2541 - val_acc: 0.5154\n",
      "Epoch 745/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2065 - acc: 0.5491 - val_loss: 1.2546 - val_acc: 0.5037\n",
      "Epoch 746/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2062 - acc: 0.5527 - val_loss: 1.2502 - val_acc: 0.5228\n",
      "Epoch 747/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2065 - acc: 0.5499 - val_loss: 1.2541 - val_acc: 0.5184\n",
      "Epoch 748/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2046 - acc: 0.5519 - val_loss: 1.2500 - val_acc: 0.5125\n",
      "Epoch 749/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2058 - acc: 0.5489 - val_loss: 1.2591 - val_acc: 0.5154\n",
      "Epoch 750/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2033 - acc: 0.5520 - val_loss: 1.2515 - val_acc: 0.5022\n",
      "Epoch 751/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2044 - acc: 0.5532 - val_loss: 1.2554 - val_acc: 0.5081\n",
      "Epoch 752/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2048 - acc: 0.5520 - val_loss: 1.2649 - val_acc: 0.5007\n",
      "Epoch 753/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2055 - acc: 0.5484 - val_loss: 1.2560 - val_acc: 0.5154\n",
      "Epoch 754/1000\n",
      "6121/6121 [==============================] - 0s 33us/step - loss: 1.2026 - acc: 0.5504 - val_loss: 1.2514 - val_acc: 0.5228\n",
      "Epoch 755/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2044 - acc: 0.5540 - val_loss: 1.2569 - val_acc: 0.5007\n",
      "Epoch 756/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2024 - acc: 0.5520 - val_loss: 1.2551 - val_acc: 0.5242\n",
      "Epoch 757/1000\n",
      "6121/6121 [==============================] - 0s 34us/step - loss: 1.2056 - acc: 0.5489 - val_loss: 1.2591 - val_acc: 0.5066\n",
      "Epoch 758/1000\n",
      "6121/6121 [==============================] - 0s 33us/step - loss: 1.2036 - acc: 0.5535 - val_loss: 1.2546 - val_acc: 0.5125\n",
      "Epoch 759/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2027 - acc: 0.5543 - val_loss: 1.2590 - val_acc: 0.5169\n",
      "Epoch 760/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2033 - acc: 0.5502 - val_loss: 1.2549 - val_acc: 0.5125\n",
      "Epoch 761/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2028 - acc: 0.5546 - val_loss: 1.2500 - val_acc: 0.5140\n",
      "Epoch 762/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2042 - acc: 0.5512 - val_loss: 1.2641 - val_acc: 0.4963\n",
      "Epoch 763/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2047 - acc: 0.5524 - val_loss: 1.2519 - val_acc: 0.5213\n",
      "Epoch 764/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2025 - acc: 0.5520 - val_loss: 1.2504 - val_acc: 0.5257\n",
      "Epoch 765/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2017 - acc: 0.5457 - val_loss: 1.2631 - val_acc: 0.5140\n",
      "Epoch 766/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2014 - acc: 0.5529 - val_loss: 1.2552 - val_acc: 0.5154\n",
      "Epoch 767/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2009 - acc: 0.5504 - val_loss: 1.2471 - val_acc: 0.5066\n",
      "Epoch 768/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2020 - acc: 0.5522 - val_loss: 1.2554 - val_acc: 0.5198\n",
      "Epoch 769/1000\n",
      "6121/6121 [==============================] - 0s 33us/step - loss: 1.2019 - acc: 0.5530 - val_loss: 1.2485 - val_acc: 0.5154\n",
      "Epoch 770/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2008 - acc: 0.5527 - val_loss: 1.2541 - val_acc: 0.5095\n",
      "Epoch 771/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2005 - acc: 0.5535 - val_loss: 1.2518 - val_acc: 0.5110\n",
      "Epoch 772/1000\n",
      "6121/6121 [==============================] - 0s 30us/step - loss: 1.2031 - acc: 0.5520 - val_loss: 1.2506 - val_acc: 0.5169\n",
      "Epoch 773/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2006 - acc: 0.5494 - val_loss: 1.2483 - val_acc: 0.5257\n",
      "Epoch 774/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2004 - acc: 0.5497 - val_loss: 1.2560 - val_acc: 0.5184\n",
      "Epoch 775/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.1998 - acc: 0.5542 - val_loss: 1.2465 - val_acc: 0.5228\n",
      "Epoch 776/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.1998 - acc: 0.5550 - val_loss: 1.2596 - val_acc: 0.5081\n",
      "Epoch 777/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2015 - acc: 0.5542 - val_loss: 1.2481 - val_acc: 0.5095\n",
      "Epoch 778/1000\n",
      "6121/6121 [==============================] - 0s 33us/step - loss: 1.1999 - acc: 0.5556 - val_loss: 1.2597 - val_acc: 0.5081\n",
      "Epoch 779/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2001 - acc: 0.5499 - val_loss: 1.2508 - val_acc: 0.5140\n",
      "Epoch 780/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.2000 - acc: 0.5543 - val_loss: 1.2554 - val_acc: 0.5051\n",
      "Epoch 781/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.1990 - acc: 0.5597 - val_loss: 1.2520 - val_acc: 0.5110\n",
      "Epoch 782/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.1986 - acc: 0.5569 - val_loss: 1.2650 - val_acc: 0.5037\n",
      "Epoch 783/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.1997 - acc: 0.5478 - val_loss: 1.2481 - val_acc: 0.5154\n",
      "Epoch 784/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.1989 - acc: 0.5540 - val_loss: 1.2560 - val_acc: 0.5140\n",
      "Epoch 785/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.1991 - acc: 0.5524 - val_loss: 1.2493 - val_acc: 0.4993\n",
      "Epoch 786/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.1974 - acc: 0.5545 - val_loss: 1.2553 - val_acc: 0.5154\n",
      "Epoch 787/1000\n",
      "6121/6121 [==============================] - 0s 33us/step - loss: 1.1993 - acc: 0.5525 - val_loss: 1.2502 - val_acc: 0.5125\n",
      "Epoch 788/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.1983 - acc: 0.5533 - val_loss: 1.2475 - val_acc: 0.5242\n",
      "Epoch 789/1000\n",
      "6121/6121 [==============================] - 0s 33us/step - loss: 1.1982 - acc: 0.5532 - val_loss: 1.2476 - val_acc: 0.5110\n",
      "Epoch 790/1000\n",
      "6121/6121 [==============================] - 0s 33us/step - loss: 1.1975 - acc: 0.5532 - val_loss: 1.2538 - val_acc: 0.5037\n",
      "Epoch 791/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.1997 - acc: 0.5548 - val_loss: 1.2487 - val_acc: 0.5095\n",
      "Epoch 792/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.1973 - acc: 0.5540 - val_loss: 1.2533 - val_acc: 0.5110\n",
      "Epoch 793/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.1976 - acc: 0.5517 - val_loss: 1.2510 - val_acc: 0.5198\n",
      "Epoch 794/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.1972 - acc: 0.5540 - val_loss: 1.2676 - val_acc: 0.4963\n",
      "Epoch 795/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.1998 - acc: 0.5507 - val_loss: 1.2577 - val_acc: 0.5110\n",
      "Epoch 796/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.1963 - acc: 0.5509 - val_loss: 1.2483 - val_acc: 0.5257\n",
      "Epoch 797/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.1977 - acc: 0.5514 - val_loss: 1.2454 - val_acc: 0.5198\n",
      "Epoch 798/1000\n",
      "6121/6121 [==============================] - 0s 30us/step - loss: 1.1977 - acc: 0.5542 - val_loss: 1.2513 - val_acc: 0.5213\n",
      "Epoch 799/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.1967 - acc: 0.5496 - val_loss: 1.2616 - val_acc: 0.5022\n",
      "Epoch 800/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.1971 - acc: 0.5566 - val_loss: 1.2503 - val_acc: 0.5140\n",
      "Epoch 801/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.1956 - acc: 0.5509 - val_loss: 1.2508 - val_acc: 0.5242\n",
      "Epoch 802/1000\n",
      "6121/6121 [==============================] - 0s 33us/step - loss: 1.1946 - acc: 0.5543 - val_loss: 1.2579 - val_acc: 0.5095\n",
      "Epoch 803/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.1969 - acc: 0.5542 - val_loss: 1.2459 - val_acc: 0.5125\n",
      "Epoch 804/1000\n",
      "6121/6121 [==============================] - 0s 30us/step - loss: 1.1944 - acc: 0.5524 - val_loss: 1.2492 - val_acc: 0.4993\n",
      "Epoch 805/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.1962 - acc: 0.5524 - val_loss: 1.2510 - val_acc: 0.5169\n",
      "Epoch 806/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.1968 - acc: 0.5527 - val_loss: 1.2488 - val_acc: 0.5095\n",
      "Epoch 807/1000\n",
      "6121/6121 [==============================] - 0s 33us/step - loss: 1.1970 - acc: 0.5504 - val_loss: 1.2446 - val_acc: 0.5184\n",
      "Epoch 808/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.1938 - acc: 0.5551 - val_loss: 1.2514 - val_acc: 0.5198\n",
      "Epoch 809/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.1945 - acc: 0.5506 - val_loss: 1.2566 - val_acc: 0.5110\n",
      "Epoch 810/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.1937 - acc: 0.5551 - val_loss: 1.2428 - val_acc: 0.5081\n",
      "Epoch 811/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.1932 - acc: 0.5538 - val_loss: 1.2660 - val_acc: 0.5007\n",
      "Epoch 812/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.1941 - acc: 0.5506 - val_loss: 1.2477 - val_acc: 0.4993\n",
      "Epoch 813/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.1947 - acc: 0.5501 - val_loss: 1.2483 - val_acc: 0.5213\n",
      "Epoch 814/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.1939 - acc: 0.5568 - val_loss: 1.2476 - val_acc: 0.5198\n",
      "Epoch 815/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.1940 - acc: 0.5538 - val_loss: 1.2467 - val_acc: 0.5286\n",
      "Epoch 816/1000\n",
      "6121/6121 [==============================] - 0s 33us/step - loss: 1.1947 - acc: 0.5530 - val_loss: 1.2431 - val_acc: 0.5286\n",
      "Epoch 817/1000\n",
      "6121/6121 [==============================] - 0s 33us/step - loss: 1.1927 - acc: 0.5558 - val_loss: 1.2455 - val_acc: 0.5110\n",
      "Epoch 818/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.1932 - acc: 0.5543 - val_loss: 1.2436 - val_acc: 0.5110\n",
      "Epoch 819/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.1947 - acc: 0.5561 - val_loss: 1.2509 - val_acc: 0.5095\n",
      "Epoch 820/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.1931 - acc: 0.5551 - val_loss: 1.2504 - val_acc: 0.5081\n",
      "Epoch 821/1000\n",
      "6121/6121 [==============================] - 0s 33us/step - loss: 1.1935 - acc: 0.5530 - val_loss: 1.2629 - val_acc: 0.4978\n",
      "Epoch 822/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.1948 - acc: 0.5535 - val_loss: 1.2436 - val_acc: 0.5125\n",
      "Epoch 823/1000\n",
      "6121/6121 [==============================] - 0s 33us/step - loss: 1.1920 - acc: 0.5566 - val_loss: 1.2481 - val_acc: 0.5198\n",
      "Epoch 824/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.1917 - acc: 0.5566 - val_loss: 1.2419 - val_acc: 0.5095\n",
      "Epoch 825/1000\n",
      "6121/6121 [==============================] - 0s 30us/step - loss: 1.1942 - acc: 0.5506 - val_loss: 1.2504 - val_acc: 0.5140\n",
      "Epoch 826/1000\n",
      "6121/6121 [==============================] - 0s 30us/step - loss: 1.1923 - acc: 0.5532 - val_loss: 1.2502 - val_acc: 0.5081\n",
      "Epoch 827/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.1920 - acc: 0.5569 - val_loss: 1.2464 - val_acc: 0.5228\n",
      "Epoch 828/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.1918 - acc: 0.5558 - val_loss: 1.2496 - val_acc: 0.5154\n",
      "Epoch 829/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.1905 - acc: 0.5556 - val_loss: 1.2466 - val_acc: 0.5022\n",
      "Epoch 830/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.1902 - acc: 0.5599 - val_loss: 1.2442 - val_acc: 0.5110\n",
      "Epoch 831/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.1926 - acc: 0.5533 - val_loss: 1.2540 - val_acc: 0.5022\n",
      "Epoch 832/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.1934 - acc: 0.5545 - val_loss: 1.2522 - val_acc: 0.5140\n",
      "Epoch 833/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.1924 - acc: 0.5576 - val_loss: 1.2462 - val_acc: 0.5228\n",
      "Epoch 834/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.1918 - acc: 0.5551 - val_loss: 1.2461 - val_acc: 0.5169\n",
      "Epoch 835/1000\n",
      "6121/6121 [==============================] - 0s 30us/step - loss: 1.1903 - acc: 0.5550 - val_loss: 1.2402 - val_acc: 0.5198\n",
      "Epoch 836/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.1907 - acc: 0.5579 - val_loss: 1.2476 - val_acc: 0.5125\n",
      "Epoch 837/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.1902 - acc: 0.5560 - val_loss: 1.2472 - val_acc: 0.5140\n",
      "Epoch 838/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.1899 - acc: 0.5527 - val_loss: 1.2450 - val_acc: 0.5140\n",
      "Epoch 839/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.1906 - acc: 0.5550 - val_loss: 1.2462 - val_acc: 0.5140\n",
      "Epoch 840/1000\n",
      "6121/6121 [==============================] - 0s 30us/step - loss: 1.1886 - acc: 0.5543 - val_loss: 1.2493 - val_acc: 0.5213\n",
      "Epoch 841/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.1893 - acc: 0.5576 - val_loss: 1.2424 - val_acc: 0.5110\n",
      "Epoch 842/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.1901 - acc: 0.5561 - val_loss: 1.2508 - val_acc: 0.5140\n",
      "Epoch 843/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.1896 - acc: 0.5566 - val_loss: 1.2485 - val_acc: 0.5066\n",
      "Epoch 844/1000\n",
      "6121/6121 [==============================] - 0s 34us/step - loss: 1.1897 - acc: 0.5560 - val_loss: 1.2444 - val_acc: 0.5242\n",
      "Epoch 845/1000\n",
      "6121/6121 [==============================] - 0s 30us/step - loss: 1.1897 - acc: 0.5568 - val_loss: 1.2482 - val_acc: 0.5095\n",
      "Epoch 846/1000\n",
      "6121/6121 [==============================] - 0s 30us/step - loss: 1.1885 - acc: 0.5551 - val_loss: 1.2440 - val_acc: 0.5169\n",
      "Epoch 847/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.1881 - acc: 0.5555 - val_loss: 1.2432 - val_acc: 0.5242\n",
      "Epoch 848/1000\n",
      "6121/6121 [==============================] - 0s 33us/step - loss: 1.1877 - acc: 0.5599 - val_loss: 1.2502 - val_acc: 0.5169\n",
      "Epoch 849/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.1893 - acc: 0.5550 - val_loss: 1.2444 - val_acc: 0.5051\n",
      "Epoch 850/1000\n",
      "6121/6121 [==============================] - 0s 33us/step - loss: 1.1888 - acc: 0.5581 - val_loss: 1.2416 - val_acc: 0.5228\n",
      "Epoch 851/1000\n",
      "6121/6121 [==============================] - 0s 33us/step - loss: 1.1894 - acc: 0.5558 - val_loss: 1.2390 - val_acc: 0.5213\n",
      "Epoch 852/1000\n",
      "6121/6121 [==============================] - 0s 33us/step - loss: 1.1876 - acc: 0.5574 - val_loss: 1.2404 - val_acc: 0.5198\n",
      "Epoch 853/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.1874 - acc: 0.5563 - val_loss: 1.2524 - val_acc: 0.5095\n",
      "Epoch 854/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.1890 - acc: 0.5582 - val_loss: 1.2426 - val_acc: 0.5066\n",
      "Epoch 855/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.1870 - acc: 0.5564 - val_loss: 1.2388 - val_acc: 0.5169\n",
      "Epoch 856/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.1869 - acc: 0.5561 - val_loss: 1.2458 - val_acc: 0.5184\n",
      "Epoch 857/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.1873 - acc: 0.5566 - val_loss: 1.2475 - val_acc: 0.5301\n",
      "Epoch 858/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.1874 - acc: 0.5560 - val_loss: 1.2403 - val_acc: 0.5169\n",
      "Epoch 859/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.1867 - acc: 0.5563 - val_loss: 1.2388 - val_acc: 0.5198\n",
      "Epoch 860/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.1877 - acc: 0.5553 - val_loss: 1.2410 - val_acc: 0.5330\n",
      "Epoch 861/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.1859 - acc: 0.5582 - val_loss: 1.2527 - val_acc: 0.4978\n",
      "Epoch 862/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.1888 - acc: 0.5553 - val_loss: 1.2418 - val_acc: 0.5301\n",
      "Epoch 863/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.1860 - acc: 0.5581 - val_loss: 1.2444 - val_acc: 0.5198\n",
      "Epoch 864/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.1880 - acc: 0.5594 - val_loss: 1.2429 - val_acc: 0.5125\n",
      "Epoch 865/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.1850 - acc: 0.5556 - val_loss: 1.2476 - val_acc: 0.5169\n",
      "Epoch 866/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.1863 - acc: 0.5579 - val_loss: 1.2465 - val_acc: 0.5066\n",
      "Epoch 867/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.1857 - acc: 0.5543 - val_loss: 1.2427 - val_acc: 0.5095\n",
      "Epoch 868/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.1860 - acc: 0.5555 - val_loss: 1.2399 - val_acc: 0.5125\n",
      "Epoch 869/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.1849 - acc: 0.5573 - val_loss: 1.2440 - val_acc: 0.5301\n",
      "Epoch 870/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.1851 - acc: 0.5589 - val_loss: 1.2427 - val_acc: 0.5228\n",
      "Epoch 871/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.1856 - acc: 0.5566 - val_loss: 1.2422 - val_acc: 0.5213\n",
      "Epoch 872/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.1843 - acc: 0.5542 - val_loss: 1.2391 - val_acc: 0.5198\n",
      "Epoch 873/1000\n",
      "6121/6121 [==============================] - 0s 34us/step - loss: 1.1852 - acc: 0.5568 - val_loss: 1.2402 - val_acc: 0.5272\n",
      "Epoch 874/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.1863 - acc: 0.5584 - val_loss: 1.2410 - val_acc: 0.5228\n",
      "Epoch 875/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.1864 - acc: 0.5555 - val_loss: 1.2407 - val_acc: 0.5184\n",
      "Epoch 876/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.1846 - acc: 0.5594 - val_loss: 1.2447 - val_acc: 0.5286\n",
      "Epoch 877/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.1846 - acc: 0.5589 - val_loss: 1.2468 - val_acc: 0.5154\n",
      "Epoch 878/1000\n",
      "6121/6121 [==============================] - 0s 33us/step - loss: 1.1868 - acc: 0.5573 - val_loss: 1.2390 - val_acc: 0.5125\n",
      "Epoch 879/1000\n",
      "6121/6121 [==============================] - 0s 33us/step - loss: 1.1837 - acc: 0.5612 - val_loss: 1.2448 - val_acc: 0.5272\n",
      "Epoch 880/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.1842 - acc: 0.5558 - val_loss: 1.2383 - val_acc: 0.5140\n",
      "Epoch 881/1000\n",
      "6121/6121 [==============================] - 0s 30us/step - loss: 1.1855 - acc: 0.5553 - val_loss: 1.2393 - val_acc: 0.5198\n",
      "Epoch 882/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.1838 - acc: 0.5556 - val_loss: 1.2422 - val_acc: 0.5081\n",
      "Epoch 883/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.1851 - acc: 0.5560 - val_loss: 1.2406 - val_acc: 0.5184\n",
      "Epoch 884/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.1830 - acc: 0.5599 - val_loss: 1.2410 - val_acc: 0.5154\n",
      "Epoch 885/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.1829 - acc: 0.5564 - val_loss: 1.2389 - val_acc: 0.5066\n",
      "Epoch 886/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.1833 - acc: 0.5599 - val_loss: 1.2409 - val_acc: 0.5198\n",
      "Epoch 887/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.1834 - acc: 0.5573 - val_loss: 1.2445 - val_acc: 0.5110\n",
      "Epoch 888/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.1824 - acc: 0.5609 - val_loss: 1.2504 - val_acc: 0.5022\n",
      "Epoch 889/1000\n",
      "6121/6121 [==============================] - 0s 30us/step - loss: 1.1828 - acc: 0.5581 - val_loss: 1.2427 - val_acc: 0.5169\n",
      "Epoch 890/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.1832 - acc: 0.5564 - val_loss: 1.2473 - val_acc: 0.5081\n",
      "Epoch 891/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.1831 - acc: 0.5589 - val_loss: 1.2435 - val_acc: 0.5051\n",
      "Epoch 892/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.1831 - acc: 0.5581 - val_loss: 1.2362 - val_acc: 0.5198\n",
      "Epoch 893/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.1813 - acc: 0.5604 - val_loss: 1.2386 - val_acc: 0.5374\n",
      "Epoch 894/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.1820 - acc: 0.5592 - val_loss: 1.2480 - val_acc: 0.5140\n",
      "Epoch 895/1000\n",
      "6121/6121 [==============================] - 0s 33us/step - loss: 1.1810 - acc: 0.5597 - val_loss: 1.2376 - val_acc: 0.5228\n",
      "Epoch 896/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.1818 - acc: 0.5587 - val_loss: 1.2442 - val_acc: 0.5066\n",
      "Epoch 897/1000\n",
      "6121/6121 [==============================] - 0s 33us/step - loss: 1.1819 - acc: 0.5586 - val_loss: 1.2407 - val_acc: 0.5140\n",
      "Epoch 898/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.1828 - acc: 0.5617 - val_loss: 1.2388 - val_acc: 0.5125\n",
      "Epoch 899/1000\n",
      "6121/6121 [==============================] - 0s 30us/step - loss: 1.1822 - acc: 0.5612 - val_loss: 1.2383 - val_acc: 0.5213\n",
      "Epoch 900/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.1804 - acc: 0.5591 - val_loss: 1.2447 - val_acc: 0.5184\n",
      "Epoch 901/1000\n",
      "6121/6121 [==============================] - 0s 33us/step - loss: 1.1819 - acc: 0.5597 - val_loss: 1.2507 - val_acc: 0.5184\n",
      "Epoch 902/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.1831 - acc: 0.5511 - val_loss: 1.2353 - val_acc: 0.5198\n",
      "Epoch 903/1000\n",
      "6121/6121 [==============================] - 0s 33us/step - loss: 1.1812 - acc: 0.5591 - val_loss: 1.2357 - val_acc: 0.5213\n",
      "Epoch 904/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.1811 - acc: 0.5612 - val_loss: 1.2398 - val_acc: 0.5169\n",
      "Epoch 905/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.1815 - acc: 0.5623 - val_loss: 1.2367 - val_acc: 0.5257\n",
      "Epoch 906/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.1795 - acc: 0.5597 - val_loss: 1.2539 - val_acc: 0.5066\n",
      "Epoch 907/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.1818 - acc: 0.5581 - val_loss: 1.2346 - val_acc: 0.5213\n",
      "Epoch 908/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.1800 - acc: 0.5607 - val_loss: 1.2400 - val_acc: 0.5184\n",
      "Epoch 909/1000\n",
      "6121/6121 [==============================] - 0s 30us/step - loss: 1.1808 - acc: 0.5576 - val_loss: 1.2404 - val_acc: 0.5110\n",
      "Epoch 910/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.1808 - acc: 0.5581 - val_loss: 1.2375 - val_acc: 0.5257\n",
      "Epoch 911/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.1797 - acc: 0.5579 - val_loss: 1.2358 - val_acc: 0.5110\n",
      "Epoch 912/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.1805 - acc: 0.5622 - val_loss: 1.2368 - val_acc: 0.5198\n",
      "Epoch 913/1000\n",
      "6121/6121 [==============================] - 0s 33us/step - loss: 1.1791 - acc: 0.5581 - val_loss: 1.2484 - val_acc: 0.5184\n",
      "Epoch 914/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.1809 - acc: 0.5537 - val_loss: 1.2459 - val_acc: 0.5140\n",
      "Epoch 915/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.1806 - acc: 0.5640 - val_loss: 1.2429 - val_acc: 0.5095\n",
      "Epoch 916/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.1781 - acc: 0.5609 - val_loss: 1.2391 - val_acc: 0.5272\n",
      "Epoch 917/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.1800 - acc: 0.5573 - val_loss: 1.2440 - val_acc: 0.5272\n",
      "Epoch 918/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.1798 - acc: 0.5574 - val_loss: 1.2416 - val_acc: 0.5095\n",
      "Epoch 919/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.1784 - acc: 0.5591 - val_loss: 1.2487 - val_acc: 0.4993\n",
      "Epoch 920/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.1797 - acc: 0.5573 - val_loss: 1.2421 - val_acc: 0.5154\n",
      "Epoch 921/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.1784 - acc: 0.5599 - val_loss: 1.2331 - val_acc: 0.5272\n",
      "Epoch 922/1000\n",
      "6121/6121 [==============================] - 0s 30us/step - loss: 1.1774 - acc: 0.5587 - val_loss: 1.2348 - val_acc: 0.5213\n",
      "Epoch 923/1000\n",
      "6121/6121 [==============================] - 0s 33us/step - loss: 1.1782 - acc: 0.5605 - val_loss: 1.2406 - val_acc: 0.5066\n",
      "Epoch 924/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.1772 - acc: 0.5586 - val_loss: 1.2419 - val_acc: 0.5272\n",
      "Epoch 925/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.1782 - acc: 0.5636 - val_loss: 1.2356 - val_acc: 0.5198\n",
      "Epoch 926/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.1803 - acc: 0.5615 - val_loss: 1.2354 - val_acc: 0.5081\n",
      "Epoch 927/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.1791 - acc: 0.5640 - val_loss: 1.2487 - val_acc: 0.5154\n",
      "Epoch 928/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.1779 - acc: 0.5643 - val_loss: 1.2389 - val_acc: 0.5066\n",
      "Epoch 929/1000\n",
      "6121/6121 [==============================] - 0s 33us/step - loss: 1.1772 - acc: 0.5641 - val_loss: 1.2352 - val_acc: 0.5140\n",
      "Epoch 930/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.1770 - acc: 0.5576 - val_loss: 1.2379 - val_acc: 0.5140\n",
      "Epoch 931/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.1768 - acc: 0.5612 - val_loss: 1.2499 - val_acc: 0.5125\n",
      "Epoch 932/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.1768 - acc: 0.5602 - val_loss: 1.2393 - val_acc: 0.5228\n",
      "Epoch 933/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.1770 - acc: 0.5591 - val_loss: 1.2392 - val_acc: 0.5272\n",
      "Epoch 934/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.1764 - acc: 0.5609 - val_loss: 1.2395 - val_acc: 0.5154\n",
      "Epoch 935/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.1775 - acc: 0.5594 - val_loss: 1.2408 - val_acc: 0.5081\n",
      "Epoch 936/1000\n",
      "6121/6121 [==============================] - 0s 30us/step - loss: 1.1775 - acc: 0.5622 - val_loss: 1.2396 - val_acc: 0.5051\n",
      "Epoch 937/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.1769 - acc: 0.5595 - val_loss: 1.2323 - val_acc: 0.5228\n",
      "Epoch 938/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.1759 - acc: 0.5612 - val_loss: 1.2375 - val_acc: 0.5184\n",
      "Epoch 939/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.1741 - acc: 0.5625 - val_loss: 1.2390 - val_acc: 0.5345\n",
      "Epoch 940/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.1775 - acc: 0.5617 - val_loss: 1.2400 - val_acc: 0.5125\n",
      "Epoch 941/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.1756 - acc: 0.5615 - val_loss: 1.2354 - val_acc: 0.5389\n",
      "Epoch 942/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.1752 - acc: 0.5630 - val_loss: 1.2328 - val_acc: 0.5184\n",
      "Epoch 943/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.1756 - acc: 0.5623 - val_loss: 1.2388 - val_acc: 0.5154\n",
      "Epoch 944/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.1741 - acc: 0.5633 - val_loss: 1.2421 - val_acc: 0.5184\n",
      "Epoch 945/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.1774 - acc: 0.5566 - val_loss: 1.2393 - val_acc: 0.5228\n",
      "Epoch 946/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.1763 - acc: 0.5594 - val_loss: 1.2410 - val_acc: 0.5257\n",
      "Epoch 947/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.1740 - acc: 0.5627 - val_loss: 1.2386 - val_acc: 0.5242\n",
      "Epoch 948/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.1768 - acc: 0.5617 - val_loss: 1.2356 - val_acc: 0.5184\n",
      "Epoch 949/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.1740 - acc: 0.5658 - val_loss: 1.2406 - val_acc: 0.5095\n",
      "Epoch 950/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.1754 - acc: 0.5602 - val_loss: 1.2327 - val_acc: 0.5213\n",
      "Epoch 951/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.1765 - acc: 0.5604 - val_loss: 1.2329 - val_acc: 0.5228\n",
      "Epoch 952/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.1737 - acc: 0.5630 - val_loss: 1.2352 - val_acc: 0.5301\n",
      "Epoch 953/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.1747 - acc: 0.5620 - val_loss: 1.2391 - val_acc: 0.5110\n",
      "Epoch 954/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.1761 - acc: 0.5591 - val_loss: 1.2322 - val_acc: 0.5184\n",
      "Epoch 955/1000\n",
      "6121/6121 [==============================] - 0s 30us/step - loss: 1.1753 - acc: 0.5610 - val_loss: 1.2514 - val_acc: 0.5037\n",
      "Epoch 956/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.1742 - acc: 0.5628 - val_loss: 1.2362 - val_acc: 0.5066\n",
      "Epoch 957/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.1740 - acc: 0.5610 - val_loss: 1.2403 - val_acc: 0.5286\n",
      "Epoch 958/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.1742 - acc: 0.5597 - val_loss: 1.2361 - val_acc: 0.5228\n",
      "Epoch 959/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.1768 - acc: 0.5605 - val_loss: 1.2321 - val_acc: 0.5228\n",
      "Epoch 960/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.1736 - acc: 0.5643 - val_loss: 1.2441 - val_acc: 0.5095\n",
      "Epoch 961/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.1736 - acc: 0.5587 - val_loss: 1.2362 - val_acc: 0.5140\n",
      "Epoch 962/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.1746 - acc: 0.5586 - val_loss: 1.2317 - val_acc: 0.5213\n",
      "Epoch 963/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.1733 - acc: 0.5669 - val_loss: 1.2372 - val_acc: 0.5228\n",
      "Epoch 964/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.1738 - acc: 0.5600 - val_loss: 1.2351 - val_acc: 0.5198\n",
      "Epoch 965/1000\n",
      "6121/6121 [==============================] - 0s 33us/step - loss: 1.1727 - acc: 0.5617 - val_loss: 1.2321 - val_acc: 0.5257\n",
      "Epoch 966/1000\n",
      "6121/6121 [==============================] - 0s 30us/step - loss: 1.1723 - acc: 0.5622 - val_loss: 1.2326 - val_acc: 0.5198\n",
      "Epoch 967/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.1715 - acc: 0.5633 - val_loss: 1.2310 - val_acc: 0.5345\n",
      "Epoch 968/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.1734 - acc: 0.5604 - val_loss: 1.2350 - val_acc: 0.5257\n",
      "Epoch 969/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.1708 - acc: 0.5662 - val_loss: 1.2357 - val_acc: 0.5198\n",
      "Epoch 970/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.1724 - acc: 0.5633 - val_loss: 1.2321 - val_acc: 0.5257\n",
      "Epoch 971/1000\n",
      "6121/6121 [==============================] - 0s 37us/step - loss: 1.1746 - acc: 0.5607 - val_loss: 1.2372 - val_acc: 0.5125\n",
      "Epoch 972/1000\n",
      "6121/6121 [==============================] - 0s 39us/step - loss: 1.1729 - acc: 0.5615 - val_loss: 1.2331 - val_acc: 0.5154\n",
      "Epoch 973/1000\n",
      "6121/6121 [==============================] - 0s 39us/step - loss: 1.1732 - acc: 0.5645 - val_loss: 1.2358 - val_acc: 0.5081\n",
      "Epoch 974/1000\n",
      "6121/6121 [==============================] - 0s 39us/step - loss: 1.1719 - acc: 0.5648 - val_loss: 1.2345 - val_acc: 0.5301\n",
      "Epoch 975/1000\n",
      "6121/6121 [==============================] - 0s 35us/step - loss: 1.1714 - acc: 0.5630 - val_loss: 1.2382 - val_acc: 0.5110\n",
      "Epoch 976/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.1713 - acc: 0.5609 - val_loss: 1.2303 - val_acc: 0.5125\n",
      "Epoch 977/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.1724 - acc: 0.5625 - val_loss: 1.2312 - val_acc: 0.5242\n",
      "Epoch 978/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.1707 - acc: 0.5630 - val_loss: 1.2349 - val_acc: 0.5140\n",
      "Epoch 979/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.1721 - acc: 0.5636 - val_loss: 1.2349 - val_acc: 0.5125\n",
      "Epoch 980/1000\n",
      "6121/6121 [==============================] - 0s 33us/step - loss: 1.1703 - acc: 0.5640 - val_loss: 1.2312 - val_acc: 0.5316\n",
      "Epoch 981/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.1707 - acc: 0.5613 - val_loss: 1.2445 - val_acc: 0.5095\n",
      "Epoch 982/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.1739 - acc: 0.5643 - val_loss: 1.2314 - val_acc: 0.5198\n",
      "Epoch 983/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.1709 - acc: 0.5607 - val_loss: 1.2366 - val_acc: 0.5198\n",
      "Epoch 984/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.1705 - acc: 0.5656 - val_loss: 1.2333 - val_acc: 0.5301\n",
      "Epoch 985/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.1716 - acc: 0.5662 - val_loss: 1.2357 - val_acc: 0.5066\n",
      "Epoch 986/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.1701 - acc: 0.5618 - val_loss: 1.2315 - val_acc: 0.5140\n",
      "Epoch 987/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.1695 - acc: 0.5640 - val_loss: 1.2282 - val_acc: 0.5272\n",
      "Epoch 988/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.1696 - acc: 0.5667 - val_loss: 1.2361 - val_acc: 0.5374\n",
      "Epoch 989/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.1698 - acc: 0.5623 - val_loss: 1.2304 - val_acc: 0.5198\n",
      "Epoch 990/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.1700 - acc: 0.5641 - val_loss: 1.2364 - val_acc: 0.5125\n",
      "Epoch 991/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.1709 - acc: 0.5651 - val_loss: 1.2332 - val_acc: 0.5286\n",
      "Epoch 992/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.1699 - acc: 0.5627 - val_loss: 1.2370 - val_acc: 0.5228\n",
      "Epoch 993/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.1692 - acc: 0.5651 - val_loss: 1.2306 - val_acc: 0.5184\n",
      "Epoch 994/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.1726 - acc: 0.5631 - val_loss: 1.2271 - val_acc: 0.5184\n",
      "Epoch 995/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.1693 - acc: 0.5618 - val_loss: 1.2329 - val_acc: 0.5110\n",
      "Epoch 996/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.1693 - acc: 0.5641 - val_loss: 1.2335 - val_acc: 0.5228\n",
      "Epoch 997/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.1691 - acc: 0.5641 - val_loss: 1.2315 - val_acc: 0.5169\n",
      "Epoch 998/1000\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.1691 - acc: 0.5605 - val_loss: 1.2407 - val_acc: 0.5242\n",
      "Epoch 999/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.1683 - acc: 0.5638 - val_loss: 1.2288 - val_acc: 0.5286\n",
      "Epoch 1000/1000\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.1697 - acc: 0.5617 - val_loss: 1.2341 - val_acc: 0.5228\n"
     ]
    }
   ],
   "source": [
    "model_history = model.fit(x=x_train, y=y_train,\n",
    "                          batch_size=batch_size,\n",
    "                          epochs=epochs,\n",
    "                          validation_split=1 - train_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = model_history.history['loss']\n",
    "train_acc = model_history.history['acc']\n",
    "valid_loss = model_history.history['val_loss']\n",
    "valid_acc = model_history.history['val_acc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_loss, 'b', label='train')\n",
    "plt.plot(valid_loss, 'r', label='valid')\n",
    "plt.legend()\n",
    "plt.title(\"Loss\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(train_acc, 'b', label='train')\n",
    "plt.plot(valid_acc, 'r', label='valid')\n",
    "plt.legend(loc=4)\n",
    "plt.title(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = model.predict_classes(x_test)\n",
    "y_test_label = np.argmax(y_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy in testing data: 0.505586592178771\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy in testing data:', accuracy_score(y_test_label, y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 5, 2, 0, 5, 5, 3, 3, 2, 4, 2, 5, 5, 3, 5, 2, 2, 3, 2, 2, 1, 0,\n",
       "       2, 0, 3, 5, 0, 4, 0, 4, 5, 0, 5, 0, 1, 1, 0, 0, 5, 5, 0, 4, 5, 2,\n",
       "       3, 5, 5, 5, 3, 5, 1, 4, 0, 0, 0, 5, 5, 0, 0, 3, 5, 3, 0, 5, 5, 0,\n",
       "       5, 5, 3, 4, 1, 5, 1, 5, 5, 5, 2, 0, 4, 0, 0, 2, 5, 2, 2, 5, 4, 5,\n",
       "       0, 5, 2, 0, 2, 5, 0, 2, 5, 2, 5, 3, 5, 4, 4, 2, 5, 0, 3, 2, 2, 5,\n",
       "       2, 5, 0, 0, 0, 2, 5, 4, 0, 4, 5, 5, 2, 0, 0, 4, 5, 2, 5, 1, 5, 3,\n",
       "       5, 2, 2, 2, 5, 0, 5, 2, 1, 0, 3, 0, 4, 1, 0, 0, 0, 2, 5, 3, 1, 5,\n",
       "       3, 2, 2, 0, 2, 2, 5, 5, 5, 0, 0, 5, 0, 1, 0, 2, 1, 4, 5, 5, 4, 5,\n",
       "       4, 2, 2, 3, 5, 0, 4, 5, 0, 4, 4, 0, 0, 5, 5, 5, 2, 0, 1, 0, 2, 1,\n",
       "       1, 5, 2, 2, 0, 4, 5, 4, 2, 1, 0, 5, 5, 5, 2, 3, 0, 1, 3, 5, 4, 2,\n",
       "       3, 5, 2, 5, 2, 2, 5, 2, 5, 4, 5, 3, 0, 0, 2, 5, 0, 2, 3, 2, 5, 0,\n",
       "       2, 0, 0, 5, 2, 2, 0, 3, 5, 3, 2, 4, 2, 5, 4, 0, 4, 0, 5, 0, 5, 2,\n",
       "       0, 0, 3, 0, 0, 1, 0, 0, 5, 0, 0, 5, 5, 3, 0, 1, 5, 0, 0, 0, 1, 0,\n",
       "       2, 2, 4, 2, 0, 2, 0, 4, 2, 5, 5, 0, 5, 2, 5, 5, 0, 5, 2, 3, 1, 5,\n",
       "       5, 2, 3, 5, 0, 2, 4, 0, 4, 2, 0, 5, 5, 1, 0, 0, 2, 5, 4, 2, 4, 0,\n",
       "       2, 2, 2, 4, 0, 2, 4, 2, 0, 0, 0, 2, 0, 4, 5, 0, 5, 2, 5, 1, 1, 0,\n",
       "       5, 5, 0, 5, 2, 4])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'dense/kernel:0' shape=(254, 64) dtype=float32>,\n",
       " <tf.Variable 'dense/bias:0' shape=(64,) dtype=float32>,\n",
       " <tf.Variable 'dense_1/kernel:0' shape=(64, 10) dtype=float32>,\n",
       " <tf.Variable 'dense_1/bias:0' shape=(10,) dtype=float32>,\n",
       " <tf.Variable 'Adam/iterations:0' shape=() dtype=int64>,\n",
       " <tf.Variable 'Adam/lr:0' shape=() dtype=float32>,\n",
       " <tf.Variable 'Adam/beta_1:0' shape=() dtype=float32>,\n",
       " <tf.Variable 'Adam/beta_2:0' shape=() dtype=float32>,\n",
       " <tf.Variable 'Adam/decay:0' shape=() dtype=float32>,\n",
       " <tf.Variable 'training/Adam/Variable:0' shape=(254, 64) dtype=float32>,\n",
       " <tf.Variable 'training/Adam/Variable_1:0' shape=(64,) dtype=float32>,\n",
       " <tf.Variable 'training/Adam/Variable_2:0' shape=(64, 10) dtype=float32>,\n",
       " <tf.Variable 'training/Adam/Variable_3:0' shape=(10,) dtype=float32>,\n",
       " <tf.Variable 'training/Adam/Variable_4:0' shape=(254, 64) dtype=float32>,\n",
       " <tf.Variable 'training/Adam/Variable_5:0' shape=(64,) dtype=float32>,\n",
       " <tf.Variable 'training/Adam/Variable_6:0' shape=(64, 10) dtype=float32>,\n",
       " <tf.Variable 'training/Adam/Variable_7:0' shape=(10,) dtype=float32>,\n",
       " <tf.Variable 'training/Adam/Variable_8:0' shape=(1,) dtype=float32>,\n",
       " <tf.Variable 'training/Adam/Variable_9:0' shape=(1,) dtype=float32>,\n",
       " <tf.Variable 'training/Adam/Variable_10:0' shape=(1,) dtype=float32>,\n",
       " <tf.Variable 'training/Adam/Variable_11:0' shape=(1,) dtype=float32>]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.global_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "model.save('my_model_v1.h5')\n",
    "\n",
    "# load model\n",
    "another_model = tf.keras.models.load_model('my_model_v1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_temp = df_all_dum[ train.shape[0]: ]\n",
    "X_test_temp.shape\n",
    "Y_test = train['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test.shape\n",
    "y_one_hot_test = np.zeros((len(Y_test), 10))  \n",
    "y_one_hot_test[np.arange(len(Y_test)), Y_test] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test_temp.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1791,)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test=X_test_temp.values\n",
    "y_predict = another_model.predict_classes(X_test)\n",
    "#y_test_label = np.argmax(Y_test.values, axis=1)\n",
    "#print('Accuracy in testing data:', accuracy_score(y_test_label, y_predict))\n",
    "y_predict.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>appearedHour</th>\n",
       "      <th>appearedMinute</th>\n",
       "      <th>appearedTimeOfDay</th>\n",
       "      <th>closeToWater</th>\n",
       "      <th>continent</th>\n",
       "      <th>cooc_1</th>\n",
       "      <th>cooc_10</th>\n",
       "      <th>cooc_100</th>\n",
       "      <th>cooc_101</th>\n",
       "      <th>cooc_102</th>\n",
       "      <th>...</th>\n",
       "      <th>city_Tokyo</th>\n",
       "      <th>city_Toronto</th>\n",
       "      <th>city_Tripoli</th>\n",
       "      <th>city_Tunis</th>\n",
       "      <th>city_Vancouver</th>\n",
       "      <th>city_Vienna</th>\n",
       "      <th>city_Vilnius</th>\n",
       "      <th>city_Warsaw</th>\n",
       "      <th>city_Zagreb</th>\n",
       "      <th>city_Zurich</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">test</th>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>35</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 254 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        appearedHour  appearedMinute  appearedTimeOfDay  closeToWater  \\\n",
       "test 0             8              26                  0             0   \n",
       "     1             2              35                  3             1   \n",
       "     2             0               5                  3             1   \n",
       "     3             7              38                  0             1   \n",
       "     4             1              27                  3             0   \n",
       "\n",
       "        continent  cooc_1  cooc_10  cooc_100  cooc_101  cooc_102     ...       \\\n",
       "test 0          3       0        0         0         0         0     ...        \n",
       "     1          1       0        0         0         0         0     ...        \n",
       "     2          1       0        0         0         0         0     ...        \n",
       "     3          3       0        0         1         0         0     ...        \n",
       "     4          1       0        0         0         0         0     ...        \n",
       "\n",
       "        city_Tokyo  city_Toronto  city_Tripoli  city_Tunis  city_Vancouver  \\\n",
       "test 0           0             0             0           0               0   \n",
       "     1           0             0             0           0               0   \n",
       "     2           0             0             0           0               0   \n",
       "     3           0             0             0           0               0   \n",
       "     4           0             0             0           0               0   \n",
       "\n",
       "        city_Vienna  city_Vilnius  city_Warsaw  city_Zagreb  city_Zurich  \n",
       "test 0            0             0            0            0            0  \n",
       "     1            0             0            0            0            0  \n",
       "     2            0             0            0            0            0  \n",
       "     3            0             0            0            0            0  \n",
       "     4            0             0            0            0            0  \n",
       "\n",
       "[5 rows x 254 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 8., 26.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 2., 35.,  3., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  5.,  3., ...,  0.,  0.,  0.],\n",
       "       ...,\n",
       "       [11., 15.,  0., ...,  0.,  0.,  0.],\n",
       "       [15., 19.,  1., ...,  0.,  0.,  0.],\n",
       "       [14., 48.,  1., ...,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_temp = df_all_dum[ train.shape[0]: ]\n",
    "pd.DataFrame({'class': y_predict} ,index = testID).to_csv('pokemon_submission.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice\n",
    "Build a cat-dog-classifier with tf.keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
