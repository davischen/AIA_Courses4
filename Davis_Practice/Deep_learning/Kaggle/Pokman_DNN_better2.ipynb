{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here, we decide to learn tensorflow from a simple example directly!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General writing flow\n",
    "1. import required libraries\n",
    "2. load data and do some data pre-processing\n",
    "3. split your data into training and validation set\n",
    "4. build the network\n",
    "5. train the model and record/monitoring the performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import required libries and set some parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm_notebook\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load data and do some pre-processing\n",
    "We use MNIST HERE (with sklearn 8x8 version rather than use tensorflow 28x28 version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "train = pd.read_csv(\"train-3.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "testID = test['id']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop Null Feature(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_all = pd.concat( [ train.drop( [ 'Id', 'SalePrice' ], axis = 1 ),\n",
    "#                      test.drop( [ 'Id' ], axis = 1 ) ],\n",
    "#                      axis = 0, ignore_index = False )\n",
    "#train.drop_duplicates(keep='first', inplace=False) \n",
    "#test.drop_duplicates(keep='first', inplace=False) \n",
    "#train_data=train.drop('class', axis=1)\n",
    "df_all = pd.concat([train, test], keys=['train', 'test'], sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_all.drop(['Id'], axis=1, inplace=True)\n",
    "#df_all.drop(['id'], axis=1, inplace=True)\n",
    "#df_all.drop(['city'],axis=1,inplace=True)\n",
    "df_all.drop(['appearedHour'], axis=1, inplace=True)\n",
    "df_all.drop(['appearedMinute'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8951, 182)\n",
      "class    1791\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# check null status\n",
    "print( df_all.shape )\n",
    "df_na = df_all.select_dtypes( exclude = [ 'object' ] ).isnull().sum()\n",
    "print( df_na[ df_na > 100 ].sort_values(ascending=False) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train = train.drop( labels = train[ train['class'].isna().any() ].index, axis = 0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['class'] = df_all['class'].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#plt.plot(train['pressure'], train['class'], '.')\n",
    "#plt.xlabel('pressure')\n",
    "#plt.ylabel('class')\n",
    "#print( train[ train['pressure'] ].SalePrice )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def FunctionalToInt(x):\n",
    "    if(x==False):\n",
    "        r = 0\n",
    "    else:\n",
    "        r = 1\n",
    "    return r\n",
    "\n",
    "df_all['closeToWater'] = df_all['closeToWater'].apply(FunctionalToInt)\n",
    "df_all['urban'] = df_all['urban'].apply(FunctionalToInt)\n",
    "df_all['suburban'] = df_all['suburban'].apply(FunctionalToInt)\n",
    "df_all['midurban'] = df_all['midurban'].apply(FunctionalToInt)\n",
    "df_all['rural'] = df_all['rural'].apply(FunctionalToInt)\n",
    "df_all['gymIn100m'] = df_all['gymIn100m'].apply(FunctionalToInt)\n",
    "df_all['gymIn250m'] = df_all['gymIn250m'].apply(FunctionalToInt)\n",
    "df_all['gymIn500m'] = df_all['gymIn500m'].apply(FunctionalToInt)\n",
    "df_all['gymIn1000m'] = df_all['gymIn1000m'].apply(FunctionalToInt)\n",
    "df_all['gymIn2500m'] = df_all['gymIn2500m'].apply(FunctionalToInt)\n",
    "df_all['gymIn5000m'] = df_all['gymIn5000m'].apply(FunctionalToInt)\n",
    "df_all['pokestopIn100m'] = df_all['pokestopIn100m'].apply(FunctionalToInt)\n",
    "df_all['pokestopIn250m'] = df_all['pokestopIn250m'].apply(FunctionalToInt)\n",
    "df_all['pokestopIn500m'] = df_all['pokestopIn500m'].apply(FunctionalToInt)\n",
    "df_all['pokestopIn1000m'] = df_all['pokestopIn1000m'].apply(FunctionalToInt)\n",
    "\n",
    "df_all['pokestopIn2500m'] = df_all['pokestopIn2500m'].apply(FunctionalToInt)\n",
    "df_all['pokestopIn5000m'] = df_all['pokestopIn5000m'].apply(FunctionalToInt)\n",
    "\n",
    "\n",
    "for num in range(1,152):\n",
    "    df_all['cooc_'+str(num)] = df_all['cooc_'+str(num)].apply(FunctionalToInt)\n",
    "\n",
    "\n",
    "\n",
    "def FunctionalToDay(x):\n",
    "    if(x=='morning'):\n",
    "        r = 0\n",
    "    elif(x=='afternoon'):\n",
    "        r = 1\n",
    "    elif(x=='evening'):\n",
    "        r = 2\n",
    "    elif(x=='night'):\n",
    "        r = 3\n",
    "    else:\n",
    "        r = 4\n",
    "    return r\n",
    "\n",
    "df_all['appearedTimeOfDay'] = df_all['appearedTimeOfDay'].apply(FunctionalToDay)\n",
    "\n",
    "\n",
    "def FunctionalToWeather(x):\n",
    "    if(x=='Clear'):\n",
    "        r = 0\n",
    "    elif(x=='PartlyCloudy'):\n",
    "        r = 1\n",
    "    elif(x=='Overcast'):\n",
    "        r = 2\n",
    "    elif(x=='MostlyCloudy'):\n",
    "        r = 3\n",
    "    elif(x=='PartlyCloudy'):\n",
    "        r = 4\n",
    "    elif(x=='BreezyandPartlyCloudy'):\n",
    "        r = 5\n",
    "    elif(x=='Dry'):\n",
    "        r = 6\n",
    "    elif(x=='LightRain'):\n",
    "        r = 7\n",
    "    elif(x=='Rain'):\n",
    "        r = 8\n",
    "    else:\n",
    "        r = 9\n",
    "    return r\n",
    "\n",
    "df_all['weather'] = df_all['weather'].apply(FunctionalToWeather)\n",
    "\n",
    "\n",
    "\n",
    "def FunctionalToDay(x):\n",
    "    if(x=='morning'):\n",
    "        r = 0\n",
    "    elif(x=='afternoon'):\n",
    "        r = 1\n",
    "    elif(x=='evening'):\n",
    "        r = 2\n",
    "    elif(x=='night'):\n",
    "        r = 3\n",
    "    else:\n",
    "        r = 4\n",
    "    return r\n",
    "\n",
    "df_all['weatherIcon'] = df_all['weather'].apply(FunctionalToWeather)\n",
    "\n",
    "\n",
    "def FunctionalToClss(x):\n",
    "    #print(x)\n",
    "    if(x=='0.0'):\n",
    "        r = '0'\n",
    "    elif(x=='1.0'):\n",
    "        r = '1'\n",
    "    elif(x=='2.0'):\n",
    "        r = '2'\n",
    "    elif(x=='3.0'):\n",
    "        r = '3'\n",
    "    elif(x=='4.0'):\n",
    "        r = '4'\n",
    "    elif(x=='5.0'):\n",
    "        r = '5'\n",
    "    elif(x=='6.0'):\n",
    "        r = '6'\n",
    "    else:\n",
    "        r = '7'\n",
    "    return r\n",
    "\n",
    "#df_all['class'] = df_all['class'].apply(FunctionalToClss)\n",
    "\n",
    "def FunctionalToLocation(x):\n",
    "    if(x=='Asia'):\n",
    "        r = 0\n",
    "    elif(x=='America'):\n",
    "        r = 1\n",
    "    elif(x=='America/Argentina'):\n",
    "        r = 2\n",
    "    elif(x=='Europe'):\n",
    "        r = 3\n",
    "    elif(x=='Pacific'):\n",
    "        r = 4\n",
    "    elif(x=='Africa'):\n",
    "        r = 5\n",
    "    else:\n",
    "        r = 6\n",
    "    return r\n",
    "\n",
    "df_all['continent'] = df_all['continent'].apply(FunctionalToLocation)\n",
    "\n",
    "def FunctionalToDensity(x):\n",
    "    if(x>=10000):\n",
    "        r = 0\n",
    "    elif(x>=9000):\n",
    "        r = 1\n",
    "    elif(x>=8000):\n",
    "        r = 2\n",
    "    elif(x>=7000):\n",
    "        r = 3\n",
    "    elif(x>=6000):\n",
    "        r = 4\n",
    "    elif(x>=5000):\n",
    "        r = 5\n",
    "    elif(x>=4000):\n",
    "        r = 6\n",
    "    elif(x>=3000):\n",
    "        r = 7\n",
    "    elif(x>=2000):\n",
    "        r = 8\n",
    "    else:\n",
    "        r = 9\n",
    "    return r\n",
    "\n",
    "#df_all['population_density'].apply(FunctionalToDensity)\n",
    "\n",
    "def FunctionalToPressure(x):\n",
    "    if(x>=1035):\n",
    "        r = 0\n",
    "    elif(x>=1030):\n",
    "        r = 1\n",
    "    elif(x>=1025):\n",
    "        r = 2\n",
    "    elif(x>=1020):\n",
    "        r = 3\n",
    "    elif(x>=1015):\n",
    "        r = 4\n",
    "    elif(x>=1010):\n",
    "        r = 5\n",
    "    elif(x>=1005):\n",
    "        r = 6\n",
    "    elif(x>=1000):\n",
    "        r = 7\n",
    "    else:\n",
    "        r = 8\n",
    "    return r\n",
    "#df_all['pressure'].apply(FunctionalToPressure)\n",
    "\n",
    "df_all.drop(['population_density'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "df_all.drop(['pressure'], axis=1, inplace=True)\n",
    "\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "MultiIndex: 8951 entries, (train, 0) to (test, 1790)\n",
      "Columns: 180 entries, appearedTimeOfDay to windSpeed\n",
      "dtypes: float64(4), int64(173), object(3)\n",
      "memory usage: 12.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df_all.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一些右偏分佈的 feature，可透過取 log 將其轉為常態分佈\n",
    "#df_all['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>appearedTimeOfDay</th>\n",
       "      <th>closeToWater</th>\n",
       "      <th>continent</th>\n",
       "      <th>cooc_1</th>\n",
       "      <th>cooc_10</th>\n",
       "      <th>cooc_100</th>\n",
       "      <th>cooc_101</th>\n",
       "      <th>cooc_102</th>\n",
       "      <th>cooc_103</th>\n",
       "      <th>cooc_104</th>\n",
       "      <th>...</th>\n",
       "      <th>city_Tokyo</th>\n",
       "      <th>city_Toronto</th>\n",
       "      <th>city_Tripoli</th>\n",
       "      <th>city_Tunis</th>\n",
       "      <th>city_Vancouver</th>\n",
       "      <th>city_Vienna</th>\n",
       "      <th>city_Vilnius</th>\n",
       "      <th>city_Warsaw</th>\n",
       "      <th>city_Zagreb</th>\n",
       "      <th>city_Zurich</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">train</th>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 252 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         appearedTimeOfDay  closeToWater  continent  cooc_1  cooc_10  \\\n",
       "train 0                  2             0          0       0        1   \n",
       "      1                  3             1          1       0        0   \n",
       "      2                  2             1          1       0        0   \n",
       "      3                  0             1          6       0        0   \n",
       "      4                  2             1          1       0        0   \n",
       "\n",
       "         cooc_100  cooc_101  cooc_102  cooc_103  cooc_104     ...       \\\n",
       "train 0         0         0         0         0         0     ...        \n",
       "      1         0         0         0         0         0     ...        \n",
       "      2         0         0         0         0         0     ...        \n",
       "      3         0         0         0         0         0     ...        \n",
       "      4         0         0         0         0         0     ...        \n",
       "\n",
       "         city_Tokyo  city_Toronto  city_Tripoli  city_Tunis  city_Vancouver  \\\n",
       "train 0           0             0             0           0               0   \n",
       "      1           0             0             0           0               0   \n",
       "      2           0             0             0           0               0   \n",
       "      3           0             0             0           0               0   \n",
       "      4           0             0             0           0               0   \n",
       "\n",
       "         city_Vienna  city_Vilnius  city_Warsaw  city_Zagreb  city_Zurich  \n",
       "train 0            0             0            0            0            0  \n",
       "      1            0             0            0            0            0  \n",
       "      2            0             0            0            0            0  \n",
       "      3            0             0            0            0            0  \n",
       "      4            0             0            0            0            0  \n",
       "\n",
       "[5 rows x 252 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.drop(['id'], axis=1, inplace=True)\n",
    "df_all.drop(['class'], axis=1, inplace=True)\n",
    "df_all_dum = pd.get_dummies(df_all); df_all_dum.head()\n",
    "df_all_dum.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_all['population_density']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>appearedTimeOfDay</th>\n",
       "      <th>closeToWater</th>\n",
       "      <th>continent</th>\n",
       "      <th>cooc_1</th>\n",
       "      <th>cooc_10</th>\n",
       "      <th>cooc_100</th>\n",
       "      <th>cooc_101</th>\n",
       "      <th>cooc_102</th>\n",
       "      <th>cooc_103</th>\n",
       "      <th>cooc_104</th>\n",
       "      <th>...</th>\n",
       "      <th>city_Tokyo</th>\n",
       "      <th>city_Toronto</th>\n",
       "      <th>city_Tripoli</th>\n",
       "      <th>city_Tunis</th>\n",
       "      <th>city_Vancouver</th>\n",
       "      <th>city_Vienna</th>\n",
       "      <th>city_Vilnius</th>\n",
       "      <th>city_Warsaw</th>\n",
       "      <th>city_Zagreb</th>\n",
       "      <th>city_Zurich</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">train</th>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 252 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         appearedTimeOfDay  closeToWater  continent  cooc_1  cooc_10  \\\n",
       "train 0                  2             0          0       0        1   \n",
       "      1                  3             1          1       0        0   \n",
       "      2                  2             1          1       0        0   \n",
       "      3                  0             1          6       0        0   \n",
       "      4                  2             1          1       0        0   \n",
       "\n",
       "         cooc_100  cooc_101  cooc_102  cooc_103  cooc_104     ...       \\\n",
       "train 0         0         0         0         0         0     ...        \n",
       "      1         0         0         0         0         0     ...        \n",
       "      2         0         0         0         0         0     ...        \n",
       "      3         0         0         0         0         0     ...        \n",
       "      4         0         0         0         0         0     ...        \n",
       "\n",
       "         city_Tokyo  city_Toronto  city_Tripoli  city_Tunis  city_Vancouver  \\\n",
       "train 0           0             0             0           0               0   \n",
       "      1           0             0             0           0               0   \n",
       "      2           0             0             0           0               0   \n",
       "      3           0             0             0           0               0   \n",
       "      4           0             0             0           0               0   \n",
       "\n",
       "         city_Vienna  city_Vilnius  city_Warsaw  city_Zagreb  city_Zurich  \n",
       "train 0            0             0            0            0            0  \n",
       "      1            0             0            0            0            0  \n",
       "      2            0             0            0            0            0  \n",
       "      3            0             0            0            0            0  \n",
       "      4            0             0            0            0            0  \n",
       "\n",
       "[5 rows x 252 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# min-max normalization\n",
    "#x_ = x_ / x_.max() \n",
    "# 將NA的值填平均值\n",
    "df_all_dum = df_all_dum.fillna( df_all_dum.mean() ); df_all_dum.head()\n",
    "# one hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import rcParams\n",
    "#x_ = df_all_dum[:train.shape[0]].values\n",
    "#y_ = train['class'].values\n",
    "\n",
    "\n",
    "#X_test = df_all_dum[ train.shape[0]: ]\n",
    "\n",
    "X_train_temp=df_all_dum[:train.shape[0]]\n",
    "\n",
    "\n",
    "\n",
    "X = X_train_temp.values\n",
    "y = train['class'].values\n",
    "#把train data與test分開\n",
    "#X, X_val, y, y_val = train_test_split(X, y, test_size = 0.1, random_state = 40)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train['class'].isna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# setting hyperparameter\n",
    "batch_size = 128\n",
    "epochs = 800\n",
    "#lr = 0.8\n",
    "train_ratio = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7160, 252)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# min-max normalization\n",
    "X = X / X.max() \n",
    "#f_all_dum.dtypes\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test = df_all_dum[ train.shape[0]: ]\n",
    "#y_one_hot=y_\n",
    "\n",
    "y_one_hot = np.zeros((len(y), 10))  \n",
    "y_one_hot[np.arange(len(y)), y] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7160, 10)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_one_hot.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Split your data into training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y_one_hot, \n",
    "                                                    test_size=0.05, \n",
    "                                                    stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Dropout, Activation,BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 128)               32384     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 41,290\n",
      "Trainable params: 41,290\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(Dense(128, activation='relu',  input_shape=(252,)))\n",
    "#model.add(BatchNormalization())\n",
    "#model.add(Dense(128, input_dim = 784, activation= \"relu\"))\n",
    "#model.add(Dropout(0.25))\n",
    "model.add(Dense(64, activation = \"relu\"))\n",
    "\n",
    "#model.add(Dense(32, activation = \"relu\"))\n",
    "#model.add((0.5))\n",
    "#model.add(Dense(64, activation='relu', input_dim=20))\n",
    "#model.add(Dropout(0.5))\n",
    "#model.add(Dense(64, activation='relu'))\n",
    "#model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax')) \n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import SGD\n",
    "opt = SGD(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=tf.keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6121 samples, validate on 681 samples\n",
      "Epoch 1/800\n",
      "6121/6121 [==============================] - 1s 93us/step - loss: 2.1878 - acc: 0.2021 - val_loss: 1.9882 - val_acc: 0.2115\n",
      "Epoch 2/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.8554 - acc: 0.2199 - val_loss: 1.7766 - val_acc: 0.2188\n",
      "Epoch 3/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.7727 - acc: 0.2191 - val_loss: 1.7602 - val_acc: 0.2188\n",
      "Epoch 4/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.7641 - acc: 0.2258 - val_loss: 1.7533 - val_acc: 0.2188\n",
      "Epoch 5/800\n",
      "6121/6121 [==============================] - 0s 24us/step - loss: 1.7633 - acc: 0.2171 - val_loss: 1.7534 - val_acc: 0.2188\n",
      "Epoch 6/800\n",
      "6121/6121 [==============================] - 0s 24us/step - loss: 1.7607 - acc: 0.2256 - val_loss: 1.7534 - val_acc: 0.2188\n",
      "Epoch 7/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.7607 - acc: 0.2202 - val_loss: 1.7492 - val_acc: 0.2188\n",
      "Epoch 8/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.7615 - acc: 0.2207 - val_loss: 1.7510 - val_acc: 0.2188\n",
      "Epoch 9/800\n",
      "6121/6121 [==============================] - 0s 24us/step - loss: 1.7601 - acc: 0.2207 - val_loss: 1.7542 - val_acc: 0.2188\n",
      "Epoch 10/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.7600 - acc: 0.2256 - val_loss: 1.7514 - val_acc: 0.2070\n",
      "Epoch 11/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.7589 - acc: 0.2206 - val_loss: 1.7491 - val_acc: 0.2188\n",
      "Epoch 12/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.7595 - acc: 0.2212 - val_loss: 1.7510 - val_acc: 0.2100\n",
      "Epoch 13/800\n",
      "6121/6121 [==============================] - 0s 24us/step - loss: 1.7591 - acc: 0.2246 - val_loss: 1.7512 - val_acc: 0.2188\n",
      "Epoch 14/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.7584 - acc: 0.2256 - val_loss: 1.7456 - val_acc: 0.2188\n",
      "Epoch 15/800\n",
      "6121/6121 [==============================] - 0s 25us/step - loss: 1.7580 - acc: 0.2227 - val_loss: 1.7513 - val_acc: 0.2188\n",
      "Epoch 16/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.7584 - acc: 0.2317 - val_loss: 1.7511 - val_acc: 0.2467\n",
      "Epoch 17/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.7569 - acc: 0.2282 - val_loss: 1.7478 - val_acc: 0.2144\n",
      "Epoch 18/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.7573 - acc: 0.2230 - val_loss: 1.7477 - val_acc: 0.2188\n",
      "Epoch 19/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.7551 - acc: 0.2292 - val_loss: 1.7494 - val_acc: 0.2188\n",
      "Epoch 20/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.7538 - acc: 0.2294 - val_loss: 1.7450 - val_acc: 0.2100\n",
      "Epoch 21/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.7541 - acc: 0.2318 - val_loss: 1.7458 - val_acc: 0.2188\n",
      "Epoch 22/800\n",
      "6121/6121 [==============================] - 0s 24us/step - loss: 1.7549 - acc: 0.2225 - val_loss: 1.7400 - val_acc: 0.2232\n",
      "Epoch 23/800\n",
      "6121/6121 [==============================] - 0s 24us/step - loss: 1.7515 - acc: 0.2354 - val_loss: 1.7415 - val_acc: 0.2761\n",
      "Epoch 24/800\n",
      "6121/6121 [==============================] - 0s 20us/step - loss: 1.7496 - acc: 0.2413 - val_loss: 1.7406 - val_acc: 0.2320\n",
      "Epoch 25/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.7479 - acc: 0.2485 - val_loss: 1.7368 - val_acc: 0.2203\n",
      "Epoch 26/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.7440 - acc: 0.2475 - val_loss: 1.7355 - val_acc: 0.2217\n",
      "Epoch 27/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.7418 - acc: 0.2348 - val_loss: 1.7314 - val_acc: 0.2702\n",
      "Epoch 28/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.7365 - acc: 0.2503 - val_loss: 1.7256 - val_acc: 0.2261\n",
      "Epoch 29/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.7320 - acc: 0.2526 - val_loss: 1.7244 - val_acc: 0.3010\n",
      "Epoch 30/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.7286 - acc: 0.2652 - val_loss: 1.7135 - val_acc: 0.2790\n",
      "Epoch 31/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.7235 - acc: 0.2554 - val_loss: 1.7194 - val_acc: 0.2614\n",
      "Epoch 32/800\n",
      "6121/6121 [==============================] - 0s 24us/step - loss: 1.7174 - acc: 0.2652 - val_loss: 1.7048 - val_acc: 0.2496\n",
      "Epoch 33/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.7085 - acc: 0.2743 - val_loss: 1.6967 - val_acc: 0.2467\n",
      "Epoch 34/800\n",
      "6121/6121 [==============================] - 0s 24us/step - loss: 1.7004 - acc: 0.2684 - val_loss: 1.6947 - val_acc: 0.2981\n",
      "Epoch 35/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.6909 - acc: 0.2745 - val_loss: 1.6772 - val_acc: 0.2819\n",
      "Epoch 36/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.6849 - acc: 0.2730 - val_loss: 1.6680 - val_acc: 0.2717\n",
      "Epoch 37/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.6722 - acc: 0.2795 - val_loss: 1.6647 - val_acc: 0.2922\n",
      "Epoch 38/800\n",
      "6121/6121 [==============================] - 0s 20us/step - loss: 1.6631 - acc: 0.2799 - val_loss: 1.6660 - val_acc: 0.3098\n",
      "Epoch 39/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.6539 - acc: 0.2825 - val_loss: 1.6414 - val_acc: 0.2849\n",
      "Epoch 40/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.6459 - acc: 0.2908 - val_loss: 1.6398 - val_acc: 0.3172\n",
      "Epoch 41/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.6376 - acc: 0.2915 - val_loss: 1.6226 - val_acc: 0.3010\n",
      "Epoch 42/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.6298 - acc: 0.2936 - val_loss: 1.6196 - val_acc: 0.2849\n",
      "Epoch 43/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.6200 - acc: 0.2986 - val_loss: 1.6133 - val_acc: 0.3010\n",
      "Epoch 44/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.6154 - acc: 0.3035 - val_loss: 1.6060 - val_acc: 0.2819\n",
      "Epoch 45/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.6128 - acc: 0.3009 - val_loss: 1.5996 - val_acc: 0.3010\n",
      "Epoch 46/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.6054 - acc: 0.3060 - val_loss: 1.5946 - val_acc: 0.3010\n",
      "Epoch 47/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.6010 - acc: 0.3109 - val_loss: 1.5998 - val_acc: 0.3084\n",
      "Epoch 48/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.5972 - acc: 0.3102 - val_loss: 1.5844 - val_acc: 0.3025\n",
      "Epoch 49/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.5926 - acc: 0.3119 - val_loss: 1.5843 - val_acc: 0.3216\n",
      "Epoch 50/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.5936 - acc: 0.3109 - val_loss: 1.5842 - val_acc: 0.2893\n",
      "Epoch 51/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.5947 - acc: 0.3161 - val_loss: 1.5767 - val_acc: 0.3025\n",
      "Epoch 52/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.5836 - acc: 0.3184 - val_loss: 1.5748 - val_acc: 0.3025\n",
      "Epoch 53/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.5803 - acc: 0.3225 - val_loss: 1.5726 - val_acc: 0.3040\n",
      "Epoch 54/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.5793 - acc: 0.3218 - val_loss: 1.5701 - val_acc: 0.3069\n",
      "Epoch 55/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.5749 - acc: 0.3205 - val_loss: 1.5685 - val_acc: 0.3069\n",
      "Epoch 56/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.5732 - acc: 0.3228 - val_loss: 1.5682 - val_acc: 0.3319\n",
      "Epoch 57/800\n",
      "6121/6121 [==============================] - 0s 20us/step - loss: 1.5727 - acc: 0.3222 - val_loss: 1.5766 - val_acc: 0.3319\n",
      "Epoch 58/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.5722 - acc: 0.3222 - val_loss: 1.5592 - val_acc: 0.3157\n",
      "Epoch 59/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.5675 - acc: 0.3279 - val_loss: 1.5534 - val_acc: 0.3186\n",
      "Epoch 60/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.5655 - acc: 0.3261 - val_loss: 1.5561 - val_acc: 0.3069\n",
      "Epoch 61/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.5633 - acc: 0.3362 - val_loss: 1.5537 - val_acc: 0.3113\n",
      "Epoch 62/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.5596 - acc: 0.3323 - val_loss: 1.5492 - val_acc: 0.3201\n",
      "Epoch 63/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.5632 - acc: 0.3307 - val_loss: 1.5454 - val_acc: 0.3348\n",
      "Epoch 64/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.5550 - acc: 0.3307 - val_loss: 1.5462 - val_acc: 0.3172\n",
      "Epoch 65/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.5564 - acc: 0.3298 - val_loss: 1.5600 - val_acc: 0.3010\n",
      "Epoch 66/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.5583 - acc: 0.3388 - val_loss: 1.5412 - val_acc: 0.3201\n",
      "Epoch 67/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.5489 - acc: 0.3326 - val_loss: 1.5368 - val_acc: 0.3289\n",
      "Epoch 68/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.5498 - acc: 0.3426 - val_loss: 1.5413 - val_acc: 0.3377\n",
      "Epoch 69/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.5451 - acc: 0.3408 - val_loss: 1.5408 - val_acc: 0.3260\n",
      "Epoch 70/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.5465 - acc: 0.3414 - val_loss: 1.5305 - val_acc: 0.3392\n",
      "Epoch 71/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.5405 - acc: 0.3509 - val_loss: 1.5371 - val_acc: 0.3480\n",
      "Epoch 72/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.5388 - acc: 0.3475 - val_loss: 1.5275 - val_acc: 0.3436\n",
      "Epoch 73/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.5368 - acc: 0.3495 - val_loss: 1.5228 - val_acc: 0.3480\n",
      "Epoch 74/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.5381 - acc: 0.3537 - val_loss: 1.5283 - val_acc: 0.3377\n",
      "Epoch 75/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.5336 - acc: 0.3524 - val_loss: 1.5140 - val_acc: 0.3465\n",
      "Epoch 76/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.5286 - acc: 0.3540 - val_loss: 1.5212 - val_acc: 0.3436\n",
      "Epoch 77/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.5268 - acc: 0.3537 - val_loss: 1.5134 - val_acc: 0.3480\n",
      "Epoch 78/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.5222 - acc: 0.3576 - val_loss: 1.5360 - val_acc: 0.3642\n",
      "Epoch 79/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.5259 - acc: 0.3615 - val_loss: 1.5237 - val_acc: 0.3216\n",
      "Epoch 80/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.5192 - acc: 0.3602 - val_loss: 1.5033 - val_acc: 0.3480\n",
      "Epoch 81/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.5162 - acc: 0.3607 - val_loss: 1.4988 - val_acc: 0.3744\n",
      "Epoch 82/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.5145 - acc: 0.3609 - val_loss: 1.5106 - val_acc: 0.3730\n",
      "Epoch 83/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.5119 - acc: 0.3764 - val_loss: 1.4971 - val_acc: 0.3554\n",
      "Epoch 84/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.5105 - acc: 0.3700 - val_loss: 1.5040 - val_acc: 0.3833\n",
      "Epoch 85/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.5063 - acc: 0.3710 - val_loss: 1.4887 - val_acc: 0.3568\n",
      "Epoch 86/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.5004 - acc: 0.3730 - val_loss: 1.5005 - val_acc: 0.3260\n",
      "Epoch 87/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.5000 - acc: 0.3740 - val_loss: 1.4836 - val_acc: 0.3686\n",
      "Epoch 88/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.4950 - acc: 0.3813 - val_loss: 1.4833 - val_acc: 0.3818\n",
      "Epoch 89/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.4908 - acc: 0.3795 - val_loss: 1.4737 - val_acc: 0.3700\n",
      "Epoch 90/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.4881 - acc: 0.3836 - val_loss: 1.4858 - val_acc: 0.3789\n",
      "Epoch 91/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.4867 - acc: 0.3843 - val_loss: 1.4682 - val_acc: 0.3979\n",
      "Epoch 92/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.4800 - acc: 0.3892 - val_loss: 1.4746 - val_acc: 0.3671\n",
      "Epoch 93/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.4819 - acc: 0.3880 - val_loss: 1.4647 - val_acc: 0.3877\n",
      "Epoch 94/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.4781 - acc: 0.3865 - val_loss: 1.4541 - val_acc: 0.4053\n",
      "Epoch 95/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.4746 - acc: 0.3937 - val_loss: 1.4693 - val_acc: 0.3935\n",
      "Epoch 96/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.4776 - acc: 0.3929 - val_loss: 1.4591 - val_acc: 0.3847\n",
      "Epoch 97/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.4693 - acc: 0.3965 - val_loss: 1.4447 - val_acc: 0.4185\n",
      "Epoch 98/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.4660 - acc: 0.3945 - val_loss: 1.4427 - val_acc: 0.4170\n",
      "Epoch 99/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.4607 - acc: 0.3986 - val_loss: 1.4452 - val_acc: 0.4097\n",
      "Epoch 100/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.4594 - acc: 0.4035 - val_loss: 1.4391 - val_acc: 0.4156\n",
      "Epoch 101/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.4538 - acc: 0.4063 - val_loss: 1.4411 - val_acc: 0.4097\n",
      "Epoch 102/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.4517 - acc: 0.4048 - val_loss: 1.4384 - val_acc: 0.4112\n",
      "Epoch 103/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.4513 - acc: 0.4068 - val_loss: 1.4260 - val_acc: 0.4229\n",
      "Epoch 104/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.4482 - acc: 0.4060 - val_loss: 1.4294 - val_acc: 0.4126\n",
      "Epoch 105/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.4460 - acc: 0.4101 - val_loss: 1.4336 - val_acc: 0.4112\n",
      "Epoch 106/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.4420 - acc: 0.4115 - val_loss: 1.4199 - val_acc: 0.4170\n",
      "Epoch 107/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.4382 - acc: 0.4173 - val_loss: 1.4308 - val_acc: 0.3935\n",
      "Epoch 108/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.4395 - acc: 0.4052 - val_loss: 1.4160 - val_acc: 0.4170\n",
      "Epoch 109/800\n",
      "6121/6121 [==============================] - 0s 20us/step - loss: 1.4308 - acc: 0.4223 - val_loss: 1.4201 - val_acc: 0.4082\n",
      "Epoch 110/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.4300 - acc: 0.4135 - val_loss: 1.4103 - val_acc: 0.4317\n",
      "Epoch 111/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.4290 - acc: 0.4153 - val_loss: 1.4152 - val_acc: 0.4053\n",
      "Epoch 112/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.4227 - acc: 0.4243 - val_loss: 1.4097 - val_acc: 0.4347\n",
      "Epoch 113/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.4263 - acc: 0.4181 - val_loss: 1.4011 - val_acc: 0.4302\n",
      "Epoch 114/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.4196 - acc: 0.4228 - val_loss: 1.4104 - val_acc: 0.4156\n",
      "Epoch 115/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.4202 - acc: 0.4197 - val_loss: 1.4000 - val_acc: 0.4258\n",
      "Epoch 116/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.4163 - acc: 0.4251 - val_loss: 1.3934 - val_acc: 0.4391\n",
      "Epoch 117/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.4126 - acc: 0.4248 - val_loss: 1.3937 - val_acc: 0.4361\n",
      "Epoch 118/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.4131 - acc: 0.4223 - val_loss: 1.3909 - val_acc: 0.4361\n",
      "Epoch 119/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.4126 - acc: 0.4243 - val_loss: 1.4026 - val_acc: 0.4361\n",
      "Epoch 120/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.4100 - acc: 0.4225 - val_loss: 1.3889 - val_acc: 0.4464\n",
      "Epoch 121/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.4086 - acc: 0.4292 - val_loss: 1.3833 - val_acc: 0.4420\n",
      "Epoch 122/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.4026 - acc: 0.4302 - val_loss: 1.3772 - val_acc: 0.4405\n",
      "Epoch 123/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.4029 - acc: 0.4303 - val_loss: 1.3805 - val_acc: 0.4435\n",
      "Epoch 124/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.4069 - acc: 0.4289 - val_loss: 1.3946 - val_acc: 0.4317\n",
      "Epoch 125/800\n",
      "6121/6121 [==============================] - 0s 24us/step - loss: 1.3976 - acc: 0.4341 - val_loss: 1.3742 - val_acc: 0.4405\n",
      "Epoch 126/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.3980 - acc: 0.4342 - val_loss: 1.3779 - val_acc: 0.4317\n",
      "Epoch 127/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.3957 - acc: 0.4311 - val_loss: 1.3841 - val_acc: 0.4258\n",
      "Epoch 128/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.3920 - acc: 0.4316 - val_loss: 1.3807 - val_acc: 0.4508\n",
      "Epoch 129/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.3927 - acc: 0.4383 - val_loss: 1.3647 - val_acc: 0.4596\n",
      "Epoch 130/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.3873 - acc: 0.4387 - val_loss: 1.3688 - val_acc: 0.4479\n",
      "Epoch 131/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.3925 - acc: 0.4352 - val_loss: 1.3704 - val_acc: 0.4391\n",
      "Epoch 132/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.3877 - acc: 0.4411 - val_loss: 1.3687 - val_acc: 0.4376\n",
      "Epoch 133/800\n",
      "6121/6121 [==============================] - 0s 20us/step - loss: 1.3825 - acc: 0.4387 - val_loss: 1.3646 - val_acc: 0.4479\n",
      "Epoch 134/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.3787 - acc: 0.4395 - val_loss: 1.3628 - val_acc: 0.4347\n",
      "Epoch 135/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.3855 - acc: 0.4439 - val_loss: 1.3885 - val_acc: 0.4435\n",
      "Epoch 136/800\n",
      "6121/6121 [==============================] - 0s 24us/step - loss: 1.3772 - acc: 0.4436 - val_loss: 1.3504 - val_acc: 0.4596\n",
      "Epoch 137/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.3798 - acc: 0.4457 - val_loss: 1.3552 - val_acc: 0.4493\n",
      "Epoch 138/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.3793 - acc: 0.4471 - val_loss: 1.3480 - val_acc: 0.4626\n",
      "Epoch 139/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.3702 - acc: 0.4473 - val_loss: 1.3582 - val_acc: 0.4596\n",
      "Epoch 140/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.3676 - acc: 0.4501 - val_loss: 1.3490 - val_acc: 0.4567\n",
      "Epoch 141/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.3687 - acc: 0.4460 - val_loss: 1.3633 - val_acc: 0.4552\n",
      "Epoch 142/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.3661 - acc: 0.4525 - val_loss: 1.3388 - val_acc: 0.4655\n",
      "Epoch 143/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.3672 - acc: 0.4514 - val_loss: 1.3426 - val_acc: 0.4714\n",
      "Epoch 144/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.3639 - acc: 0.4530 - val_loss: 1.3430 - val_acc: 0.4537\n",
      "Epoch 145/800\n",
      "6121/6121 [==============================] - 0s 24us/step - loss: 1.3619 - acc: 0.4494 - val_loss: 1.3342 - val_acc: 0.4640\n",
      "Epoch 146/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.3621 - acc: 0.4498 - val_loss: 1.3395 - val_acc: 0.4699\n",
      "Epoch 147/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.3602 - acc: 0.4578 - val_loss: 1.3338 - val_acc: 0.4714\n",
      "Epoch 148/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.3628 - acc: 0.4525 - val_loss: 1.3442 - val_acc: 0.4714\n",
      "Epoch 149/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.3561 - acc: 0.4516 - val_loss: 1.3345 - val_acc: 0.4449\n",
      "Epoch 150/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.3608 - acc: 0.4538 - val_loss: 1.3269 - val_acc: 0.4581\n",
      "Epoch 151/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.3550 - acc: 0.4516 - val_loss: 1.3509 - val_acc: 0.4728\n",
      "Epoch 152/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.3501 - acc: 0.4592 - val_loss: 1.3278 - val_acc: 0.4772\n",
      "Epoch 153/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.3507 - acc: 0.4570 - val_loss: 1.3287 - val_acc: 0.4860\n",
      "Epoch 154/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.3433 - acc: 0.4615 - val_loss: 1.3278 - val_acc: 0.4802\n",
      "Epoch 155/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.3454 - acc: 0.4654 - val_loss: 1.3218 - val_acc: 0.4728\n",
      "Epoch 156/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.3443 - acc: 0.4601 - val_loss: 1.3315 - val_acc: 0.4816\n",
      "Epoch 157/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.3412 - acc: 0.4602 - val_loss: 1.3129 - val_acc: 0.4890\n",
      "Epoch 158/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.3408 - acc: 0.4628 - val_loss: 1.3270 - val_acc: 0.4684\n",
      "Epoch 159/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.3384 - acc: 0.4571 - val_loss: 1.3163 - val_acc: 0.4875\n",
      "Epoch 160/800\n",
      "6121/6121 [==============================] - 0s 24us/step - loss: 1.3404 - acc: 0.4695 - val_loss: 1.3208 - val_acc: 0.4860\n",
      "Epoch 161/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.3337 - acc: 0.4614 - val_loss: 1.3090 - val_acc: 0.4772\n",
      "Epoch 162/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.3311 - acc: 0.4699 - val_loss: 1.3086 - val_acc: 0.4831\n",
      "Epoch 163/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.3276 - acc: 0.4635 - val_loss: 1.3085 - val_acc: 0.4890\n",
      "Epoch 164/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.3349 - acc: 0.4679 - val_loss: 1.3065 - val_acc: 0.4860\n",
      "Epoch 165/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.3294 - acc: 0.4682 - val_loss: 1.3166 - val_acc: 0.4919\n",
      "Epoch 166/800\n",
      "6121/6121 [==============================] - 0s 20us/step - loss: 1.3320 - acc: 0.4723 - val_loss: 1.3074 - val_acc: 0.4846\n",
      "Epoch 167/800\n",
      "6121/6121 [==============================] - 0s 20us/step - loss: 1.3217 - acc: 0.4735 - val_loss: 1.3176 - val_acc: 0.4919\n",
      "Epoch 168/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.3234 - acc: 0.4721 - val_loss: 1.2992 - val_acc: 0.4699\n",
      "Epoch 169/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.3225 - acc: 0.4766 - val_loss: 1.2928 - val_acc: 0.4993\n",
      "Epoch 170/800\n",
      "6121/6121 [==============================] - 0s 24us/step - loss: 1.3173 - acc: 0.4769 - val_loss: 1.3006 - val_acc: 0.5022\n",
      "Epoch 171/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.3191 - acc: 0.4749 - val_loss: 1.2853 - val_acc: 0.5007\n",
      "Epoch 172/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.3168 - acc: 0.4813 - val_loss: 1.2860 - val_acc: 0.5007\n",
      "Epoch 173/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.3121 - acc: 0.4743 - val_loss: 1.2863 - val_acc: 0.4772\n",
      "Epoch 174/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.3106 - acc: 0.4788 - val_loss: 1.2824 - val_acc: 0.5066\n",
      "Epoch 175/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.3066 - acc: 0.4862 - val_loss: 1.3065 - val_acc: 0.5125\n",
      "Epoch 176/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.3128 - acc: 0.4769 - val_loss: 1.2771 - val_acc: 0.5125\n",
      "Epoch 177/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.3131 - acc: 0.4792 - val_loss: 1.3026 - val_acc: 0.4993\n",
      "Epoch 178/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.3145 - acc: 0.4805 - val_loss: 1.2948 - val_acc: 0.5095\n",
      "Epoch 179/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.3093 - acc: 0.4769 - val_loss: 1.2755 - val_acc: 0.5125\n",
      "Epoch 180/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.3019 - acc: 0.4882 - val_loss: 1.2710 - val_acc: 0.5198\n",
      "Epoch 181/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.2984 - acc: 0.4873 - val_loss: 1.2717 - val_acc: 0.5095\n",
      "Epoch 182/800\n",
      "6121/6121 [==============================] - 0s 24us/step - loss: 1.2986 - acc: 0.4862 - val_loss: 1.2787 - val_acc: 0.5272\n",
      "Epoch 183/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.2999 - acc: 0.4878 - val_loss: 1.2754 - val_acc: 0.5198\n",
      "Epoch 184/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.2956 - acc: 0.4836 - val_loss: 1.2652 - val_acc: 0.5007\n",
      "Epoch 185/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.2960 - acc: 0.4868 - val_loss: 1.2780 - val_acc: 0.5095\n",
      "Epoch 186/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.2999 - acc: 0.4854 - val_loss: 1.2603 - val_acc: 0.5286\n",
      "Epoch 187/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.2931 - acc: 0.4906 - val_loss: 1.2680 - val_acc: 0.5154\n",
      "Epoch 188/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.2935 - acc: 0.4885 - val_loss: 1.2626 - val_acc: 0.5022\n",
      "Epoch 189/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.2931 - acc: 0.4911 - val_loss: 1.2579 - val_acc: 0.5257\n",
      "Epoch 190/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.2906 - acc: 0.4854 - val_loss: 1.2515 - val_acc: 0.5184\n",
      "Epoch 191/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.2899 - acc: 0.4909 - val_loss: 1.2672 - val_acc: 0.5198\n",
      "Epoch 192/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.2874 - acc: 0.4924 - val_loss: 1.2772 - val_acc: 0.5286\n",
      "Epoch 193/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.2937 - acc: 0.4898 - val_loss: 1.2495 - val_acc: 0.5257\n",
      "Epoch 194/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.2883 - acc: 0.4914 - val_loss: 1.2490 - val_acc: 0.5433\n",
      "Epoch 195/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.2812 - acc: 0.4926 - val_loss: 1.2601 - val_acc: 0.5330\n",
      "Epoch 196/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.2833 - acc: 0.4929 - val_loss: 1.2527 - val_acc: 0.5360\n",
      "Epoch 197/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.2806 - acc: 0.4932 - val_loss: 1.2533 - val_acc: 0.5374\n",
      "Epoch 198/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.2756 - acc: 0.4980 - val_loss: 1.2437 - val_acc: 0.5419\n",
      "Epoch 199/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.2775 - acc: 0.4976 - val_loss: 1.2501 - val_acc: 0.5051\n",
      "Epoch 200/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.2763 - acc: 0.4949 - val_loss: 1.2400 - val_acc: 0.5198\n",
      "Epoch 201/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.2730 - acc: 0.4965 - val_loss: 1.2551 - val_acc: 0.5330\n",
      "Epoch 202/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.2733 - acc: 0.4952 - val_loss: 1.2363 - val_acc: 0.5389\n",
      "Epoch 203/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.2769 - acc: 0.4934 - val_loss: 1.2349 - val_acc: 0.5374\n",
      "Epoch 204/800\n",
      "6121/6121 [==============================] - 0s 20us/step - loss: 1.2707 - acc: 0.4993 - val_loss: 1.2431 - val_acc: 0.5066\n",
      "Epoch 205/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.2757 - acc: 0.4955 - val_loss: 1.2454 - val_acc: 0.5286\n",
      "Epoch 206/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.2732 - acc: 0.4958 - val_loss: 1.2507 - val_acc: 0.5286\n",
      "Epoch 207/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.2673 - acc: 0.4924 - val_loss: 1.2322 - val_acc: 0.5154\n",
      "Epoch 208/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.2670 - acc: 0.4988 - val_loss: 1.2282 - val_acc: 0.5316\n",
      "Epoch 209/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.2628 - acc: 0.5061 - val_loss: 1.2325 - val_acc: 0.5492\n",
      "Epoch 210/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.2641 - acc: 0.4976 - val_loss: 1.2283 - val_acc: 0.5257\n",
      "Epoch 211/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.2599 - acc: 0.5079 - val_loss: 1.2272 - val_acc: 0.5477\n",
      "Epoch 212/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.2588 - acc: 0.5045 - val_loss: 1.2255 - val_acc: 0.5286\n",
      "Epoch 213/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.2581 - acc: 0.5040 - val_loss: 1.2280 - val_acc: 0.5477\n",
      "Epoch 214/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.2578 - acc: 0.5066 - val_loss: 1.2355 - val_acc: 0.5345\n",
      "Epoch 215/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.2685 - acc: 0.4986 - val_loss: 1.2389 - val_acc: 0.5360\n",
      "Epoch 216/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.2634 - acc: 0.5033 - val_loss: 1.2271 - val_acc: 0.5169\n",
      "Epoch 217/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.2565 - acc: 0.5079 - val_loss: 1.2466 - val_acc: 0.5066\n",
      "Epoch 218/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.2554 - acc: 0.5030 - val_loss: 1.2254 - val_acc: 0.5463\n",
      "Epoch 219/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.2529 - acc: 0.5061 - val_loss: 1.2204 - val_acc: 0.5316\n",
      "Epoch 220/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.2518 - acc: 0.5009 - val_loss: 1.2212 - val_acc: 0.5463\n",
      "Epoch 221/800\n",
      "6121/6121 [==============================] - 0s 20us/step - loss: 1.2544 - acc: 0.5091 - val_loss: 1.2538 - val_acc: 0.5242\n",
      "Epoch 222/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.2665 - acc: 0.5065 - val_loss: 1.2132 - val_acc: 0.5477\n",
      "Epoch 223/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.2522 - acc: 0.5048 - val_loss: 1.2108 - val_acc: 0.5404\n",
      "Epoch 224/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.2585 - acc: 0.5025 - val_loss: 1.2156 - val_acc: 0.5389\n",
      "Epoch 225/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.2448 - acc: 0.5120 - val_loss: 1.2119 - val_acc: 0.5374\n",
      "Epoch 226/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.2517 - acc: 0.5074 - val_loss: 1.2332 - val_acc: 0.5360\n",
      "Epoch 227/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.2509 - acc: 0.5097 - val_loss: 1.2080 - val_acc: 0.5419\n",
      "Epoch 228/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.2471 - acc: 0.5123 - val_loss: 1.2253 - val_acc: 0.5316\n",
      "Epoch 229/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.2548 - acc: 0.5083 - val_loss: 1.2156 - val_acc: 0.5345\n",
      "Epoch 230/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.2466 - acc: 0.5118 - val_loss: 1.2203 - val_acc: 0.5463\n",
      "Epoch 231/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.2440 - acc: 0.5140 - val_loss: 1.2216 - val_acc: 0.5286\n",
      "Epoch 232/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.2448 - acc: 0.5122 - val_loss: 1.2167 - val_acc: 0.5477\n",
      "Epoch 233/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.2430 - acc: 0.5125 - val_loss: 1.2137 - val_acc: 0.5521\n",
      "Epoch 234/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.2411 - acc: 0.5117 - val_loss: 1.2010 - val_acc: 0.5374\n",
      "Epoch 235/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.2406 - acc: 0.5127 - val_loss: 1.2017 - val_acc: 0.5419\n",
      "Epoch 236/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.2387 - acc: 0.5112 - val_loss: 1.2112 - val_acc: 0.5301\n",
      "Epoch 237/800\n",
      "6121/6121 [==============================] - 0s 24us/step - loss: 1.2369 - acc: 0.5127 - val_loss: 1.2105 - val_acc: 0.5272\n",
      "Epoch 238/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.2430 - acc: 0.5130 - val_loss: 1.2142 - val_acc: 0.5360\n",
      "Epoch 239/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.2433 - acc: 0.5125 - val_loss: 1.2029 - val_acc: 0.5433\n",
      "Epoch 240/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.2358 - acc: 0.5179 - val_loss: 1.2479 - val_acc: 0.5095\n",
      "Epoch 241/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.2489 - acc: 0.5161 - val_loss: 1.2072 - val_acc: 0.5477\n",
      "Epoch 242/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.2402 - acc: 0.5118 - val_loss: 1.2049 - val_acc: 0.5419\n",
      "Epoch 243/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.2358 - acc: 0.5149 - val_loss: 1.1987 - val_acc: 0.5360\n",
      "Epoch 244/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.2400 - acc: 0.5182 - val_loss: 1.2172 - val_acc: 0.5360\n",
      "Epoch 245/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.2415 - acc: 0.5118 - val_loss: 1.2034 - val_acc: 0.5463\n",
      "Epoch 246/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.2370 - acc: 0.5151 - val_loss: 1.2081 - val_acc: 0.5492\n",
      "Epoch 247/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.2343 - acc: 0.5153 - val_loss: 1.2124 - val_acc: 0.5477\n",
      "Epoch 248/800\n",
      "6121/6121 [==============================] - 0s 20us/step - loss: 1.2374 - acc: 0.5195 - val_loss: 1.2088 - val_acc: 0.5374\n",
      "Epoch 249/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.2347 - acc: 0.5169 - val_loss: 1.1946 - val_acc: 0.5374\n",
      "Epoch 250/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.2370 - acc: 0.5200 - val_loss: 1.1997 - val_acc: 0.5536\n",
      "Epoch 251/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.2319 - acc: 0.5184 - val_loss: 1.1964 - val_acc: 0.5565\n",
      "Epoch 252/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.2303 - acc: 0.5169 - val_loss: 1.2060 - val_acc: 0.5419\n",
      "Epoch 253/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.2361 - acc: 0.5118 - val_loss: 1.1940 - val_acc: 0.5448\n",
      "Epoch 254/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.2311 - acc: 0.5195 - val_loss: 1.1948 - val_acc: 0.5404\n",
      "Epoch 255/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.2310 - acc: 0.5158 - val_loss: 1.1956 - val_acc: 0.5433\n",
      "Epoch 256/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.2280 - acc: 0.5146 - val_loss: 1.1887 - val_acc: 0.5389\n",
      "Epoch 257/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.2290 - acc: 0.5197 - val_loss: 1.1941 - val_acc: 0.5477\n",
      "Epoch 258/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.2264 - acc: 0.5202 - val_loss: 1.2013 - val_acc: 0.5433\n",
      "Epoch 259/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.2331 - acc: 0.5143 - val_loss: 1.1914 - val_acc: 0.5477\n",
      "Epoch 260/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.2282 - acc: 0.5146 - val_loss: 1.1883 - val_acc: 0.5389\n",
      "Epoch 261/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.2268 - acc: 0.5207 - val_loss: 1.2195 - val_acc: 0.5404\n",
      "Epoch 262/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.2261 - acc: 0.5213 - val_loss: 1.2031 - val_acc: 0.5521\n",
      "Epoch 263/800\n",
      "6121/6121 [==============================] - 0s 24us/step - loss: 1.2275 - acc: 0.5192 - val_loss: 1.2213 - val_acc: 0.5184\n",
      "Epoch 264/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.2267 - acc: 0.5161 - val_loss: 1.1866 - val_acc: 0.5507\n",
      "Epoch 265/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.2302 - acc: 0.5182 - val_loss: 1.2247 - val_acc: 0.5301\n",
      "Epoch 266/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.2286 - acc: 0.5167 - val_loss: 1.1909 - val_acc: 0.5404\n",
      "Epoch 267/800\n",
      "6121/6121 [==============================] - 0s 20us/step - loss: 1.2239 - acc: 0.5202 - val_loss: 1.2370 - val_acc: 0.5301\n",
      "Epoch 268/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.2361 - acc: 0.5182 - val_loss: 1.2059 - val_acc: 0.5477\n",
      "Epoch 269/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.2244 - acc: 0.5220 - val_loss: 1.1920 - val_acc: 0.5609\n",
      "Epoch 270/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.2302 - acc: 0.5151 - val_loss: 1.2025 - val_acc: 0.5433\n",
      "Epoch 271/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.2218 - acc: 0.5231 - val_loss: 1.1899 - val_acc: 0.5477\n",
      "Epoch 272/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.2301 - acc: 0.5197 - val_loss: 1.2164 - val_acc: 0.5463\n",
      "Epoch 273/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.2209 - acc: 0.5272 - val_loss: 1.1887 - val_acc: 0.5477\n",
      "Epoch 274/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.2214 - acc: 0.5225 - val_loss: 1.1838 - val_acc: 0.5536\n",
      "Epoch 275/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.2221 - acc: 0.5238 - val_loss: 1.1886 - val_acc: 0.5551\n",
      "Epoch 276/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.2256 - acc: 0.5220 - val_loss: 1.1822 - val_acc: 0.5419\n",
      "Epoch 277/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.2203 - acc: 0.5265 - val_loss: 1.1859 - val_acc: 0.5477\n",
      "Epoch 278/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.2277 - acc: 0.5221 - val_loss: 1.2200 - val_acc: 0.5492\n",
      "Epoch 279/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.2212 - acc: 0.5176 - val_loss: 1.2099 - val_acc: 0.5463\n",
      "Epoch 280/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.2180 - acc: 0.5272 - val_loss: 1.1990 - val_acc: 0.5404\n",
      "Epoch 281/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.2206 - acc: 0.5231 - val_loss: 1.2154 - val_acc: 0.5301\n",
      "Epoch 282/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.2217 - acc: 0.5218 - val_loss: 1.1934 - val_acc: 0.5492\n",
      "Epoch 283/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.2206 - acc: 0.5187 - val_loss: 1.1906 - val_acc: 0.5360\n",
      "Epoch 284/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.2263 - acc: 0.5198 - val_loss: 1.1935 - val_acc: 0.5551\n",
      "Epoch 285/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.2146 - acc: 0.5267 - val_loss: 1.2064 - val_acc: 0.5536\n",
      "Epoch 286/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.2161 - acc: 0.5256 - val_loss: 1.1880 - val_acc: 0.5389\n",
      "Epoch 287/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.2196 - acc: 0.5233 - val_loss: 1.1823 - val_acc: 0.5433\n",
      "Epoch 288/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.2149 - acc: 0.5300 - val_loss: 1.1914 - val_acc: 0.5492\n",
      "Epoch 289/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.2143 - acc: 0.5272 - val_loss: 1.1814 - val_acc: 0.5477\n",
      "Epoch 290/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.2140 - acc: 0.5241 - val_loss: 1.1875 - val_acc: 0.5536\n",
      "Epoch 291/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.2134 - acc: 0.5221 - val_loss: 1.1860 - val_acc: 0.5507\n",
      "Epoch 292/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.2268 - acc: 0.5179 - val_loss: 1.1794 - val_acc: 0.5463\n",
      "Epoch 293/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.2202 - acc: 0.5202 - val_loss: 1.1819 - val_acc: 0.5536\n",
      "Epoch 294/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.2201 - acc: 0.5221 - val_loss: 1.1829 - val_acc: 0.5624\n",
      "Epoch 295/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.2147 - acc: 0.5200 - val_loss: 1.1925 - val_acc: 0.5360\n",
      "Epoch 296/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.2131 - acc: 0.5261 - val_loss: 1.1834 - val_acc: 0.5580\n",
      "Epoch 297/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.2133 - acc: 0.5252 - val_loss: 1.1888 - val_acc: 0.5433\n",
      "Epoch 298/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.2122 - acc: 0.5279 - val_loss: 1.1789 - val_acc: 0.5536\n",
      "Epoch 299/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.2164 - acc: 0.5230 - val_loss: 1.1980 - val_acc: 0.5374\n",
      "Epoch 300/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.2234 - acc: 0.5198 - val_loss: 1.1822 - val_acc: 0.5551\n",
      "Epoch 301/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.2111 - acc: 0.5298 - val_loss: 1.1818 - val_acc: 0.5507\n",
      "Epoch 302/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.2122 - acc: 0.5277 - val_loss: 1.1851 - val_acc: 0.5536\n",
      "Epoch 303/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.2113 - acc: 0.5285 - val_loss: 1.1792 - val_acc: 0.5477\n",
      "Epoch 304/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.2135 - acc: 0.5288 - val_loss: 1.1929 - val_acc: 0.5580\n",
      "Epoch 305/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.2132 - acc: 0.5272 - val_loss: 1.1849 - val_acc: 0.5477\n",
      "Epoch 306/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.2178 - acc: 0.5233 - val_loss: 1.1920 - val_acc: 0.5492\n",
      "Epoch 307/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.2121 - acc: 0.5297 - val_loss: 1.1865 - val_acc: 0.5477\n",
      "Epoch 308/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.2105 - acc: 0.5270 - val_loss: 1.1910 - val_acc: 0.5565\n",
      "Epoch 309/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.2110 - acc: 0.5308 - val_loss: 1.2006 - val_acc: 0.5389\n",
      "Epoch 310/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.2115 - acc: 0.5275 - val_loss: 1.1884 - val_acc: 0.5419\n",
      "Epoch 311/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.2132 - acc: 0.5300 - val_loss: 1.1878 - val_acc: 0.5463\n",
      "Epoch 312/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.2111 - acc: 0.5308 - val_loss: 1.1753 - val_acc: 0.5580\n",
      "Epoch 313/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.2195 - acc: 0.5277 - val_loss: 1.1939 - val_acc: 0.5580\n",
      "Epoch 314/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.2101 - acc: 0.5277 - val_loss: 1.1721 - val_acc: 0.5551\n",
      "Epoch 315/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.2099 - acc: 0.5323 - val_loss: 1.1772 - val_acc: 0.5609\n",
      "Epoch 316/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.2104 - acc: 0.5331 - val_loss: 1.1914 - val_acc: 0.5433\n",
      "Epoch 317/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.2056 - acc: 0.5295 - val_loss: 1.1782 - val_acc: 0.5595\n",
      "Epoch 318/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.2067 - acc: 0.5310 - val_loss: 1.1890 - val_acc: 0.5521\n",
      "Epoch 319/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.2097 - acc: 0.5314 - val_loss: 1.1963 - val_acc: 0.5330\n",
      "Epoch 320/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.2115 - acc: 0.5326 - val_loss: 1.1767 - val_acc: 0.5551\n",
      "Epoch 321/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.2028 - acc: 0.5293 - val_loss: 1.1941 - val_acc: 0.5521\n",
      "Epoch 322/800\n",
      "6121/6121 [==============================] - 0s 20us/step - loss: 1.2067 - acc: 0.5355 - val_loss: 1.1909 - val_acc: 0.5536\n",
      "Epoch 323/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.2119 - acc: 0.5275 - val_loss: 1.1944 - val_acc: 0.5330\n",
      "Epoch 324/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.2066 - acc: 0.5324 - val_loss: 1.2097 - val_acc: 0.5419\n",
      "Epoch 325/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.2116 - acc: 0.5274 - val_loss: 1.1791 - val_acc: 0.5580\n",
      "Epoch 326/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.2031 - acc: 0.5292 - val_loss: 1.1722 - val_acc: 0.5536\n",
      "Epoch 327/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.2022 - acc: 0.5375 - val_loss: 1.1705 - val_acc: 0.5492\n",
      "Epoch 328/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.2059 - acc: 0.5293 - val_loss: 1.1704 - val_acc: 0.5492\n",
      "Epoch 329/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.2071 - acc: 0.5303 - val_loss: 1.2032 - val_acc: 0.5507\n",
      "Epoch 330/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.2006 - acc: 0.5319 - val_loss: 1.1690 - val_acc: 0.5448\n",
      "Epoch 331/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.2086 - acc: 0.5308 - val_loss: 1.1688 - val_acc: 0.5595\n",
      "Epoch 332/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.2011 - acc: 0.5344 - val_loss: 1.1804 - val_acc: 0.5507\n",
      "Epoch 333/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.1987 - acc: 0.5313 - val_loss: 1.1770 - val_acc: 0.5536\n",
      "Epoch 334/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1974 - acc: 0.5360 - val_loss: 1.1717 - val_acc: 0.5477\n",
      "Epoch 335/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.2011 - acc: 0.5331 - val_loss: 1.1725 - val_acc: 0.5653\n",
      "Epoch 336/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.2107 - acc: 0.5290 - val_loss: 1.2210 - val_acc: 0.5433\n",
      "Epoch 337/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.2049 - acc: 0.5334 - val_loss: 1.1718 - val_acc: 0.5507\n",
      "Epoch 338/800\n",
      "6121/6121 [==============================] - 0s 28us/step - loss: 1.2020 - acc: 0.5334 - val_loss: 1.1743 - val_acc: 0.5463\n",
      "Epoch 339/800\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.1993 - acc: 0.5344 - val_loss: 1.1755 - val_acc: 0.5536\n",
      "Epoch 340/800\n",
      "6121/6121 [==============================] - 0s 33us/step - loss: 1.1979 - acc: 0.5352 - val_loss: 1.1690 - val_acc: 0.5521\n",
      "Epoch 341/800\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.1965 - acc: 0.5427 - val_loss: 1.1790 - val_acc: 0.5463\n",
      "Epoch 342/800\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.2022 - acc: 0.5310 - val_loss: 1.1803 - val_acc: 0.5492\n",
      "Epoch 343/800\n",
      "6121/6121 [==============================] - 0s 31us/step - loss: 1.1955 - acc: 0.5336 - val_loss: 1.1684 - val_acc: 0.5595\n",
      "Epoch 344/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1974 - acc: 0.5385 - val_loss: 1.1911 - val_acc: 0.5419\n",
      "Epoch 345/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1999 - acc: 0.5314 - val_loss: 1.1899 - val_acc: 0.5374\n",
      "Epoch 346/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.2035 - acc: 0.5334 - val_loss: 1.1748 - val_acc: 0.5477\n",
      "Epoch 347/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.1946 - acc: 0.5365 - val_loss: 1.1712 - val_acc: 0.5492\n",
      "Epoch 348/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.2056 - acc: 0.5339 - val_loss: 1.1818 - val_acc: 0.5433\n",
      "Epoch 349/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.2018 - acc: 0.5336 - val_loss: 1.2005 - val_acc: 0.5389\n",
      "Epoch 350/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1991 - acc: 0.5381 - val_loss: 1.1764 - val_acc: 0.5624\n",
      "Epoch 351/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1974 - acc: 0.5364 - val_loss: 1.1709 - val_acc: 0.5653\n",
      "Epoch 352/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.1987 - acc: 0.5365 - val_loss: 1.1718 - val_acc: 0.5639\n",
      "Epoch 353/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.1942 - acc: 0.5364 - val_loss: 1.1785 - val_acc: 0.5521\n",
      "Epoch 354/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.2001 - acc: 0.5385 - val_loss: 1.1892 - val_acc: 0.5419\n",
      "Epoch 355/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.1969 - acc: 0.5364 - val_loss: 1.1714 - val_acc: 0.5507\n",
      "Epoch 356/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1963 - acc: 0.5372 - val_loss: 1.1766 - val_acc: 0.5609\n",
      "Epoch 357/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1940 - acc: 0.5422 - val_loss: 1.1791 - val_acc: 0.5477\n",
      "Epoch 358/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.1970 - acc: 0.5373 - val_loss: 1.1643 - val_acc: 0.5521\n",
      "Epoch 359/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.1956 - acc: 0.5396 - val_loss: 1.1700 - val_acc: 0.5492\n",
      "Epoch 360/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1928 - acc: 0.5398 - val_loss: 1.1767 - val_acc: 0.5595\n",
      "Epoch 361/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.1968 - acc: 0.5424 - val_loss: 1.1773 - val_acc: 0.5492\n",
      "Epoch 362/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1966 - acc: 0.5386 - val_loss: 1.1662 - val_acc: 0.5595\n",
      "Epoch 363/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.1934 - acc: 0.5432 - val_loss: 1.1759 - val_acc: 0.5580\n",
      "Epoch 364/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.2028 - acc: 0.5403 - val_loss: 1.1928 - val_acc: 0.5404\n",
      "Epoch 365/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1968 - acc: 0.5318 - val_loss: 1.1651 - val_acc: 0.5536\n",
      "Epoch 366/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1893 - acc: 0.5450 - val_loss: 1.1792 - val_acc: 0.5463\n",
      "Epoch 367/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.1948 - acc: 0.5368 - val_loss: 1.1867 - val_acc: 0.5536\n",
      "Epoch 368/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.1936 - acc: 0.5393 - val_loss: 1.1687 - val_acc: 0.5624\n",
      "Epoch 369/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.1933 - acc: 0.5403 - val_loss: 1.1712 - val_acc: 0.5477\n",
      "Epoch 370/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1917 - acc: 0.5445 - val_loss: 1.1733 - val_acc: 0.5551\n",
      "Epoch 371/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.1935 - acc: 0.5385 - val_loss: 1.1834 - val_acc: 0.5521\n",
      "Epoch 372/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.1957 - acc: 0.5378 - val_loss: 1.1661 - val_acc: 0.5580\n",
      "Epoch 373/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.1921 - acc: 0.5448 - val_loss: 1.1921 - val_acc: 0.5448\n",
      "Epoch 374/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.1942 - acc: 0.5401 - val_loss: 1.1780 - val_acc: 0.5536\n",
      "Epoch 375/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1990 - acc: 0.5406 - val_loss: 1.1632 - val_acc: 0.5448\n",
      "Epoch 376/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.1875 - acc: 0.5439 - val_loss: 1.1675 - val_acc: 0.5580\n",
      "Epoch 377/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1900 - acc: 0.5370 - val_loss: 1.1685 - val_acc: 0.5551\n",
      "Epoch 378/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1926 - acc: 0.5465 - val_loss: 1.1614 - val_acc: 0.5521\n",
      "Epoch 379/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.1871 - acc: 0.5463 - val_loss: 1.1645 - val_acc: 0.5521\n",
      "Epoch 380/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1866 - acc: 0.5455 - val_loss: 1.1844 - val_acc: 0.5477\n",
      "Epoch 381/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.1889 - acc: 0.5430 - val_loss: 1.1661 - val_acc: 0.5477\n",
      "Epoch 382/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1922 - acc: 0.5391 - val_loss: 1.1630 - val_acc: 0.5609\n",
      "Epoch 383/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.1940 - acc: 0.5440 - val_loss: 1.1622 - val_acc: 0.5521\n",
      "Epoch 384/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.1822 - acc: 0.5457 - val_loss: 1.1658 - val_acc: 0.5639\n",
      "Epoch 385/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1910 - acc: 0.5483 - val_loss: 1.1881 - val_acc: 0.5580\n",
      "Epoch 386/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.1898 - acc: 0.5442 - val_loss: 1.1745 - val_acc: 0.5698\n",
      "Epoch 387/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.1857 - acc: 0.5471 - val_loss: 1.1695 - val_acc: 0.5536\n",
      "Epoch 388/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1832 - acc: 0.5447 - val_loss: 1.1677 - val_acc: 0.5551\n",
      "Epoch 389/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1827 - acc: 0.5493 - val_loss: 1.1598 - val_acc: 0.5492\n",
      "Epoch 390/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1881 - acc: 0.5444 - val_loss: 1.1831 - val_acc: 0.5433\n",
      "Epoch 391/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.1902 - acc: 0.5422 - val_loss: 1.1688 - val_acc: 0.5595\n",
      "Epoch 392/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1912 - acc: 0.5489 - val_loss: 1.1824 - val_acc: 0.5580\n",
      "Epoch 393/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1896 - acc: 0.5391 - val_loss: 1.1636 - val_acc: 0.5595\n",
      "Epoch 394/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1861 - acc: 0.5457 - val_loss: 1.1616 - val_acc: 0.5609\n",
      "Epoch 395/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.1879 - acc: 0.5448 - val_loss: 1.1597 - val_acc: 0.5536\n",
      "Epoch 396/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.1831 - acc: 0.5476 - val_loss: 1.1654 - val_acc: 0.5492\n",
      "Epoch 397/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.1883 - acc: 0.5373 - val_loss: 1.1615 - val_acc: 0.5521\n",
      "Epoch 398/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.1827 - acc: 0.5532 - val_loss: 1.1598 - val_acc: 0.5580\n",
      "Epoch 399/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1828 - acc: 0.5430 - val_loss: 1.1661 - val_acc: 0.5668\n",
      "Epoch 400/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1836 - acc: 0.5465 - val_loss: 1.1642 - val_acc: 0.5536\n",
      "Epoch 401/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1838 - acc: 0.5473 - val_loss: 1.1692 - val_acc: 0.5624\n",
      "Epoch 402/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1810 - acc: 0.5502 - val_loss: 1.1807 - val_acc: 0.5624\n",
      "Epoch 403/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1872 - acc: 0.5476 - val_loss: 1.1600 - val_acc: 0.5536\n",
      "Epoch 404/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1816 - acc: 0.5502 - val_loss: 1.1618 - val_acc: 0.5698\n",
      "Epoch 405/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.1791 - acc: 0.5550 - val_loss: 1.1565 - val_acc: 0.5536\n",
      "Epoch 406/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1862 - acc: 0.5497 - val_loss: 1.1712 - val_acc: 0.5521\n",
      "Epoch 407/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1814 - acc: 0.5499 - val_loss: 1.1634 - val_acc: 0.5492\n",
      "Epoch 408/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1814 - acc: 0.5517 - val_loss: 1.1625 - val_acc: 0.5653\n",
      "Epoch 409/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.1833 - acc: 0.5465 - val_loss: 1.1700 - val_acc: 0.5477\n",
      "Epoch 410/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1828 - acc: 0.5470 - val_loss: 1.1742 - val_acc: 0.5536\n",
      "Epoch 411/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.1820 - acc: 0.5535 - val_loss: 1.1567 - val_acc: 0.5653\n",
      "Epoch 412/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1794 - acc: 0.5509 - val_loss: 1.1658 - val_acc: 0.5595\n",
      "Epoch 413/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1874 - acc: 0.5470 - val_loss: 1.2050 - val_acc: 0.5507\n",
      "Epoch 414/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1829 - acc: 0.5488 - val_loss: 1.1750 - val_acc: 0.5698\n",
      "Epoch 415/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.1821 - acc: 0.5488 - val_loss: 1.1593 - val_acc: 0.5551\n",
      "Epoch 416/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1753 - acc: 0.5529 - val_loss: 1.1765 - val_acc: 0.5698\n",
      "Epoch 417/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1902 - acc: 0.5455 - val_loss: 1.1650 - val_acc: 0.5624\n",
      "Epoch 418/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1779 - acc: 0.5578 - val_loss: 1.1595 - val_acc: 0.5536\n",
      "Epoch 419/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.1815 - acc: 0.5515 - val_loss: 1.1944 - val_acc: 0.5374\n",
      "Epoch 420/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.1792 - acc: 0.5475 - val_loss: 1.1775 - val_acc: 0.5477\n",
      "Epoch 421/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.1784 - acc: 0.5533 - val_loss: 1.1592 - val_acc: 0.5595\n",
      "Epoch 422/800\n",
      "6121/6121 [==============================] - 0s 24us/step - loss: 1.1785 - acc: 0.5514 - val_loss: 1.1750 - val_acc: 0.5624\n",
      "Epoch 423/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.1795 - acc: 0.5507 - val_loss: 1.2299 - val_acc: 0.5272\n",
      "Epoch 424/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.1830 - acc: 0.5465 - val_loss: 1.1740 - val_acc: 0.5771\n",
      "Epoch 425/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1815 - acc: 0.5497 - val_loss: 1.1669 - val_acc: 0.5580\n",
      "Epoch 426/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1813 - acc: 0.5499 - val_loss: 1.1653 - val_acc: 0.5580\n",
      "Epoch 427/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1780 - acc: 0.5515 - val_loss: 1.1700 - val_acc: 0.5580\n",
      "Epoch 428/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.1774 - acc: 0.5519 - val_loss: 1.1805 - val_acc: 0.5492\n",
      "Epoch 429/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.1745 - acc: 0.5514 - val_loss: 1.1708 - val_acc: 0.5580\n",
      "Epoch 430/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1763 - acc: 0.5546 - val_loss: 1.1691 - val_acc: 0.5653\n",
      "Epoch 431/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1747 - acc: 0.5494 - val_loss: 1.1632 - val_acc: 0.5624\n",
      "Epoch 432/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.1691 - acc: 0.5566 - val_loss: 1.1584 - val_acc: 0.5463\n",
      "Epoch 433/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1715 - acc: 0.5584 - val_loss: 1.1714 - val_acc: 0.5565\n",
      "Epoch 434/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1688 - acc: 0.5568 - val_loss: 1.1724 - val_acc: 0.5551\n",
      "Epoch 435/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1810 - acc: 0.5463 - val_loss: 1.1553 - val_acc: 0.5565\n",
      "Epoch 436/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.1735 - acc: 0.5566 - val_loss: 1.1595 - val_acc: 0.5595\n",
      "Epoch 437/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1745 - acc: 0.5519 - val_loss: 1.1652 - val_acc: 0.5507\n",
      "Epoch 438/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.1778 - acc: 0.5542 - val_loss: 1.1737 - val_acc: 0.5565\n",
      "Epoch 439/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1722 - acc: 0.5546 - val_loss: 1.1628 - val_acc: 0.5521\n",
      "Epoch 440/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1724 - acc: 0.5569 - val_loss: 1.1633 - val_acc: 0.5551\n",
      "Epoch 441/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1725 - acc: 0.5548 - val_loss: 1.1564 - val_acc: 0.5712\n",
      "Epoch 442/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.1718 - acc: 0.5553 - val_loss: 1.1823 - val_acc: 0.5609\n",
      "Epoch 443/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.1855 - acc: 0.5504 - val_loss: 1.1584 - val_acc: 0.5624\n",
      "Epoch 444/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.1730 - acc: 0.5578 - val_loss: 1.1570 - val_acc: 0.5742\n",
      "Epoch 445/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.1761 - acc: 0.5527 - val_loss: 1.1551 - val_acc: 0.5609\n",
      "Epoch 446/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1782 - acc: 0.5542 - val_loss: 1.1737 - val_acc: 0.5492\n",
      "Epoch 447/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.1690 - acc: 0.5589 - val_loss: 1.1571 - val_acc: 0.5595\n",
      "Epoch 448/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.1697 - acc: 0.5584 - val_loss: 1.1663 - val_acc: 0.5580\n",
      "Epoch 449/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1712 - acc: 0.5548 - val_loss: 1.1641 - val_acc: 0.5756\n",
      "Epoch 450/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.1717 - acc: 0.5576 - val_loss: 1.1729 - val_acc: 0.5668\n",
      "Epoch 451/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.1738 - acc: 0.5591 - val_loss: 1.1634 - val_acc: 0.5565\n",
      "Epoch 452/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.1676 - acc: 0.5560 - val_loss: 1.1735 - val_acc: 0.5316\n",
      "Epoch 453/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1737 - acc: 0.5571 - val_loss: 1.1512 - val_acc: 0.5712\n",
      "Epoch 454/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1757 - acc: 0.5502 - val_loss: 1.1576 - val_acc: 0.5609\n",
      "Epoch 455/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1669 - acc: 0.5543 - val_loss: 1.1711 - val_acc: 0.5609\n",
      "Epoch 456/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1688 - acc: 0.5568 - val_loss: 1.1541 - val_acc: 0.5580\n",
      "Epoch 457/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.1665 - acc: 0.5569 - val_loss: 1.1632 - val_acc: 0.5683\n",
      "Epoch 458/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.1671 - acc: 0.5600 - val_loss: 1.1585 - val_acc: 0.5565\n",
      "Epoch 459/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1719 - acc: 0.5592 - val_loss: 1.1535 - val_acc: 0.5580\n",
      "Epoch 460/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.1684 - acc: 0.5569 - val_loss: 1.1550 - val_acc: 0.5624\n",
      "Epoch 461/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.1619 - acc: 0.5613 - val_loss: 1.1511 - val_acc: 0.5580\n",
      "Epoch 462/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.1691 - acc: 0.5573 - val_loss: 1.1678 - val_acc: 0.5609\n",
      "Epoch 463/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.1653 - acc: 0.5584 - val_loss: 1.1650 - val_acc: 0.5580\n",
      "Epoch 464/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.1647 - acc: 0.5604 - val_loss: 1.1647 - val_acc: 0.5551\n",
      "Epoch 465/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1764 - acc: 0.5548 - val_loss: 1.1519 - val_acc: 0.5683\n",
      "Epoch 466/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1618 - acc: 0.5591 - val_loss: 1.1591 - val_acc: 0.5609\n",
      "Epoch 467/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.1668 - acc: 0.5591 - val_loss: 1.1728 - val_acc: 0.5580\n",
      "Epoch 468/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.1669 - acc: 0.5617 - val_loss: 1.1587 - val_acc: 0.5595\n",
      "Epoch 469/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.1731 - acc: 0.5573 - val_loss: 1.1718 - val_acc: 0.5507\n",
      "Epoch 470/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1703 - acc: 0.5578 - val_loss: 1.1441 - val_acc: 0.5683\n",
      "Epoch 471/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.1708 - acc: 0.5587 - val_loss: 1.1468 - val_acc: 0.5844\n",
      "Epoch 472/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1626 - acc: 0.5569 - val_loss: 1.1450 - val_acc: 0.5786\n",
      "Epoch 473/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1615 - acc: 0.5602 - val_loss: 1.1510 - val_acc: 0.5771\n",
      "Epoch 474/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.1620 - acc: 0.5658 - val_loss: 1.1700 - val_acc: 0.5389\n",
      "Epoch 475/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1642 - acc: 0.5602 - val_loss: 1.1654 - val_acc: 0.5492\n",
      "Epoch 476/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1740 - acc: 0.5595 - val_loss: 1.1445 - val_acc: 0.5668\n",
      "Epoch 477/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.1571 - acc: 0.5711 - val_loss: 1.1478 - val_acc: 0.5624\n",
      "Epoch 478/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1624 - acc: 0.5617 - val_loss: 1.1756 - val_acc: 0.5595\n",
      "Epoch 479/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.1633 - acc: 0.5625 - val_loss: 1.1562 - val_acc: 0.5595\n",
      "Epoch 480/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.1682 - acc: 0.5586 - val_loss: 1.1565 - val_acc: 0.5668\n",
      "Epoch 481/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1628 - acc: 0.5602 - val_loss: 1.1591 - val_acc: 0.5727\n",
      "Epoch 482/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.1646 - acc: 0.5635 - val_loss: 1.1726 - val_acc: 0.5551\n",
      "Epoch 483/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.1784 - acc: 0.5479 - val_loss: 1.1581 - val_acc: 0.5742\n",
      "Epoch 484/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.1694 - acc: 0.5609 - val_loss: 1.1706 - val_acc: 0.5507\n",
      "Epoch 485/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1631 - acc: 0.5604 - val_loss: 1.1611 - val_acc: 0.5639\n",
      "Epoch 486/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1658 - acc: 0.5594 - val_loss: 1.1514 - val_acc: 0.5683\n",
      "Epoch 487/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.1554 - acc: 0.5636 - val_loss: 1.1493 - val_acc: 0.5624\n",
      "Epoch 488/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.1608 - acc: 0.5595 - val_loss: 1.1661 - val_acc: 0.5580\n",
      "Epoch 489/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.1623 - acc: 0.5602 - val_loss: 1.1625 - val_acc: 0.5595\n",
      "Epoch 490/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1646 - acc: 0.5623 - val_loss: 1.1761 - val_acc: 0.5668\n",
      "Epoch 491/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1631 - acc: 0.5664 - val_loss: 1.1457 - val_acc: 0.5639\n",
      "Epoch 492/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.1557 - acc: 0.5684 - val_loss: 1.1494 - val_acc: 0.5756\n",
      "Epoch 493/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.1667 - acc: 0.5578 - val_loss: 1.1755 - val_acc: 0.5653\n",
      "Epoch 494/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1654 - acc: 0.5618 - val_loss: 1.1689 - val_acc: 0.5624\n",
      "Epoch 495/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.1671 - acc: 0.5599 - val_loss: 1.1604 - val_acc: 0.5727\n",
      "Epoch 496/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1628 - acc: 0.5625 - val_loss: 1.1566 - val_acc: 0.5771\n",
      "Epoch 497/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.1593 - acc: 0.5636 - val_loss: 1.1668 - val_acc: 0.5771\n",
      "Epoch 498/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1583 - acc: 0.5654 - val_loss: 1.1634 - val_acc: 0.5830\n",
      "Epoch 499/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.1546 - acc: 0.5623 - val_loss: 1.1556 - val_acc: 0.5727\n",
      "Epoch 500/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1556 - acc: 0.5680 - val_loss: 1.1469 - val_acc: 0.5742\n",
      "Epoch 501/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.1554 - acc: 0.5592 - val_loss: 1.1581 - val_acc: 0.5536\n",
      "Epoch 502/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1580 - acc: 0.5645 - val_loss: 1.1562 - val_acc: 0.5683\n",
      "Epoch 503/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.1510 - acc: 0.5672 - val_loss: 1.1424 - val_acc: 0.5742\n",
      "Epoch 504/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1566 - acc: 0.5602 - val_loss: 1.1520 - val_acc: 0.5668\n",
      "Epoch 505/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1553 - acc: 0.5659 - val_loss: 1.1425 - val_acc: 0.5742\n",
      "Epoch 506/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1522 - acc: 0.5677 - val_loss: 1.1875 - val_acc: 0.5683\n",
      "Epoch 507/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1635 - acc: 0.5651 - val_loss: 1.1458 - val_acc: 0.5668\n",
      "Epoch 508/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1499 - acc: 0.5658 - val_loss: 1.1493 - val_acc: 0.5609\n",
      "Epoch 509/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.1535 - acc: 0.5677 - val_loss: 1.1387 - val_acc: 0.5756\n",
      "Epoch 510/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1520 - acc: 0.5654 - val_loss: 1.1459 - val_acc: 0.5756\n",
      "Epoch 511/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1545 - acc: 0.5682 - val_loss: 1.1480 - val_acc: 0.5698\n",
      "Epoch 512/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1581 - acc: 0.5633 - val_loss: 1.1505 - val_acc: 0.5727\n",
      "Epoch 513/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.1560 - acc: 0.5690 - val_loss: 1.1461 - val_acc: 0.5742\n",
      "Epoch 514/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.1544 - acc: 0.5656 - val_loss: 1.1513 - val_acc: 0.5742\n",
      "Epoch 515/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.1550 - acc: 0.5654 - val_loss: 1.1495 - val_acc: 0.5815\n",
      "Epoch 516/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1591 - acc: 0.5676 - val_loss: 1.1480 - val_acc: 0.5786\n",
      "Epoch 517/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1562 - acc: 0.5682 - val_loss: 1.1819 - val_acc: 0.5551\n",
      "Epoch 518/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1548 - acc: 0.5640 - val_loss: 1.1433 - val_acc: 0.5800\n",
      "Epoch 519/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1569 - acc: 0.5705 - val_loss: 1.1650 - val_acc: 0.5786\n",
      "Epoch 520/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.1609 - acc: 0.5636 - val_loss: 1.1610 - val_acc: 0.5565\n",
      "Epoch 521/800\n",
      "6121/6121 [==============================] - 0s 20us/step - loss: 1.1541 - acc: 0.5669 - val_loss: 1.1594 - val_acc: 0.5580\n",
      "Epoch 522/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.1525 - acc: 0.5694 - val_loss: 1.1368 - val_acc: 0.5609\n",
      "Epoch 523/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.1504 - acc: 0.5710 - val_loss: 1.1467 - val_acc: 0.5830\n",
      "Epoch 524/800\n",
      "6121/6121 [==============================] - 0s 24us/step - loss: 1.1493 - acc: 0.5700 - val_loss: 1.1403 - val_acc: 0.5815\n",
      "Epoch 525/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.1505 - acc: 0.5664 - val_loss: 1.1436 - val_acc: 0.5727\n",
      "Epoch 526/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.1507 - acc: 0.5649 - val_loss: 1.1420 - val_acc: 0.5727\n",
      "Epoch 527/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.1458 - acc: 0.5690 - val_loss: 1.1477 - val_acc: 0.5639\n",
      "Epoch 528/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1457 - acc: 0.5728 - val_loss: 1.1459 - val_acc: 0.5859\n",
      "Epoch 529/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.1448 - acc: 0.5741 - val_loss: 1.1437 - val_acc: 0.5815\n",
      "Epoch 530/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1481 - acc: 0.5694 - val_loss: 1.1425 - val_acc: 0.5683\n",
      "Epoch 531/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1494 - acc: 0.5685 - val_loss: 1.1509 - val_acc: 0.5624\n",
      "Epoch 532/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1472 - acc: 0.5666 - val_loss: 1.1589 - val_acc: 0.5668\n",
      "Epoch 533/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1506 - acc: 0.5667 - val_loss: 1.1401 - val_acc: 0.5742\n",
      "Epoch 534/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1468 - acc: 0.5674 - val_loss: 1.1600 - val_acc: 0.5742\n",
      "Epoch 535/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1518 - acc: 0.5697 - val_loss: 1.1534 - val_acc: 0.5756\n",
      "Epoch 536/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.1484 - acc: 0.5659 - val_loss: 1.1328 - val_acc: 0.5800\n",
      "Epoch 537/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1429 - acc: 0.5734 - val_loss: 1.1459 - val_acc: 0.5742\n",
      "Epoch 538/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1403 - acc: 0.5726 - val_loss: 1.1335 - val_acc: 0.5888\n",
      "Epoch 539/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.1433 - acc: 0.5725 - val_loss: 1.1361 - val_acc: 0.5683\n",
      "Epoch 540/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1462 - acc: 0.5697 - val_loss: 1.1461 - val_acc: 0.5639\n",
      "Epoch 541/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.1423 - acc: 0.5725 - val_loss: 1.1549 - val_acc: 0.5698\n",
      "Epoch 542/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1474 - acc: 0.5672 - val_loss: 1.1305 - val_acc: 0.5815\n",
      "Epoch 543/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.1470 - acc: 0.5723 - val_loss: 1.1485 - val_acc: 0.5815\n",
      "Epoch 544/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1424 - acc: 0.5764 - val_loss: 1.1547 - val_acc: 0.5639\n",
      "Epoch 545/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.1425 - acc: 0.5716 - val_loss: 1.1412 - val_acc: 0.5800\n",
      "Epoch 546/800\n",
      "6121/6121 [==============================] - 0s 20us/step - loss: 1.1380 - acc: 0.5738 - val_loss: 1.1365 - val_acc: 0.5683\n",
      "Epoch 547/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.1452 - acc: 0.5702 - val_loss: 1.1453 - val_acc: 0.5874\n",
      "Epoch 548/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1459 - acc: 0.5685 - val_loss: 1.1571 - val_acc: 0.5712\n",
      "Epoch 549/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1497 - acc: 0.5656 - val_loss: 1.1310 - val_acc: 0.5815\n",
      "Epoch 550/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1380 - acc: 0.5778 - val_loss: 1.1295 - val_acc: 0.5712\n",
      "Epoch 551/800\n",
      "6121/6121 [==============================] - 0s 20us/step - loss: 1.1377 - acc: 0.5782 - val_loss: 1.1368 - val_acc: 0.5874\n",
      "Epoch 552/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1450 - acc: 0.5731 - val_loss: 1.1320 - val_acc: 0.5786\n",
      "Epoch 553/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1464 - acc: 0.5705 - val_loss: 1.1334 - val_acc: 0.5800\n",
      "Epoch 554/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.1436 - acc: 0.5703 - val_loss: 1.1345 - val_acc: 0.5844\n",
      "Epoch 555/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1432 - acc: 0.5661 - val_loss: 1.1462 - val_acc: 0.5742\n",
      "Epoch 556/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1373 - acc: 0.5698 - val_loss: 1.1372 - val_acc: 0.5844\n",
      "Epoch 557/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.1358 - acc: 0.5783 - val_loss: 1.1416 - val_acc: 0.5698\n",
      "Epoch 558/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1410 - acc: 0.5716 - val_loss: 1.1495 - val_acc: 0.5727\n",
      "Epoch 559/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1402 - acc: 0.5721 - val_loss: 1.1287 - val_acc: 0.5859\n",
      "Epoch 560/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1360 - acc: 0.5774 - val_loss: 1.1359 - val_acc: 0.5668\n",
      "Epoch 561/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.1417 - acc: 0.5702 - val_loss: 1.1370 - val_acc: 0.5742\n",
      "Epoch 562/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.1378 - acc: 0.5720 - val_loss: 1.1383 - val_acc: 0.5668\n",
      "Epoch 563/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.1395 - acc: 0.5765 - val_loss: 1.1386 - val_acc: 0.5639\n",
      "Epoch 564/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.1344 - acc: 0.5752 - val_loss: 1.1351 - val_acc: 0.5874\n",
      "Epoch 565/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.1343 - acc: 0.5790 - val_loss: 1.1537 - val_acc: 0.5727\n",
      "Epoch 566/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.1389 - acc: 0.5721 - val_loss: 1.1496 - val_acc: 0.5800\n",
      "Epoch 567/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.1368 - acc: 0.5694 - val_loss: 1.1567 - val_acc: 0.5786\n",
      "Epoch 568/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1352 - acc: 0.5783 - val_loss: 1.1520 - val_acc: 0.5653\n",
      "Epoch 569/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1370 - acc: 0.5715 - val_loss: 1.1565 - val_acc: 0.5698\n",
      "Epoch 570/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.1411 - acc: 0.5726 - val_loss: 1.1288 - val_acc: 0.5844\n",
      "Epoch 571/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.1430 - acc: 0.5708 - val_loss: 1.1565 - val_acc: 0.5771\n",
      "Epoch 572/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.1300 - acc: 0.5752 - val_loss: 1.1259 - val_acc: 0.5815\n",
      "Epoch 573/800\n",
      "6121/6121 [==============================] - 0s 24us/step - loss: 1.1429 - acc: 0.5726 - val_loss: 1.1649 - val_acc: 0.5521\n",
      "Epoch 574/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.1360 - acc: 0.5803 - val_loss: 1.1412 - val_acc: 0.5771\n",
      "Epoch 575/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1342 - acc: 0.5800 - val_loss: 1.1431 - val_acc: 0.5786\n",
      "Epoch 576/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1331 - acc: 0.5749 - val_loss: 1.1302 - val_acc: 0.5800\n",
      "Epoch 577/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.1343 - acc: 0.5705 - val_loss: 1.1366 - val_acc: 0.5844\n",
      "Epoch 578/800\n",
      "6121/6121 [==============================] - 0s 27us/step - loss: 1.1328 - acc: 0.5796 - val_loss: 1.1410 - val_acc: 0.5712\n",
      "Epoch 579/800\n",
      "6121/6121 [==============================] - 0s 24us/step - loss: 1.1303 - acc: 0.5769 - val_loss: 1.1280 - val_acc: 0.5830\n",
      "Epoch 580/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1383 - acc: 0.5734 - val_loss: 1.1382 - val_acc: 0.5903\n",
      "Epoch 581/800\n",
      "6121/6121 [==============================] - 0s 28us/step - loss: 1.1273 - acc: 0.5824 - val_loss: 1.1275 - val_acc: 0.5830\n",
      "Epoch 582/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1324 - acc: 0.5774 - val_loss: 1.1277 - val_acc: 0.5800\n",
      "Epoch 583/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1321 - acc: 0.5692 - val_loss: 1.1277 - val_acc: 0.5786\n",
      "Epoch 584/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.1310 - acc: 0.5757 - val_loss: 1.1244 - val_acc: 0.5844\n",
      "Epoch 585/800\n",
      "6121/6121 [==============================] - 0s 24us/step - loss: 1.1345 - acc: 0.5764 - val_loss: 1.1316 - val_acc: 0.5771\n",
      "Epoch 586/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.1307 - acc: 0.5746 - val_loss: 1.1388 - val_acc: 0.5888\n",
      "Epoch 587/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1344 - acc: 0.5723 - val_loss: 1.1394 - val_acc: 0.5653\n",
      "Epoch 588/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.1288 - acc: 0.5775 - val_loss: 1.1424 - val_acc: 0.5668\n",
      "Epoch 589/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.1312 - acc: 0.5788 - val_loss: 1.1378 - val_acc: 0.5727\n",
      "Epoch 590/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1320 - acc: 0.5707 - val_loss: 1.1424 - val_acc: 0.5771\n",
      "Epoch 591/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.1294 - acc: 0.5796 - val_loss: 1.1500 - val_acc: 0.5668\n",
      "Epoch 592/800\n",
      "6121/6121 [==============================] - 0s 20us/step - loss: 1.1287 - acc: 0.5751 - val_loss: 1.1243 - val_acc: 0.5903\n",
      "Epoch 593/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.1273 - acc: 0.5818 - val_loss: 1.1528 - val_acc: 0.5580\n",
      "Epoch 594/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.1302 - acc: 0.5762 - val_loss: 1.1316 - val_acc: 0.5771\n",
      "Epoch 595/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1348 - acc: 0.5754 - val_loss: 1.1231 - val_acc: 0.5888\n",
      "Epoch 596/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.1376 - acc: 0.5723 - val_loss: 1.1513 - val_acc: 0.5683\n",
      "Epoch 597/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1317 - acc: 0.5769 - val_loss: 1.1801 - val_acc: 0.5551\n",
      "Epoch 598/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1350 - acc: 0.5801 - val_loss: 1.1275 - val_acc: 0.5932\n",
      "Epoch 599/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1289 - acc: 0.5747 - val_loss: 1.1278 - val_acc: 0.5844\n",
      "Epoch 600/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1338 - acc: 0.5754 - val_loss: 1.1471 - val_acc: 0.5815\n",
      "Epoch 601/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.1319 - acc: 0.5801 - val_loss: 1.1426 - val_acc: 0.5771\n",
      "Epoch 602/800\n",
      "6121/6121 [==============================] - 0s 20us/step - loss: 1.1313 - acc: 0.5775 - val_loss: 1.1249 - val_acc: 0.5756\n",
      "Epoch 603/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1272 - acc: 0.5738 - val_loss: 1.1366 - val_acc: 0.5859\n",
      "Epoch 604/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.1241 - acc: 0.5826 - val_loss: 1.1349 - val_acc: 0.5771\n",
      "Epoch 605/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.1252 - acc: 0.5813 - val_loss: 1.1209 - val_acc: 0.5888\n",
      "Epoch 606/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1243 - acc: 0.5811 - val_loss: 1.1462 - val_acc: 0.5698\n",
      "Epoch 607/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.1252 - acc: 0.5788 - val_loss: 1.1305 - val_acc: 0.5815\n",
      "Epoch 608/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1234 - acc: 0.5831 - val_loss: 1.1432 - val_acc: 0.5536\n",
      "Epoch 609/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.1227 - acc: 0.5806 - val_loss: 1.1364 - val_acc: 0.5653\n",
      "Epoch 610/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1242 - acc: 0.5756 - val_loss: 1.1356 - val_acc: 0.5844\n",
      "Epoch 611/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.1285 - acc: 0.5796 - val_loss: 1.1357 - val_acc: 0.5742\n",
      "Epoch 612/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.1261 - acc: 0.5777 - val_loss: 1.1292 - val_acc: 0.5786\n",
      "Epoch 613/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1232 - acc: 0.5782 - val_loss: 1.1253 - val_acc: 0.5756\n",
      "Epoch 614/800\n",
      "6121/6121 [==============================] - 0s 24us/step - loss: 1.1246 - acc: 0.5759 - val_loss: 1.1266 - val_acc: 0.5668\n",
      "Epoch 615/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.1237 - acc: 0.5790 - val_loss: 1.1242 - val_acc: 0.5815\n",
      "Epoch 616/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.1179 - acc: 0.5850 - val_loss: 1.1267 - val_acc: 0.5756\n",
      "Epoch 617/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1224 - acc: 0.5774 - val_loss: 1.1358 - val_acc: 0.5786\n",
      "Epoch 618/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1277 - acc: 0.5790 - val_loss: 1.1278 - val_acc: 0.5815\n",
      "Epoch 619/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1270 - acc: 0.5785 - val_loss: 1.1415 - val_acc: 0.5800\n",
      "Epoch 620/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1345 - acc: 0.5783 - val_loss: 1.1226 - val_acc: 0.5874\n",
      "Epoch 621/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1199 - acc: 0.5841 - val_loss: 1.1377 - val_acc: 0.5800\n",
      "Epoch 622/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.1248 - acc: 0.5829 - val_loss: 1.1464 - val_acc: 0.5800\n",
      "Epoch 623/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1261 - acc: 0.5800 - val_loss: 1.1347 - val_acc: 0.5888\n",
      "Epoch 624/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1232 - acc: 0.5790 - val_loss: 1.1488 - val_acc: 0.5698\n",
      "Epoch 625/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.1262 - acc: 0.5738 - val_loss: 1.1306 - val_acc: 0.5844\n",
      "Epoch 626/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.1232 - acc: 0.5741 - val_loss: 1.1359 - val_acc: 0.5712\n",
      "Epoch 627/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.1238 - acc: 0.5821 - val_loss: 1.1333 - val_acc: 0.5712\n",
      "Epoch 628/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1327 - acc: 0.5775 - val_loss: 1.1495 - val_acc: 0.5698\n",
      "Epoch 629/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.1328 - acc: 0.5733 - val_loss: 1.1219 - val_acc: 0.5888\n",
      "Epoch 630/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.1188 - acc: 0.5756 - val_loss: 1.1637 - val_acc: 0.5595\n",
      "Epoch 631/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1210 - acc: 0.5837 - val_loss: 1.1185 - val_acc: 0.5918\n",
      "Epoch 632/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1182 - acc: 0.5850 - val_loss: 1.1329 - val_acc: 0.5786\n",
      "Epoch 633/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1214 - acc: 0.5811 - val_loss: 1.1314 - val_acc: 0.5830\n",
      "Epoch 634/800\n",
      "6121/6121 [==============================] - 0s 20us/step - loss: 1.1149 - acc: 0.5847 - val_loss: 1.1220 - val_acc: 0.5947\n",
      "Epoch 635/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.1212 - acc: 0.5829 - val_loss: 1.1196 - val_acc: 0.5903\n",
      "Epoch 636/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.1133 - acc: 0.5862 - val_loss: 1.1269 - val_acc: 0.5830\n",
      "Epoch 637/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1237 - acc: 0.5759 - val_loss: 1.1293 - val_acc: 0.5830\n",
      "Epoch 638/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1173 - acc: 0.5842 - val_loss: 1.1339 - val_acc: 0.5800\n",
      "Epoch 639/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.1193 - acc: 0.5837 - val_loss: 1.1366 - val_acc: 0.5844\n",
      "Epoch 640/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1119 - acc: 0.5876 - val_loss: 1.1128 - val_acc: 0.5918\n",
      "Epoch 641/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1349 - acc: 0.5738 - val_loss: 1.1721 - val_acc: 0.5595\n",
      "Epoch 642/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1292 - acc: 0.5752 - val_loss: 1.1373 - val_acc: 0.5844\n",
      "Epoch 643/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.1262 - acc: 0.5756 - val_loss: 1.1242 - val_acc: 0.5800\n",
      "Epoch 644/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.1147 - acc: 0.5878 - val_loss: 1.1153 - val_acc: 0.5918\n",
      "Epoch 645/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1174 - acc: 0.5811 - val_loss: 1.1321 - val_acc: 0.5742\n",
      "Epoch 646/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.1209 - acc: 0.5728 - val_loss: 1.1548 - val_acc: 0.5800\n",
      "Epoch 647/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.1149 - acc: 0.5783 - val_loss: 1.1312 - val_acc: 0.5815\n",
      "Epoch 648/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.1163 - acc: 0.5847 - val_loss: 1.1192 - val_acc: 0.6006\n",
      "Epoch 649/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1117 - acc: 0.5836 - val_loss: 1.1205 - val_acc: 0.5874\n",
      "Epoch 650/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1114 - acc: 0.5886 - val_loss: 1.1148 - val_acc: 0.5859\n",
      "Epoch 651/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.1156 - acc: 0.5847 - val_loss: 1.1428 - val_acc: 0.5830\n",
      "Epoch 652/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1130 - acc: 0.5827 - val_loss: 1.1277 - val_acc: 0.5947\n",
      "Epoch 653/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1218 - acc: 0.5826 - val_loss: 1.1258 - val_acc: 0.5844\n",
      "Epoch 654/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1174 - acc: 0.5803 - val_loss: 1.1594 - val_acc: 0.5742\n",
      "Epoch 655/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1140 - acc: 0.5826 - val_loss: 1.1166 - val_acc: 0.5815\n",
      "Epoch 656/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1204 - acc: 0.5829 - val_loss: 1.1161 - val_acc: 0.5918\n",
      "Epoch 657/800\n",
      "6121/6121 [==============================] - 0s 29us/step - loss: 1.1202 - acc: 0.5827 - val_loss: 1.1244 - val_acc: 0.5888\n",
      "Epoch 658/800\n",
      "6121/6121 [==============================] - 0s 33us/step - loss: 1.1234 - acc: 0.5749 - val_loss: 1.1207 - val_acc: 0.5786\n",
      "Epoch 659/800\n",
      "6121/6121 [==============================] - 0s 34us/step - loss: 1.1098 - acc: 0.5831 - val_loss: 1.1180 - val_acc: 0.6050\n",
      "Epoch 660/800\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.1100 - acc: 0.5839 - val_loss: 1.1491 - val_acc: 0.5668\n",
      "Epoch 661/800\n",
      "6121/6121 [==============================] - 0s 32us/step - loss: 1.1107 - acc: 0.5855 - val_loss: 1.1333 - val_acc: 0.5639\n",
      "Epoch 662/800\n",
      "6121/6121 [==============================] - 0s 30us/step - loss: 1.1148 - acc: 0.5850 - val_loss: 1.1384 - val_acc: 0.5786\n",
      "Epoch 663/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.1091 - acc: 0.5868 - val_loss: 1.1130 - val_acc: 0.5859\n",
      "Epoch 664/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.1174 - acc: 0.5821 - val_loss: 1.1373 - val_acc: 0.5756\n",
      "Epoch 665/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.1125 - acc: 0.5821 - val_loss: 1.1383 - val_acc: 0.5786\n",
      "Epoch 666/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1120 - acc: 0.5865 - val_loss: 1.1303 - val_acc: 0.5727\n",
      "Epoch 667/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1116 - acc: 0.5788 - val_loss: 1.1403 - val_acc: 0.6021\n",
      "Epoch 668/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1088 - acc: 0.5862 - val_loss: 1.1214 - val_acc: 0.5991\n",
      "Epoch 669/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1079 - acc: 0.5863 - val_loss: 1.1326 - val_acc: 0.5712\n",
      "Epoch 670/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.1082 - acc: 0.5875 - val_loss: 1.1187 - val_acc: 0.5800\n",
      "Epoch 671/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1110 - acc: 0.5862 - val_loss: 1.1095 - val_acc: 0.5903\n",
      "Epoch 672/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1152 - acc: 0.5821 - val_loss: 1.1335 - val_acc: 0.5756\n",
      "Epoch 673/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.1131 - acc: 0.5849 - val_loss: 1.1213 - val_acc: 0.6006\n",
      "Epoch 674/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.1074 - acc: 0.5881 - val_loss: 1.1191 - val_acc: 0.5786\n",
      "Epoch 675/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.1154 - acc: 0.5818 - val_loss: 1.1203 - val_acc: 0.5830\n",
      "Epoch 676/800\n",
      "6121/6121 [==============================] - 0s 25us/step - loss: 1.1089 - acc: 0.5839 - val_loss: 1.1150 - val_acc: 0.5800\n",
      "Epoch 677/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.1112 - acc: 0.5821 - val_loss: 1.1195 - val_acc: 0.5977\n",
      "Epoch 678/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.1089 - acc: 0.5867 - val_loss: 1.1296 - val_acc: 0.5815\n",
      "Epoch 679/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.1153 - acc: 0.5816 - val_loss: 1.1270 - val_acc: 0.5786\n",
      "Epoch 680/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.1069 - acc: 0.5847 - val_loss: 1.1200 - val_acc: 0.5932\n",
      "Epoch 681/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.1033 - acc: 0.5914 - val_loss: 1.1235 - val_acc: 0.5844\n",
      "Epoch 682/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.1102 - acc: 0.5849 - val_loss: 1.1186 - val_acc: 0.5918\n",
      "Epoch 683/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1069 - acc: 0.5886 - val_loss: 1.1238 - val_acc: 0.5800\n",
      "Epoch 684/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1058 - acc: 0.5901 - val_loss: 1.1259 - val_acc: 0.5786\n",
      "Epoch 685/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1080 - acc: 0.5872 - val_loss: 1.1278 - val_acc: 0.5712\n",
      "Epoch 686/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1091 - acc: 0.5845 - val_loss: 1.1091 - val_acc: 0.5962\n",
      "Epoch 687/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1041 - acc: 0.5898 - val_loss: 1.1205 - val_acc: 0.5800\n",
      "Epoch 688/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.1052 - acc: 0.5862 - val_loss: 1.1215 - val_acc: 0.5800\n",
      "Epoch 689/800\n",
      "6121/6121 [==============================] - 0s 25us/step - loss: 1.1174 - acc: 0.5783 - val_loss: 1.1158 - val_acc: 0.5786\n",
      "Epoch 690/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.1020 - acc: 0.5906 - val_loss: 1.1123 - val_acc: 0.5844\n",
      "Epoch 691/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.1063 - acc: 0.5839 - val_loss: 1.1238 - val_acc: 0.5918\n",
      "Epoch 692/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1110 - acc: 0.5891 - val_loss: 1.1075 - val_acc: 0.6006\n",
      "Epoch 693/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.0997 - acc: 0.5896 - val_loss: 1.1186 - val_acc: 0.5815\n",
      "Epoch 694/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1062 - acc: 0.5883 - val_loss: 1.1163 - val_acc: 0.5874\n",
      "Epoch 695/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1069 - acc: 0.5896 - val_loss: 1.1117 - val_acc: 0.5947\n",
      "Epoch 696/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1022 - acc: 0.5899 - val_loss: 1.1127 - val_acc: 0.5888\n",
      "Epoch 697/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.1060 - acc: 0.5875 - val_loss: 1.1106 - val_acc: 0.6050\n",
      "Epoch 698/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1065 - acc: 0.5875 - val_loss: 1.1144 - val_acc: 0.5815\n",
      "Epoch 699/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1027 - acc: 0.5921 - val_loss: 1.1361 - val_acc: 0.5800\n",
      "Epoch 700/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.1108 - acc: 0.5862 - val_loss: 1.1136 - val_acc: 0.5771\n",
      "Epoch 701/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1059 - acc: 0.5850 - val_loss: 1.1232 - val_acc: 0.5786\n",
      "Epoch 702/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1033 - acc: 0.5883 - val_loss: 1.1321 - val_acc: 0.5756\n",
      "Epoch 703/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1058 - acc: 0.5898 - val_loss: 1.1169 - val_acc: 0.5800\n",
      "Epoch 704/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.1095 - acc: 0.5837 - val_loss: 1.1077 - val_acc: 0.6006\n",
      "Epoch 705/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.1028 - acc: 0.5860 - val_loss: 1.1405 - val_acc: 0.5771\n",
      "Epoch 706/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1094 - acc: 0.5878 - val_loss: 1.1108 - val_acc: 0.5962\n",
      "Epoch 707/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1031 - acc: 0.5850 - val_loss: 1.1407 - val_acc: 0.5698\n",
      "Epoch 708/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.0997 - acc: 0.5837 - val_loss: 1.1118 - val_acc: 0.5888\n",
      "Epoch 709/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1050 - acc: 0.5831 - val_loss: 1.1265 - val_acc: 0.5874\n",
      "Epoch 710/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.1055 - acc: 0.5890 - val_loss: 1.1262 - val_acc: 0.5859\n",
      "Epoch 711/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.1059 - acc: 0.5862 - val_loss: 1.1183 - val_acc: 0.5830\n",
      "Epoch 712/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1092 - acc: 0.5811 - val_loss: 1.1103 - val_acc: 0.5815\n",
      "Epoch 713/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.1024 - acc: 0.5859 - val_loss: 1.1082 - val_acc: 0.5874\n",
      "Epoch 714/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1041 - acc: 0.5860 - val_loss: 1.1276 - val_acc: 0.5844\n",
      "Epoch 715/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.1037 - acc: 0.5872 - val_loss: 1.1259 - val_acc: 0.5698\n",
      "Epoch 716/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.0996 - acc: 0.5886 - val_loss: 1.1137 - val_acc: 0.5932\n",
      "Epoch 717/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.1001 - acc: 0.5922 - val_loss: 1.1196 - val_acc: 0.5815\n",
      "Epoch 718/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1019 - acc: 0.5893 - val_loss: 1.1340 - val_acc: 0.5668\n",
      "Epoch 719/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.1016 - acc: 0.5886 - val_loss: 1.1173 - val_acc: 0.5991\n",
      "Epoch 720/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.1056 - acc: 0.5847 - val_loss: 1.1411 - val_acc: 0.5830\n",
      "Epoch 721/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.1023 - acc: 0.5898 - val_loss: 1.1066 - val_acc: 0.5932\n",
      "Epoch 722/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.0991 - acc: 0.5942 - val_loss: 1.1453 - val_acc: 0.5771\n",
      "Epoch 723/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.0989 - acc: 0.5899 - val_loss: 1.1123 - val_acc: 0.5815\n",
      "Epoch 724/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.0952 - acc: 0.5912 - val_loss: 1.1239 - val_acc: 0.5962\n",
      "Epoch 725/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1007 - acc: 0.5863 - val_loss: 1.1371 - val_acc: 0.5786\n",
      "Epoch 726/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.1027 - acc: 0.5859 - val_loss: 1.1218 - val_acc: 0.5756\n",
      "Epoch 727/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.0997 - acc: 0.5911 - val_loss: 1.1190 - val_acc: 0.5962\n",
      "Epoch 728/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.1024 - acc: 0.5896 - val_loss: 1.1152 - val_acc: 0.5874\n",
      "Epoch 729/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.0963 - acc: 0.5921 - val_loss: 1.1254 - val_acc: 0.5800\n",
      "Epoch 730/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.0977 - acc: 0.5898 - val_loss: 1.1117 - val_acc: 0.5859\n",
      "Epoch 731/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.0989 - acc: 0.5903 - val_loss: 1.1107 - val_acc: 0.5903\n",
      "Epoch 732/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.0920 - acc: 0.5908 - val_loss: 1.1116 - val_acc: 0.5947\n",
      "Epoch 733/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.1003 - acc: 0.5893 - val_loss: 1.1643 - val_acc: 0.5551\n",
      "Epoch 734/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1150 - acc: 0.5751 - val_loss: 1.1310 - val_acc: 0.5786\n",
      "Epoch 735/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.0973 - acc: 0.5885 - val_loss: 1.1202 - val_acc: 0.5786\n",
      "Epoch 736/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.1015 - acc: 0.5901 - val_loss: 1.1139 - val_acc: 0.5800\n",
      "Epoch 737/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1007 - acc: 0.5854 - val_loss: 1.1251 - val_acc: 0.5859\n",
      "Epoch 738/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.1008 - acc: 0.5904 - val_loss: 1.1006 - val_acc: 0.5991\n",
      "Epoch 739/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.0983 - acc: 0.5876 - val_loss: 1.1128 - val_acc: 0.5786\n",
      "Epoch 740/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.0968 - acc: 0.5922 - val_loss: 1.1182 - val_acc: 0.5874\n",
      "Epoch 741/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.0994 - acc: 0.5896 - val_loss: 1.1221 - val_acc: 0.5874\n",
      "Epoch 742/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.1009 - acc: 0.5832 - val_loss: 1.1230 - val_acc: 0.5859\n",
      "Epoch 743/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.1015 - acc: 0.5867 - val_loss: 1.1103 - val_acc: 0.5830\n",
      "Epoch 744/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.0953 - acc: 0.5911 - val_loss: 1.1139 - val_acc: 0.5844\n",
      "Epoch 745/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.0901 - acc: 0.5935 - val_loss: 1.1142 - val_acc: 0.5800\n",
      "Epoch 746/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.0919 - acc: 0.5906 - val_loss: 1.1187 - val_acc: 0.5918\n",
      "Epoch 747/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.0997 - acc: 0.5885 - val_loss: 1.1204 - val_acc: 0.5800\n",
      "Epoch 748/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.1025 - acc: 0.5914 - val_loss: 1.1469 - val_acc: 0.5815\n",
      "Epoch 749/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.0978 - acc: 0.5930 - val_loss: 1.1241 - val_acc: 0.5874\n",
      "Epoch 750/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.1004 - acc: 0.5899 - val_loss: 1.1104 - val_acc: 0.5830\n",
      "Epoch 751/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.0929 - acc: 0.5899 - val_loss: 1.1086 - val_acc: 0.5888\n",
      "Epoch 752/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.0963 - acc: 0.5885 - val_loss: 1.1284 - val_acc: 0.5727\n",
      "Epoch 753/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.0981 - acc: 0.5834 - val_loss: 1.1165 - val_acc: 0.5932\n",
      "Epoch 754/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.0891 - acc: 0.5950 - val_loss: 1.1095 - val_acc: 0.5815\n",
      "Epoch 755/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.0960 - acc: 0.5831 - val_loss: 1.1057 - val_acc: 0.5962\n",
      "Epoch 756/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.0986 - acc: 0.5852 - val_loss: 1.1055 - val_acc: 0.5859\n",
      "Epoch 757/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.0974 - acc: 0.5926 - val_loss: 1.1203 - val_acc: 0.5756\n",
      "Epoch 758/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.0946 - acc: 0.5903 - val_loss: 1.1002 - val_acc: 0.6021\n",
      "Epoch 759/800\n",
      "6121/6121 [==============================] - 0s 20us/step - loss: 1.0948 - acc: 0.5911 - val_loss: 1.1058 - val_acc: 0.6065\n",
      "Epoch 760/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.0986 - acc: 0.5839 - val_loss: 1.1155 - val_acc: 0.5756\n",
      "Epoch 761/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.0965 - acc: 0.5886 - val_loss: 1.1065 - val_acc: 0.5918\n",
      "Epoch 762/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.0957 - acc: 0.5914 - val_loss: 1.1351 - val_acc: 0.5903\n",
      "Epoch 763/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.0921 - acc: 0.5899 - val_loss: 1.1196 - val_acc: 0.5844\n",
      "Epoch 764/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.0964 - acc: 0.5929 - val_loss: 1.1171 - val_acc: 0.5786\n",
      "Epoch 765/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.0966 - acc: 0.5873 - val_loss: 1.1046 - val_acc: 0.5918\n",
      "Epoch 766/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.0904 - acc: 0.5901 - val_loss: 1.1316 - val_acc: 0.5727\n",
      "Epoch 767/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.0957 - acc: 0.5872 - val_loss: 1.1122 - val_acc: 0.5800\n",
      "Epoch 768/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.0915 - acc: 0.5867 - val_loss: 1.1096 - val_acc: 0.5918\n",
      "Epoch 769/800\n",
      "6121/6121 [==============================] - 0s 20us/step - loss: 1.0930 - acc: 0.5896 - val_loss: 1.1101 - val_acc: 0.5977\n",
      "Epoch 770/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.0906 - acc: 0.5868 - val_loss: 1.1464 - val_acc: 0.5800\n",
      "Epoch 771/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.0956 - acc: 0.5908 - val_loss: 1.1084 - val_acc: 0.5859\n",
      "Epoch 772/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.0863 - acc: 0.6007 - val_loss: 1.1120 - val_acc: 0.5977\n",
      "Epoch 773/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.0975 - acc: 0.5899 - val_loss: 1.1138 - val_acc: 0.5859\n",
      "Epoch 774/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.0902 - acc: 0.5893 - val_loss: 1.1163 - val_acc: 0.5844\n",
      "Epoch 775/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.0910 - acc: 0.5873 - val_loss: 1.1110 - val_acc: 0.5771\n",
      "Epoch 776/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.0949 - acc: 0.5893 - val_loss: 1.1471 - val_acc: 0.5580\n",
      "Epoch 777/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.0896 - acc: 0.5893 - val_loss: 1.1148 - val_acc: 0.5815\n",
      "Epoch 778/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.0872 - acc: 0.5945 - val_loss: 1.1285 - val_acc: 0.5786\n",
      "Epoch 779/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.0942 - acc: 0.5924 - val_loss: 1.1026 - val_acc: 0.5977\n",
      "Epoch 780/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.0915 - acc: 0.5932 - val_loss: 1.1229 - val_acc: 0.5918\n",
      "Epoch 781/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.0956 - acc: 0.5924 - val_loss: 1.1479 - val_acc: 0.5639\n",
      "Epoch 782/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.0888 - acc: 0.5899 - val_loss: 1.1007 - val_acc: 0.5977\n",
      "Epoch 783/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.0878 - acc: 0.5953 - val_loss: 1.1161 - val_acc: 0.5786\n",
      "Epoch 784/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.0861 - acc: 0.5924 - val_loss: 1.1053 - val_acc: 0.5962\n",
      "Epoch 785/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.0928 - acc: 0.5839 - val_loss: 1.1033 - val_acc: 0.5991\n",
      "Epoch 786/800\n",
      "6121/6121 [==============================] - 0s 23us/step - loss: 1.0890 - acc: 0.5940 - val_loss: 1.1054 - val_acc: 0.5932\n",
      "Epoch 787/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.0894 - acc: 0.5932 - val_loss: 1.1109 - val_acc: 0.5844\n",
      "Epoch 788/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.0822 - acc: 0.5976 - val_loss: 1.0980 - val_acc: 0.5859\n",
      "Epoch 789/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.0867 - acc: 0.5919 - val_loss: 1.1103 - val_acc: 0.5815\n",
      "Epoch 790/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.0972 - acc: 0.5837 - val_loss: 1.1360 - val_acc: 0.5742\n",
      "Epoch 791/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.0866 - acc: 0.5952 - val_loss: 1.1127 - val_acc: 0.5771\n",
      "Epoch 792/800\n",
      "6121/6121 [==============================] - 0s 21us/step - loss: 1.1008 - acc: 0.5875 - val_loss: 1.1047 - val_acc: 0.5815\n",
      "Epoch 793/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.0893 - acc: 0.5883 - val_loss: 1.1079 - val_acc: 0.5888\n",
      "Epoch 794/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.0982 - acc: 0.5873 - val_loss: 1.1056 - val_acc: 0.5859\n",
      "Epoch 795/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.0841 - acc: 0.5919 - val_loss: 1.1031 - val_acc: 0.5844\n",
      "Epoch 796/800\n",
      "6121/6121 [==============================] - 0s 20us/step - loss: 1.0833 - acc: 0.5961 - val_loss: 1.1088 - val_acc: 0.5888\n",
      "Epoch 797/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.0816 - acc: 0.5989 - val_loss: 1.1035 - val_acc: 0.5815\n",
      "Epoch 798/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.0853 - acc: 0.5899 - val_loss: 1.1081 - val_acc: 0.5932\n",
      "Epoch 799/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.0886 - acc: 0.5875 - val_loss: 1.1617 - val_acc: 0.5668\n",
      "Epoch 800/800\n",
      "6121/6121 [==============================] - 0s 22us/step - loss: 1.0907 - acc: 0.5894 - val_loss: 1.1220 - val_acc: 0.5786\n"
     ]
    }
   ],
   "source": [
    "model_history = model.fit(x=x_train, y=y_train,\n",
    "                          batch_size=batch_size,\n",
    "                          epochs=epochs,\n",
    "                          validation_split=1 - train_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = model_history.history['loss']\n",
    "train_acc = model_history.history['acc']\n",
    "valid_loss = model_history.history['val_loss']\n",
    "valid_acc = model_history.history['val_acc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvqOYd8AAAIABJREFUeJzt3Xd8FHX+x/HXB4h0pYpI91RUPECIiKKc5WyoYEFFPQsWbHdnF/VOxcP7nfUsZ0VF9AQREM+CYjkLJ4galBIBKQoSakBAqoTk8/tjdlNINglhN5PdvJ+Pxzwy5bszn2w27539zuyMuTsiIpJaaoRdgIiIxJ/CXUQkBSncRURSkMJdRCQFKdxFRFKQwl1EJAUp3EVEUpDCXVKemS0ys9+HXYdIZVK4i4ikIIW7VFtmdoWZLTCzn83sLTPbOzLfzOwRM1tlZuvNbKaZHRxZ1sfMZpvZBjNbamY3h/tbiJRM4S7VkpkdC/wDOAdoCSwGRkcWnwD0BvYHGgHnAmsiy14ArnT3hsDBwMeVWLZIudUKuwCRkFwADHf3bwDM7HZgrZm1B3KAhsABwFfuPqfQ43KAg8xshruvBdZWatUi5aQ9d6mu9ibYWwfA3TcS7J23cvePgSeAJ4GVZjbMzHaPND0L6AMsNrPPzOzwSq5bpFwU7lJdLQPaRSfMrD7QFFgK4O6Pu3t3oBNB98wtkflfu3s/YE/gP8CYSq5bpFwU7lJdpJlZnehAEMoDzayrmdUG/g/40t0XmdmhZnaYmaUBm4CtQK6Z7WZmF5jZHu6eA/wC5Ib2G4mUQuEu1cW7wJZCw1HAncDrwHLgN8CASNvdgecI+tMXE3TXPBRZdiGwyMx+Aa4C/lBJ9YvsFNPNOkREUo/23EVEUpDCXUQkBSncRURSkMJdRCQFhfYN1WbNmnn79u3D2ryISFKaNm3aandvXla70MK9ffv2ZGRkhLV5EZGkZGaLy26lbhkRkZSkcBcRSUEKdxGRFFRmn7uZtQFeBvYC8oBh7v7YDm0uAAZHJjcCV7v7jDjXKiLVXE5ODllZWWzdujXsUhKuTp06tG7dmrS0tAo9vjwHVLcDN7n7N2bWEJhmZh+6++xCbX4Efufua83sZGAYcFiFKhIRiSErK4uGDRvSvn17zCzschLG3VmzZg1ZWVl06NChQusos1vG3ZdHb2jg7huAOUCrHdpMidy4AGAq0LpC1YiIlGLr1q00bdo0pYMdwMxo2rTpLn1C2ak+98hdag4Bviyl2WXAezEeP8jMMswsIzs7e2c2LSICkPLBHrWrv2e5w93MGhBcHvV6d/8lRptjCMJ9cEnL3X2Yu6e7e3rz5mWeg1+izEy46y5YtapCDxcRqRbKFe6Rmxa8Dox09/Ex2nQGngf6ufuaktrEw5w5MHQoaMdfRCrbunXreOqpp3b6cX369GHdunUJqCi2MsPdgs8GLwBz3P2fMdq0BcYDF7r7vPiWWFSNSMV5eYnciohIcbHCPTe39BtyvfvuuzRq1ChRZZWoPGfL9CK4+8wsM5semXcH0BbA3Z8B7iK4/+RTkX6i7e6eHv9yIdoNpXAXkcp22223sXDhQrp27UpaWhoNGjSgZcuWTJ8+ndmzZ3P66aezZMkStm7dynXXXcegQYOAgsutbNy4kZNPPpkjjzySKVOm0KpVK958803q1q0b91rLDHd3/xwotWff3S8HLo9XUaWJ7rnrBlIi1dv118P06WW32xldu8Kjj8Zeft9995GZmcn06dP59NNPOeWUU8jMzMw/XXH48OE0adKELVu2cOihh3LWWWfRtGnTIuuYP38+r776Ks899xznnHMOr7/+On/4Q/zv1hjahcMqSt0yIlJV9OjRo8h56I8//jhvvPEGAEuWLGH+/PnFwr1Dhw507doVgO7du7No0aKE1JZ04a5uGRGB0vewK0v9+vXzxz/99FM++ugjvvjiC+rVq8fRRx9d4nnqtWvXzh+vWbMmW7ZsSUhtSXdtGXXLiEhYGjZsyIYNG0pctn79eho3bky9evWYO3cuU6dOreTqitKeu4hIOTVt2pRevXpx8MEHU7duXVq0aJG/7KSTTuKZZ56hc+fOdOzYkZ49e4ZYaRKGu/rcRSRMo0aNKnF+7dq1ee+9Er+cn9+v3qxZMzIzM/Pn33zzzXGvL0rdMiIiKSjpwl3dMiIiZUu6cNeeu4hI2ZI23LXnLiISW9KFu7plRETKlnThrm4ZEZGyJW24a89dRKq6Bg0aALBs2TL69+9fYpujjz6ajIyMuG876cJd3TIikmz23ntvxo0bV6nbTNovMalbRkQq2+DBg2nXrh3XXHMNAEOGDMHMmDRpEmvXriUnJ4d7772Xfv36FXncokWLOPXUU8nMzGTLli0MHDiQ2bNnc+CBBybs2jJJG+7acxep5kK45u+AAQO4/vrr88N9zJgxTJw4kRtuuIHdd9+d1atX07NnT/r27RvzHqhPP/009erVY+bMmcycOZNu3brF93eISLpwV7eMiITlkEMOYdWqVSxbtozs7GwaN25My5YtueGGG5g0aRI1atRg6dKlrFy5kr322qvEdUyaNIk///nPAHTu3JnOnTsnpNakC3d1y4gIENo1f/v378+4ceNYsWIFAwYMYOTIkWRnZzNt2jTS0tJo3759iZf6LSzWXn08Jd0BVXXLiEiYBgwYwOjRoxk3bhz9+/dn/fr17LnnnqSlpfHJJ5+wePHiUh/fu3dvRo4cCUBmZiYzZ85MSJ1Jt+eubhkRCVOnTp3YsGEDrVq1omXLllxwwQWcdtpppKen07VrVw444IBSH3/11VczcOBAOnfuTNeuXenRo0dC6ky6cFe3jIiEbdasWfnjzZo144svviix3caNG4HgBtnRS/3WrVuX0aNHJ7zGpOuW0Z67iEjZki7c1ecuIlK2pA13dcuIVE9eTf75d/X3TLpwV7eMSPVVp04d1qxZk/IB7+6sWbOGOnXqVHgdOqAqIkmjdevWZGVlkZ2dHXYpCVenTh1at25d4ccnbbhrz12k+klLS6NDhw5hl5EU1C0jIpKCygx3M2tjZp+Y2Rwz+87MriuhjZnZ42a2wMxmmlliroSDumVERMqjPN0y24Gb3P0bM2sITDOzD919dqE2JwP7RYbDgKcjP+NO3TIiImUrc8/d3Ze7+zeR8Q3AHKDVDs36AS97YCrQyMxaxr1a1C0jIlIeO9XnbmbtgUOAL3dY1ApYUmg6i+JvAJjZIDPLMLOMih7tVreMiEjZyh3uZtYAeB243t1/2XFxCQ8pFr/uPszd0909vXnz5jtXaUTdzyYyhwNosGJBhR4vIlIdlCvczSyNINhHuvv4EppkAW0KTbcGlu16ecXV2LyRA/ge+7X06yWLiFRn5TlbxoAXgDnu/s8Yzd4CLoqcNdMTWO/uy+NYZ0E9NYOSLS83EasXEUkJ5TlbphdwITDLzKI3LLwDaAvg7s8A7wJ9gAXAZmBg/EsNWK2aAHiujqiKiMRSZri7++eU3KdeuI0D18arqNJE99zJ1Z67iEgsyfcN1cieu8JdRCS2pA13dcuIiMSWfOGuA6oiImVKvnDXnruISJmSL9x1QFVEpEzJF+7RA6q6uIyISExJF+410nS2jIhIWZIu3POvHKZwFxGJKenCPX/PXd0yIiIxJV2464CqiEjZki/cdUBVRKRMSRfu0W4ZfYlJRCS2pAv3aLeMa89dRCSmpAt3akb23NXnLiISU/KFew1dW0ZEpCzJF+41dW0ZEZGyJG24a89dRCS25Av36DdUdUBVRCSm5Av3mtHz3LXnLiISS/KFu/bcRUTKlHzhHj2gmqM9dxGRWJI23PMU7iIiMSVfuEe6ZXJz1C0jIhJL8oV7tFtmu/bcRURiSb5w1567iEiZki/ctecuIlKmMsPdzIab2Sozy4yxfA8ze9vMZpjZd2Y2MP5lFhLZc8/brj13EZFYyrPnPgI4qZTl1wKz3b0LcDTwsJnttuulxaA9dxGRMpUZ7u4+Cfi5tCZAQzMzoEGk7fb4lFeC6KmQCncRkZhqxWEdTwBvAcuAhsC57p64PhMzAFzdMiIiMcXjgOqJwHRgb6Ar8ISZ7V5SQzMbZGYZZpaRnZ1d4Q3mWk11y4iIlCIe4T4QGO+BBcCPwAElNXT3Ye6e7u7pzZs3r/AG3Wroeu4iIqWIR7j/BBwHYGYtgI7AD3FYb0x52nMXESlVmX3uZvYqwVkwzcwsC7gbSANw92eAocAIM5sFGDDY3VcnrGIgr0YtLDcnkZsQEUlqZYa7u59XxvJlwAlxq6gcNtZvQZNflpGTA2lplbllEZHkkHzfUAVy2+5Du7wfueEGmDkT3MOuSESkaknKcG9y6D4cVHMeTz+ZS5cuTq1a0LUrnHEGfPwxrFoVdoUiIuFKynBP+/3RNMxdRy612F6zNhltz+SeVVez+T/vc+Zx62jZEt5/P+wqRUTCk5ThTr9+cP75ANTMzeGQRW/Qb/kzvM9JrKMxl6S9Qr9+8OWXIdcpIhKS5Az3unVh5MggvRcuhL/+FfbYA444AoAXfr2QO3e7n4sugi1bQq5VRCQEyRnuUT16wD77wNChsG4dTJ4MEycC8JcNt3HkvBcYNSrkGkVEQpDc4V6SE0+EzZvxI47gsRo38NR9v7BtW9hFiYhUrtQLd4C6dbGHHqJB3gYGLBjKuHFhFyQiUrlSM9wBevbEjz6aW3iIhfeNCbsaEZFKlbrhbob9/vcA3DnrXNatC7keEZFKlLrhDnD55QAsoyVTpoRci4hIJUrtcG/RgpwbbqUpa5jy8dawqxERqTSpHe5A2jFHUptt5L3zbtiliIhUmpQPd447jl/q78WJ8x5n8+awixERqRypH+716rHpiBPo4D8wYULYxYiIVI7UD3egRY92tGIpn36oG3yISPVQLcK9xm86UJM8Vk1N6N3/RESqjGoR7vToAUDP2S+Qo513EakGqke4H3gg29PqcFPugywaPTXsakREEq56hHuNGmT/6zUAFn+8MORiREQSr3qEO7DX2UcBsPRb3YNPRFJftQl3a9yI7VaL9fNX6YbaIpLyqk24Y8ave+xJ182TmTs37GJERBKr+oQ7sL1PP3rzPya/tSbsUkREEqpahfse/Y8HYObbi0OuREQksapVuNOuHQCrv/pBN84WkZRWvcJ9//3ZXq8h5+eM4NNPwy5GRCRxygx3MxtuZqvMLLOUNkeb2XQz+87MPotviXHUoAH8+TpO5j1G3L8y7GpERBKmPHvuI4CTYi00s0bAU0Bfd+8EnB2f0hKj1h8GUJM8mn02jiVLwq5GRCQxygx3d58E/FxKk/OB8e7+U6R91f6WUKdO/LpfJx7gVv5z//dhVyMikhDx6HPfH2hsZp+a2TQzuyhWQzMbZGYZZpaRnZ0dh01XTO0BZ1KfzfzpyQMYOhS2bw+tFBGRhIhHuNcCugOnACcCd5rZ/iU1dPdh7p7u7unNmzePw6Yr6NJL80fvugs+/DC8UkREEiEe4Z4FTHT3Te6+GpgEdInDehOnfXv4y18AuJYn+OabcMsREYm3eIT7m8BRZlbLzOoBhwFz4rDexOrbF4AHawzm8Qd/5fsJC9BNVkUkVZTnVMhXgS+AjmaWZWaXmdlVZnYVgLvPASYCM4GvgOfdPeZpk1VGjx7w3nvUzdvMU5suouOp+7Gkz5VhVyUiEhfmIV0iMT093TMyMkLZdj73oNP93nsByKIVvy7I4je/CbcsEZFYzGyau6eX1a56fUN1R2YwdCg89RQAy2u2Yd994a/958L69SEXJyJScdU73KOuvhouvphDc6fSlze59/UDmdGqj677LiJJS+EeFTnA+ianA9Bl0xRefz3MgkREKk7hHnXGGTB+fJFZZ58N774bUj0iIrtA4R5lFgT8v/6VP+sn2vCXi7OYODHEukREKkDhvqM//pFof0wbsvh2dRv6nJwXhH/krBoRkapO4V6SM8+EN97In3yBy4KRO+8MqSARkZ2jcI/l9NNhyBAABjIif3ZubjjliIjsDIV7ae66C3a4wJnOoBGRZKBwL41ZsUtGvn/+CN55J6R6RETKSeFeli5dYOxYOPpoAF7IHcgD533LL7+EW5aISGkU7uXRv39wmYKISRu78fRFX4RYkIhI6RTu5XX44UUmb3mzF5lV/9qXIlJNKdzLq2ZN+PVX2H13AGrg3HYbbNsWcl0iIiVQuO+M3XaDzz7Ln3xngnH2SRvIywuxJhGREijcd1bXrvD88/mTP3yyiDZtYOXKEGsSEdmBwr0iCt1gexadyV62jSefDLEeEZEdKNwrwgyefTZ/chu1mfBWrr69KiJVhsK9ogYNghNOyJ/8dcYcRo8OsR4RkUIU7rvixRfzR/u0+IY//hE2bw6xHhGRCIX7rth77/zRa4+fx7p10LOnbr8qIuFTuO+q3Fxo1452r/ydd67/iMxMGDw47KJEpLpTuO+qGjXgoosAOOXR4xl+2hs8+yycdVbIdYlItaZwj4e//Q1GjADgkrfOZAh3M2H8VpYtC7csEam+FO7xcvHFcMQRANzN3xjF+bRqBZ98EnJdIlItKdzj6YIL8kfP5A2MPI49Fu3Bi0ilU7jH09VXw9NP509+WLcfAKecoguMiUjlKjPczWy4ma0ys1IvcGtmh5pZrpn1j195ScYMrrwy/9rvx215hxn7nM7s6b+yxx7w1Vch1yci1UZ59txHACeV1sDMagL3A+/HoabkZgZ//Wt+F03nH97k7QEj2bf+cg47DH77W8jJCblGEUl5ZYa7u08Cfi6j2Z+A14FV8SgqJbzySv7oCf+5lllr9qYti8nMDO7aJyKSSLvc525mrYAzgGfK0XaQmWWYWUZ2dvaubrrqmzo1+Ll1KwDvnvsST3AtF1zg3HgjLFwYYm0iktLicUD1UWCwu5d5TUR3H+bu6e6e3rx58zhsuoo77LD8898BOr12N9fyFM1YzSOPQPd91/HrtTfmh7+ISLzEI9zTgdFmtgjoDzxlZqfHYb2p4eKLYfbsIrNW1WxJOxZxN/dQ+6lHmP3XkSEVJyKpapfD3d07uHt7d28PjAOucff/7HJlqeTAA+GOO/InLTeXhX98hEasA+Dhh+HDD8MqTkRSUXlOhXwV+ALoaGZZZnaZmV1lZlclvrwUctVVcMAB+ZM1X3qRc9tMASCHNEaORPdiFZG4qVVWA3c/r7wrc/dLdqmaVNamDcyZA5Mmwcknw4YN1NuwAYBDD9rMXS+tZbeXGvLNjFp06gQ1a4Zcr4gkNX1DtbL17h2cB1/IwMPnspYmPMZ1dOkCtWoVuZKBiMhOM3cPZcPp6emekZERyrarhLw8mDcv6I+PyK3XgJ4HbSD6tMycGXzpSUQkysymuXt6We205x6WGjWK9MED1GxQj6+/hn/emMU20hjYOUO37RORClG4h+3OOwvGV62CsWO54Z9tSGM71/Ik9esHZ1LmlvktAhGRAgr3sP3tb+AOe+0VTJ9zTv6ili2CLrNOneC88+DXX8MoUESSkcK9qnj55WKzTjrmV7KGPE8Nchk7FurUgb59g/cCEZHSKNyriuOPh59+yr8fKwCjR9NqyBVs7tQjf9bbb8PSpSHUJyJJReFelbRpAy+9BOPHQ9Om+bNrf/cNX93wKgcdVNBs8uSQahSRpKBwr4rOOANWr4bhw/NnHfrI+cw69FIcoy9vcuSR8OWX8MYb8OOPIdYqIlWSznOv6syKzVq2X29azf8sf7pXL/j888osSkTCovPcU8X8+fDqq0Vm7d1lT8a+vAXHeJ7LmDwZVq4MqT4RqZIU7lXdvvvCgAFBN82IEVC/PowbR/85wX1aL2M45/AaHTpAkybw7LPhlisiVYPCPVk0bRpcG75Ll2D6H//IX3TDPm+yZQusXRtcfPKBB+DCC2HJkpBqFZHQKdyTzeOPF5vVc/+1/PADDBoUfBdq8ODgFq533RVCfSJSJSjck0337nDTTUXn/fwzHdrl8eyzwanyQ4YEs0eMgDFjKrtAEakKFO7J6J57YNiwgumvvgouAG9G2k8Lubv7O8w/czAA554LRx0V3M513LiQ6hWRSqdwT0b168MVVwQnue+zT9FlTzwBp53GvuMf4OuvgtNcP/88yP+zz4Ypwc2fyM6G9esruW4RqTQK92R2+umQkRHc2Snq0UfzR9MP2sz27cFOfvS6ZL16wSefQNu20KEDrFsHPXvCjBmVXLuIJJTCPdk1bgzvvgsvvlh82bnnUtO3c8UVsHw5vPBCMPvYY2Hr1uDsmmefDb7p2rVr5ZYtIomlcE8Vl1wCDz9cdN6ECXDLLcFFyebM4dJL8hgzBho2LGgy4rY5XM5zQHBp+csu0426RVKBLj+QagYPDk5031GDBrBxI8ycyfYDf8uKFfC/Z77jvL8fDICRBwSXOpg+PXI6/datTOz/HFx9DSedojt2i1QFuvxAdfWPfwRHUC++GNq3L5i/cWPwc8oUatWC1vV+zg92gPpsyh/v2jW4Ztl7vf+Pkyb8mVdPfYX33tN15EWSicI91dSoERw1HTECvv66+PKffw76XQ49tMjsMc9vYNEiGDUKWrcOumd++HoNAHuwnj59glXfeGNw7vyOXTfZ9z7LN2MWJOZ3EpGdVivsAiSBmjULTofp1KngDh933AG1asEPPxRp2ufzO2D8KtpNmED//vDYY5B3S/DebxTssj/ySPDzggsgMxOGDoV9227jwDuvIo89GZmzkrPOCu4albLWrg3OJd1//7ArEYnN3UMZunfv7lJJfv3VfcEC9/R096B3JfYwY0bB4/70J3fwbQ884suXu48e7d6iRfGHHM7k/Alw79nT3cz9zjvdJ08Ofrq7/+9/7hdf7L59u3turnt2dijPxq7bZ5/g9xUJAZDh5chYdctUB7vtBr/5TXCPvttvDzrVr7ii5LZdugQHZSHohwHSauax117Bt12XL4ctW+Daa6F75xw+2vtCptCryCqmToUT/T0+GjqFXr2Cvfv774dLLw1uNPXgg8EHiObNgw8WK1cG7wwrVgTfyyrVqFHw/PO7+ITsoh0+9YhUSeV5B0jEoD33KmDuXPc6dWLvwderVzB94IHuK1cWffwDDxR73PJarf36336UP/0nHstfbBb8rF+/4CGnnBL8fPbZgnnLlwd79tu2uW/evEPN0UZhitaQmxtuHVItEa89dzMbbmarzCwzxvILzGxmZJhiZl3i/g4kidGxY8n36KtTJ9iD37y5YN6cOdCiRbCrHVV4PGKv7Vk8knZr/vTjXMdxPYMzccyC69zk5BS0nzAh+HnllQXzWrYMDhfsvVs2o+pdxu1/3sTYscEHjypl69awKxCJqTzdMiOAk0pZ/iPwO3fvDAwFhpXSVqqavfYKEvbMM4PpyZODA7CxNG4M6emQmxscWCzJDrcG/GhqA/zfr7BpE3z2WdC189ln8PLLQXfNxIlw8/XbueJyzz9GuXYt/IPbuYzhbPnXc3x8ztP07VtwYHfmzOAyCkOGBJsbPbp4GaNGwapVO/FcxPL557DffgWnk0YVfvMTqWrKs3sPtAcyy9GuMbC0POtUt0wVdsUVQbdD+/axD7yef37sZb/9bfF5Z55ZdBujR7t/+GEwvnVr0Oaee9zd/fXX3Z++cLL/t06fIuvoRkaRA7fRYR77+td0d3A/+WT3p55yv+UWzz+46+6+dGlwfPillyrwfBx+eLCyzz8PpqMb/umnij2/IruAcnbLxDvcbwaeL2X5ICADyGjbtm3inwWpmLVr3TMygvFNm9wvv9x90aLYYb7j0LZt8XnNmrmvWBEM0eQF97vvdt9996JvDGefXeJ67z6+6Fk5bVnkh/FFoXl5Bf375Pp93OoHMNvN3LvztZ/FWAf3p59279vX/dRT3Zctc1+yxP3VV91zcmI8H0cd5Q6+9qU3i9Y0d25l/UVE8lV6uAPHAHOApuVZp/bck9Bnn7l/8EHJe+YlDQ8+6N60adF5Bx9c/jeJHYf//jd/PGfzNt/Q64Qiy7fWbuiLev/Bzz/f/exuC/LnF93jz/NmrPK6bPKJnODH8F/fY4+C1XTo4P7jj+4DBwYfJJYudd942LHu4Hdwb9F6vvmmxKdpyhT3VasS+HcYPNj9008TuAGpyio13IHOwEJg//KszxXuyS0nJ9j9fest9xdfdO/Y0X3q1ODUltNOKwi/KVPc161z79q17OAu6QR6cK9Zs2D8uOOKBmusda1cGYRfCcv6HbHKHfwbitaUQ02vzRYH947M8dMZn794IsGbyJNcXeQxr/V+wnMmfuTu7o8/HuzIb/tunk/gZE8/YINv377zT+22bUEvVUx5eQU1RD3zjPuIETu/sV2Vk+M+dmxQk1SaSgt3oC2wADiiPOuKDgr3FLV1a9Dt8ve/F8z78Uf3li0LQmnffYuG7sSJQaqVFNR77132G8PODG+/HXPZZw9M9X79PH8a8vyAPdf4apqUus677wq6g9qw2Kc36u0O/kce95sHrvYFk1f4c3cv8bffLng6Jkxw/+stW/3nlds8Ly/41TOHTXb/97+9Z/dtvmfzvOA0y+uuc7/xRvcvvyx48KZNxcN9x+nKcs89wXbffLPyt12NxS3cgVeB5UAOkAVcBlwFXBVZ/jywFpgeGcq1YYV7NZSXF+zJu7vPmRNMFz4oeeihBUH1yCPu777rfuGF8Q33Tp1KD373/Omthx1VrnV+Q1f/mUaltnmT07x7py1+xu/WeDNW5c8fWP81/8+BtxVrv/6/XxedF/0YsHJl1Qn36EH1l1+u+DqGDnWvVSt+NVUDcd1zT8SgcJcSjRwZHNCNWrjQ/cQTg5dqgwbut93mfs45BYF2+unu993nfvPN7kOG7Frw16oV3zeSHYY/8Zh/zhEVX8esWcHzESvc//e/+P0dZs8OPjWU9kWtM84Itjt2bPnXO21a0IUXFa095tHsSjRvnvsTTyR+Oxs37tLDFe6SWrZtKx4AGzcWD5/vvnPv3dt91KjgjeDxx4Nr67z8snvjxkXDsnv3gvGGDXcuaK++2v2mm4IATOAbQuFhQ+sD/OM/vJA//fXX7mNG5xZtt3p18Dzk5rpfemlwOuuKFUWfo9mzCz5B7Sg7O/h2cu3awfrJY2Z/AAAMiElEQVQWLoz9Nzn55KDNmDHl/zvGemNas6Zg3kcfFe3WqyytWwe1FPtadBwNHx5sY/78Cq9C4S6yo9zc4KjnrFnBcYA1a4K9yHvvDf7Zzj036Ab67ruC0Hn99eBnr17BweK+fd3POqtg72vpUvd27YI2u+1WPJTHjk1Y2Ndgu/dgapF5X9zxli9a5O5fF3TrbLzk2uCUUzP33/0umH/sscWfn6++Kr6dadOKt8vLc3///YLrSOzMwdxY4X7ppe59+pTcprJEf59lyxK3jb59g2288UaFV6FwF9kVOTnBcQH34Bz/ss4IGTvWPSsr2EueOzc4gyU7u2g3SvTgcFpa8HOH/v+8Qw5xX7vW8/oXPc8/743/+Ipjzi1X4L/JaX49//R36hesY5G1K9Yux2r5+LHbfcNjz7vXrRuc2XT88cXX+e67xX/Xyy4r2uaAA9x//jl401y6tHj7WbPcO3cu2p0WVfhsqOj86PjcucHB95LWGQ8bNxb9BNMoctxk9uyi7aZMKfjeR0k2bQpOCsjLcx82LKj3uuuKdi9GnXlmsI1x4ypctsJdpKpYsqTgzeHHH4M3jpycIBQLB1t0D3j9evf77w+6l6LdBLNmlSvcyzssoq3/icfyp7/ofIVnH92/WLu82rWD4IqKdivsOBxV6OBz4SAcP77k9lGFL04HwSm20fHom8hDDwVfHJg3r2LP//vvB9+RKGzNmuK17LlnMD15ctG2ZX2SuOiiYPlrrxX9XQYPLt42+gb32msV+11c4S5S9U2f7v7YY0H4v/VW8U8HmzcX3fvbujW4Jn/HjgUBsm5d0I1wb9EvWP103MVlBvwqmhWbt5mSrxL6+UGX+88Pv1DmOvOHBQuKdA2VGO6LF5e+jquuKj4vN7d8B1+XLg0uNfpFwTeYi7w5HHhgwfxbbgnmtWkTTE+YUHRdJYX7L78UHMDu1i1Y/q9/Fa31L38Jln/ySbD37+5+3nnBsl04w0jhLpLKvvqqoNsoasiQ4GyP6JvEpk3FvyFcxvAwN8RcNoWevp0aMd8ACg+5Z59TepsLLyzoBok1XHxxyfNbtgze9KJdRjk5XuwbYz17Fn/c1KnBczN5cvFlGzcW3ITllVcK1lP4S2OFD0yfG+kmu/zyguV//3vRdd53X9C28JvDH/4QjD/5ZIX/9Ap3EQmOFzzwQNC10qpVsEf9738H//o9egSfGiIHEpct3ubz5+V5bq8jYwbu37ndr+GJUkP5Ga4sNu9ZrtipN5kyh1tvDX6edVbwJnHBBcGnmOhploW/NFd4iN5AYMfh7LMLDoi3ahV0g82fH7yJFG4XfUMp6fsSO37SePDBoG3hcL/kkqLBXwEKdxGJrXDXxpo1wQHRqM2bg08BkfPXN4x6y+f8/lrPa9rUN06d5RumfZ8fWLmH9fSNN92VP30t//I2LPbVNPFN1M2fX4Pt3on4HjeIOXz4oftee8VnXdE97ehw2mlBQJd0y8o+Ra9i6kOGFL9cRLR//o47KvynU7iLSGL99FPw/QN3z9u4yfP+8lefPW2z//hj0MVs5Prt16zzq3p/5x9+6J6Z6f7x/sFe/Qf8vsgB3cLDVHoUm/cuJ/n2veJ8KYrKGLKyCsa3bCm44ukufOFM4S4iocrOLvkM0pUrgy8bE4mgSfVO9FH73+112Oz/4lrfj+/91mYv+ElNpvp+BJ8STuUt71trgjv4O/Txp7nSW7LUwf3z+kWvDlquYeDAgvFXXskfz6Bb8bZPPRWfoI9+6atbt116XhXuIlLlrVhR8IXQbdvc33uvYPqDD4Ju7N69gzMwOx+U4y+0uM3bsLhIZu7H9/4sV/hvmO8OPoS7/E885nXZlH+Z5uN533/LDM+gmx9Te7Kf1DPoS19xxlU+aJD7A7et8S8GDffmFFy757v73vKFT77nEya4zz7vb55X+AybwkObNkF//bBhQdFlhfwuZl95w92CtpUvPT3dMzIyQtm2iCS/efPgllvgwguD2y4OHVp0eaNG0LkzTJoUTO+5Z5Cu2dmR5axlPXuw462k/8eRHMlkjDyg6C0jz27yX8b8/HtW12rBc9d9R6cZo2hxzzVszanJSy/BjTfCQdccTY3/fVbkcbdyP6fyDkelTcW+/bb0W1mWwcymuXt6me0U7iKSKr7/PrgB+8EHF53/wQfQrVtw4/WJE4N7wNeoAb/7HbRuDVlZBW1P+d1Gam9ZxyfzW5d4m+DfMpNFtGcDu8esoymr2e+g3Tiu2QxqT/qAuwjeeR5+OLj98CmnwEEHVex3VLiLiJRh2bJgD79ePRg8GLp0gfPPD5bl5EDHjtCqFdxzDxx3XDD/k0+gSRN49FF48cWd216zZrB6Ndx0Ezz0UMVqVriLiOyiLVuCvXwz2LQJFi8uusedmxt8AnjpJfj2W7jyShgzBo45Bu6+Gz6L9M7ssw8sWRK8YRx+OHz+efC4ilC4i4iEbM4c+PJLuOQS+OmnYG//mmtg330rvk6Fu4hICipvuFfwg4GIiFRlCncRkRSkcBcRSUEKdxGRFKRwFxFJQQp3EZEUpHAXEUlBCncRkRQU2peYzCwbWFzBhzcDVsexnHiqqrWprp2junaO6to5u1JXO3dvXlaj0MJ9V5hZRnm+oRWGqlqb6to5qmvnqK6dUxl1qVtGRCQFKdxFRFJQsob7sLALKEVVrU117RzVtXNU185JeF1J2ecuIiKlS9Y9dxERKYXCXUQkBSVduJvZSWb2vZktMLPbKnnbw81slZllFprXxMw+NLP5kZ+NI/PNzB6P1DnTzLolsK42ZvaJmc0xs+/M7LqqUJuZ1TGzr8xsRqSueyLzO5jZl5G6XjOz3SLza0emF0SWt09EXYXqq2lm35rZO1WlLjNbZGazzGy6mWVE5lWF11gjMxtnZnMjr7PDw67LzDpGnqfo8IuZXR92XZFt3RB5zWea2auR/4XKfX25e9IMQE1gIbAPsBswAzioErffG+gGZBaa9wBwW2T8NuD+yHgf4D3AgJ7AlwmsqyXQLTLeEJgHHBR2bZH1N4iMpwFfRrY3BhgQmf8McHVk/Brgmcj4AOC1BP89bwRGAe9EpkOvC1gENNthXlV4jb0EXB4Z3w1oVBXqKlRfTWAF0C7suoBWwI9A3UKvq0sq+/WV0Cc8AU/a4cD7haZvB26v5BraUzTcvwdaRsZbAt9Hxp8FziupXSXU+CZwfFWqDagHfAMcRvDNvFo7/k2B94HDI+O1Iu0sQfW0Bv4LHAu8E/mHrwp1LaJ4uIf6dwR2j4SVVaW6dqjlBGByVaiLINyXAE0ir5d3gBMr+/WVbN0y0SctKisyL0wt3H05QOTnnpH5odQa+Uh3CMFecui1Rbo+pgOrgA8JPnmtc/ftJWw7v67I8vVA00TUBTwK3ArkRaabVpG6HPjAzKaZ2aDIvLD/jvsA2cCLkW6s582sfhWoq7ABwKuR8VDrcvelwEPAT8BygtfLNCr59ZVs4W4lzKuq53JWeq1m1gB4Hbje3X8prWkJ8xJSm7vnuntXgj3lHsCBpWy7Uuoys1OBVe4+rfDssOuK6OXu3YCTgWvNrHcpbSurrloE3ZFPu/shwCaC7o6w6wo2FvRd9wXGltW0hHmJeH01BvoBHYC9gfoEf89Y205IXckW7llAm0LTrYFlIdUStdLMWgJEfq6KzK/UWs0sjSDYR7r7+KpUG4C7rwM+JejrbGRmtUrYdn5dkeV7AD8noJxeQF8zWwSMJuiaebQK1IW7L4v8XAW8QfCGGPbfMQvIcvcvI9PjCMI+7LqiTga+cfeVkemw6/o98KO7Z7t7DjAeOIJKfn0lW7h/DewXOeq8G8FHsbdCrukt4OLI+MUE/d3R+RdFjtD3BNZHPyrGm5kZ8AIwx93/WVVqM7PmZtYoMl6X4EU/B/gE6B+jrmi9/YGPPdIRGU/ufru7t3b39gSvoY/d/YKw6zKz+mbWMDpO0I+cSch/R3dfASwxs46RWccBs8Ouq5DzKOiSiW4/zLp+AnqaWb3I/2b0+arc11ciD3IkYiA44j2PoO/2L5W87VcJ+tByCN5tLyPoG/svMD/ys0mkrQFPRuqcBaQnsK4jCT7GzQSmR4Y+YdcGdAa+jdSVCdwVmb8P8BWwgOCjdO3I/DqR6QWR5ftUwt/0aArOlgm1rsj2Z0SG76Kv77D/jpFtdQUyIn/L/wCNq0hd9YA1wB6F5lWFuu4B5kZe9/8Galf260uXHxARSUHJ1i0jIiLloHAXEUlBCncRkRSkcBcRSUEKdxGRFKRwFxFJQQp3EZEU9P+jAltK4FoKAwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEICAYAAACzliQjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvqOYd8AAAIABJREFUeJzt3Xl8VNX5+PHPk7Dv+74EFRRUBImggLihoraoFRWLilpFrbjSb12wVhGt1Z9LsVqk7oqiggpaW1ELBQWFgOyLrEpAFkE22RLy/P44dzJ3tmQCmZlk8rxfr3nNveeee+fJJHnmzrnnniOqijHGmIohI9UBGGOMSR5L+sYYU4FY0jfGmArEkr4xxlQglvSNMaYCsaRvjDEViCV9Y4ypQCzpm7QhIlNF5GcRqZrqWIwpqyzpm7QgIlnAqYAC/ZP4upWS9VrGlAZL+iZdXA18DbwKDA4Uikh1EXlSRL4XkR0i8qWIVPe29RaRGSKyXUTWicg1XvlUEbned4xrRORL37qKyC0isgJY4ZX9zTvGThGZIyKn+upnish9IrJKRHZ521uLyHMi8qT/hxCRj0TkjkS8QcaAJX2TPq4GxnqPc0WkqVf+/4BuQE+gAfBHoEBE2gD/Bp4FGgNdgHkleL2LgB5AJ299tneMBsBbwHsiUs3bdhdwBXA+UAe4DtgDvAZcISIZACLSCDgLeLskP7gxJWFJ35R7ItIbaAu8q6pzgFXAb71keh1wu6quV9WDqjpDVfcDg4DPVfVtVc1T1a2qWpKk/xdV3aaqewFU9U3vGPmq+iRQFTjaq3s9cL+qLldnvld3FrADl+gBBgJTVXXTYb4lxsRkSd+kg8HAZFX9yVt/yytrBFTDfQiEax2jPF7r/CsiMkxElnpNSNuBut7rF/darwFXestXAm8cRkzGFMsuQplyzWufvwzIFJGNXnFVoB7QHNgHHAnMD9t1HdA9xmF/AWr41ptFqVM4PK3Xfn837ox9saoWiMjPgPhe60hgUZTjvAksEpETgI7AhzFiMqZU2Jm+Ke8uAg7i2ta7eI+OwHRcO//LwFMi0sK7oHqK16VzLNBXRC4TkUoi0lBEunjHnAf8RkRqiMhRwO+KiaE2kA9sASqJyAO4tvuAF4GHRaS9OJ1FpCGAqubirge8AUwINBcZkyiW9E15Nxh4RVV/UNWNgQfwd1y7/T3AQlxi3Qb8FchQ1R9wF1aHeeXzgBO8Yz4NHAA24ZpfxhYTw6e4i8LfAd/jvl34m3+eAt4FJgM7gZeA6r7trwHHY007JgnEJlExJrVEpA+umSdLVQtSHY9Jb3amb0wKiUhl4HbgRUv4Jhks6RuTIiLSEdiOu+D8TIrDMRWENe8YY0wFYmf6xhhTgZS5fvqNGjXSrKysVIdhjDHlypw5c35S1cbF1StzST8rK4ucnJxUh2GMMeWKiHwfTz1r3jHGmArEkr4xxlQglvSNMaYCsaRvjDEViCV9Y4ypQCzpG2NMBWJJ3xhjKhBL+sYYUxr27IFXXoEyPrRNXElfRPqJyHIRWSki98Soc5mILBGRxSLylq98sIis8B6DSytwY0wFNW8eXHUVHDxY8n3vvBM+/7z0YwK4+Wa47jr45pvEHL+UFDvgmohk4iaHOBsIzPJzhaou8dVpj5sk4kxV/VlEmqjqZhFpAOQA2bjp5eYA3VT151ivl52drXZHrjEmpiOOgDVrYNUqaNnSfQj06FH8fqqQkRFcLm1t28IPP0BODnTrVvrHL4aIzFHV7OLqxXOm3x1YqaqrVfUAMA64MKzODcBzgWSuqpu98nOBz1R1m7ftM6BfvD+EMcbw9dfw5pvB9cAZfkYGPPEEnHwyzJgBjz4KGzZEP8Zbb8F//xtc370b/vQnOHCg9OL84Qf3nJcXUjRs2KF9KUmUeJJ+S0Knfsv1yvw6AB1E5CsR+VpE+pVgX0RkiIjkiEjOli1b4o/eGJP+TjnFNecEFHhzzYjArl1u+bHHYPhwGDAAtm+PPMagQdC3b3B9xAgYORJefz203qhR7vWimTULGjWCn34qOt79+91zXh63DdrK609tYfZsXJv/1q0wbhyccgo/rd9fuMvdd8PY4iblLCXxJH2JUhb+3agS0B44HbgCeFFE6sW5L6o6RlWzVTW7ceNiB4kzxlRkgaSflwfNmrnlwMnizJlQv36w7rhx8MsvkcfYvds9f/xxaPntt7tvFkuXwoQJ8OmnwW0jR7qkPX165PG+/DK4HPj2cPnlfPhlI7bQhMz1P8CRR7oPjSuugK+/5rRWK5k1C37+GR5/HK68Mv634HDEM8pmLtDat94KCP8OlQt8rap5wBoRWY77EMjFfRD49516qMEaY9LcvffC2WfDmWdGbhs82G0PtJUcOBA8q5Yo55czZrgEe8klkdsCHxwTJ0aPo1On4PL778OyZcFmmypV2LvXrdapA99/D21PPTVYPxDTBx8Ei1aug40bQ14ii7X06HEsd9wRPYREiedMfzbQXkTaiUgVYCAwKazOh8AZACLSCNfcsxr4FDhHROqLSH3gHK/MGJPOLrkkeBYer2+/dc00Z50FmzfD2rUwd25w++uvw/nnw6ZNbj0vL5hg8/NDjyVCwcyv3fLMmREvdTDf1+DgfYjk9zkjely/+Q3cd1/hGfy0GZU460zlzLo5HHssrOgV2ilx2Tvzubn3wpCyhXPzCHcEqwF45hm4m8dQJDmN/6pa7AM4H9eDZxUw3CsbAfT3lgV4ClgCLAQG+va9DljpPa4t7rW6deumxphyYMkS1RdeCC175RXVRYtUXf+Y+I/1ww/BfeJ9zJ6tet99Jd8PdHqnG4Lrv/ziYihmnx+yTlUF/RWT9AZeUAU9l3/H9Xp3ZH0QUfYI9xauHkRUQTet3n3Ivw4gR+PI53H101fVT1S1g6oeqaqPeGUPqOokb1lV9S5V7aSqx6vqON++L6vqUd7jlcP/mDLGlIo334TevYPr27e7s9nAxdGAAweit4uffDLceCOsWOG6K65ZA9deC8cdF6xz8CB07w7jx0fuv2YNtG8Pc+bABReUPH7/mX4JLV0SPNP/uM3NvDl6d7H75K51Z+snMJ8uzAPgLp6K6/W2r43spd6EzXQjh3W0IsO71NntuP2Jv7crnk+GZD7sTN+YBNm7V3X8eNWCArdeeJp5UHXjxtAzUVXVH39U/eIL1Z49o5+1B+oOHuye//SnyLPcDz90z5mZofsGykH1oosO6Wz97y0e0f/V61/i/ZZwjP6T35V4v9l0K1zeTp0S7XsnT0Ytn8ivQ9ZH3LT+kH+9lOaZvjEmiVTdnaMLFpTuMZ980nVpDL94uW8f/PhjaFnTptC8uWtfnzHDlW3YADfc4OpPmBCs+9pr7jnaxdSLLnIvX6kS+vtbYMUKOneG/9zlu7T34YeH9CPdsmE4fbaHX14sWn6TFlThABLZibBYtQh+G6jLzhLte1/PqVHLO7I0tN5d+0ocV0lZ0jemrNm61V3dO/304uv26eMuboZTdXeqgmtGqVQpmOzD73ifOdM10fht3kyE4cPhxRfdDVEDBhQfm4/s34/843m49FJYuIBaq0vxA60EKnXqQFazfRzXsaDE+zZl0yG/bqPZ/4la3p6VIeuZ+YfWXFUSlvSNKWv27HHPP8ccrSRo+nT4978jy997D7p2de3pU6e6LoqzZ7tt338f2kbfty9cdlnxrzVtmnt+4IGom0N6xMQyfz4LOIHefFV8XZ+xzf/A1oxGJdonqs6dydy6hR7NfyjxrvWJctNXvPLCeu9cdx3asKFbPvvsYPk+O9M3pnzZt+/wu91Fu2gaTVEXMQNDAsye7QYBC99v5MiSx7V6dZGbn380jg+pOPTls4iyNb++jUoDLj7kY36eea77BnXzzS4B+4Zk0N69WT7kyZId8P773fM11xRf9+abI8tq1UJatXLLxx4L77zjlg/lgnYJWdI3pjRVrw4DBxZdZ9cumDw59vbdvp4kixdHr/PFF/CHP0SWr1sH//xn9OaZgAMH3Nl+KRjFrYXLt/L3Eu174gmRH467htzFF/RlJMNDyht0bkXdqu4seAVHxT7o66+7u3B9I2lO6XE3Tcc+BQ0awDHHROwimZkc/esOsY/5+OPB5dGjYf58ePhh14T20EOx9wP3QRPt5rCHHw5+uHfuDIGRCMJu4EqEeO7INcaURLTuieBuKhoxAtavd+3ra9e6ro7h/El/2zb3vHChOxt8+GHXtdI/jgy45pl589xt/lFuRvI78OUsqvx86O3TfmvJogXr2RA5pBbLOJpjWB51v/zWWcydlxExUEv1wZdzyVao8a8C8LV0tM0S+MrdgPUagxnJn6IHdMUV7vqFzxlT/uw+jANyciA7G44/3r2vmZlQo0Zwe6VKbiiG7Gx3Qb1yZQDeyxzIpTfeGPp6tWtHxjB0KPzd+wCsX9/dthuuTp3gGD6nnhocRsIfR4LYmb4xybB/P1x8MTz/fPCC6vz5kX3iISTp79qWx84Zi9zZ4COPuPrRBhR77z13MbaYhA9QZeuPwWEIYsgns9jjABSQwY80j7ptd8funHNW9KauSt/MiF5esyrjx8Pgq9z1gY2VWjGEF2jRAne37uWXc9XES6MHoxqR8AGoWjV0vVs39/M/8YRbz8goTOwRdZ56qnDbry+OcuxatdzzMccEO14++2xwuwjUrBk93gkT4JZb3Jg8gRj9H04JYknfmHCbN7smkmiWLw/trjhtmmuzXbuWIu+q+d3vIhPyhRfCiSdG1vUl/QEX5TG6l28kyLy80Au8118f+zWLkFe3Id2IPm/FR/yKe/lLXMfZ1/FEoo+rCCdmC5M/j5Fi6taNXl6lCgAN67sPpQYPDOW894fQtSvQpg2MG8fR3X37Rhv8LODXv3bPGVFiEN+QB5mZoUk/MKRDoAuqt61azSgfhJUruw9x/7DN4L5BBGKLdfZ+5pnuG4EIVKtWdN1SZEnfmHCXXgpDhrhEHqDq2sKPOSa0u+Jpp7l23lNOKfrs+T/Ru+yxcqXrC793r1s+88yQMeFrsZsGbAt5vf1LVwXDOuLIqId9gijt/T6Ld7RiD5EJpjPzebTreD4g9KJpbuMuIetbz7qMz1/N5ajr+sR8jYx8r8fKbbdFbgyc0Q4aBIFeLFCY9APvZZVqGVwcfv3Wn6B79w4OWRxu/Piir2306OGS7H33uYupsc6yA8k/2rcIgP793T0NfscdF7zbuSSJ3M70jYnTwoXuIma4JUvcoyQCyd7fze6++0KbCaZNc10hAzZuDB30a+3a0JurAsksmv/+1/WyefBBmDIlZGD1CQzgel4K1l28mKpXBrtXLt0S2Y1xKcfwR54IKTsY9q++hcasD2uHL0BYSGcGD6nKKo6iX/P5hdtaNgrtKdSwkdB3cEuGDYs9b0lh76JHHw2WBdq3A4n0zTdd23bgTDcs6Ue94StQN6B6dXeRNlyVKsELpNE0bOgupvbp49rmA11l/cNIQOg3gkMRq3nHL/DB0LPnob1GCdiFXJMeOnd2z+FNLMce65737Yts241m//5gd0d/18knw7r0nXZa5L7+pH/ccW7/goLQr+9FCbQPh98dW4T7n27A+97yS1zH73iZlqyPqJdXqTqZ+cGfp12nGpx2RB3wDSe/7oq7qfuJ62yybBlce21n+PLvMHQoErigfPLJ7iKn12Qi4p3knnCCS7KBewEgmET97/vy5UWffYcn/WhNM/Ek0UM1bx60bh1aFkj6sc70ixPP2XtWFnz1VVKmWbQzfZN469cXPzDWSy+5s63wIXIDDhwIJuN4+Wdh++ab0Iumc+e6jBXe99zflr97t0s+K1ZE3lwTzaW+C4yBD4xly9xzcR84p53Ggc+9m59K0G0vj2BTxxiGAFCHyIvDVWpVYcP9z1HQ2437ftT5HfjoI9yFygcfhPPOo+3Td7B9uzs5fuYZl8f5/e9dX/R33nHJb+hQd8DwZDxnjvsw8AskfX+ybNYs+AEdTeB9Cnx4R0v64P5eRoyIfZxDdcIJkd8aAn+Th5r0/T/DDTfAv/4VvV7PnvGdmBwmO9M3iVVQAK1auZ4r778fu94NN7h/9D17ondxu/ZaN8/p5MmhdzDu2OF6rvhNnerOmv1tqaed5rLZoEHubPWLL1z5p5+G3jxza7DfOfPnu9eK9y7JaHfGLlvmmoKide0LU2XV0mLrBAzlWQrI4BOCQzCswrXv6wUXcHAS+DvgZFSpTIuHfw8FN8GYMW5CEnBdEosiAq/4BscNJOPwZpdoTR+BpB+P8Hbzos70IfKGs0Q63OYdvzFjDv8YhyueUdmS+bBRNsupvLzg6I1+u3YFRxEsSqDObbe59VdfVX3gATcC5FtvhY5OqKr6+uuq99+vetVVkdsCy3PmxB75sEuX4PODD6pecIHq2rUlHnkx3sfB2nV0f6t2pXa8HsxUUJ0wIfjzPv/MftUNG1T37Al9H0D1nHNK5desr73mjnfVVUX/HkG1U6fI8lhefFG1Rg3V/Hy3Pneuq79qVenEfTj++lcXy//936EfI57/gcNEnKNspjzJhz8s6ZdToHrZZZHlGzYE/+BXrVJdutSV5+erfvqpS+qB/QOPL74oOukFEgKonntu7KT/5ZclS6ZnnFFqSTna4wMujCibz/HF7rfhf9+Flo0cqW+8ofrQQxr680b7nQQer79eOr/nV191xxs8OPr2mTNVb7/d1cnKioylPHr0URf7Pfcc+jFefNH9XSdQqSZ9oB+wHDf71T1Rtl8DbAHmeY/rfdsO+sonFfdalvTLofXrY/9TL18emchUVT/6yC0/8EDwn+pQHl27hq7feWdw+fPPDz9Z16tXakl/HJcVLi/hGFXQO3hKn730f7ph8Tb9uenRuoIjI/f1fXDuaXFE5Hsc672/6SZXPnly9G9hh+KVV9wxr7kmdp3du12dESOKj7E8+OorF/uUKamOpEjxJv1i2/RFJBN4DjgbN9H5bBGZpKrh/eDeUdWhUQ6xV1W7RCk35c2NN7p22jfeCC1fHuVW+zvucDen/O1v0Y8VaCcfOxZWrYpeJx7ffhu6/vTTweW9ew/9uAHNm4fcAftLnWbU3Bl6oXUjTWkWx7C7ywiO+9KuR1P4Zhl5VObEO/rQvBPoukX8/L1A+7B/y5o16VVnAV/t7IxUjdL189VX3YXqcM8/7+4OPdQLkNEEehg1KmLEy5o13YVvfxv4XXfB0UeXXhzJ1LOn60gQftdueVXcpwJwCvCpb/1e4N6wOtcAf4+x/+54Pn0CDzvTT5EpU9zcpkWJdrb25ZeurTN8W1Fnve+/r/qrX5XsTLlZs5KfXZ911iGdkRf8/hY98I8XVUH3NWxeWN6Sde5HDKv/KlfHddxq7AmuX3yxKugQRuuuXVHe56pVg3Xz87VDLfdtaucNd5XCL/swHDyoOmpU8LqBKTMoreYdYADwom/9qvAE7yX9H4EFwHigtW9bPpADfA1cFOM1hnh1ctq0aZOUN8iECSSYDh3c13NVd6H0/ffd8t69oYn97bdVH344Mrl98skhJdpiH9nZiTlulMeArNl6Ov9VBc0jU0c1HqHfcFJhlUG8UVj3KL7TLFaH7P8el2iBSOF6W9ao7t2rq1apbh76oGqLFqqXXqoKuuCOlyJ/F5s2qW7dGvJ+t2+vegQrdcfWvMT/LZhyqTST/qVRkv6zYXUaAlW95ZuA//q2tfCejwDWAkcW9Xp2pp8i/sR3yimh6+Hbw9cT8ejbVzUjI7j+619H1rn11pIdc8gQ17btKyvwvcYVjNWZRw7SauzRNqwtLA8/zOTJqiN7/kuHMFpBtT5bQyrc3eH9wvdoD9UK37IQAwe6+kVdYPW932vWqI4eXWq/bZOG4k368dyclQv4b1FrBYTceK2qW1U1cPfNP4Fuvm0bvOfVwFSgaxyvaRLl229jDyYWUNxIjSW9SSrgrLOilz/xhBvmwO/NN0MnI2nfPnK/WrXcjTRZWW5OV2D3qJfYdep5UV8mb9Q/2PJTsH95XbZzU8Hzhetv81tOWfUm+6hOLq2iHmPmTNd1/8aJ5/PHlTeSnw8vv1MrpM4DI7w29Fmz+HHayoiZCIES9/nOynKXVIw5XPEk/dlAexFpJyJVgIFAyGzEIuIfbag/uNl+RaS+iFT1lhsBvYASDoRiStWJJ7rBxI46KnhnYDHD7NIl7Dp8tDHgi3PJJZHHCWjfPnK6vlqhiZQBAwoTe0idH3+E775zF4T79uWkZwfzp+nnRH2ZKtUyaNIkuL6TuoxhCLXZSVVCb8B6b0Im46pczSDeDCnv6p2yNGrkRsTNzISLLgu9uFqjlvdvddJJHHFqS46KNudHIOkf7ixbxpRQsZf1VTVfRIYCn+Lu8XtZVReLyAjc14lJwG0i0h/Xfr8N18YP0BF4QUQKcB8wj2lkrx+TCqtWwW9/C6+9FjkhR7j584veHo9333VDEzRvHjnjU+fOkXdvho9X0q2b+5bSokWwrGHD4FgtZ50FZ53FMoGOtAnZddfUOQw+fW2MwITdRN4t+5vfwLYfX+MK3wCQDz0U513yrjmzaJb0TYrE1ZdLVT8BPgkre8C3fC+uV0/4fjOA4w8zRpMoO3e64RFijQVSUps2ubPxXr3c4FF+GRluKIJhw+D//i+YGDMy3DeH8JEwA7ffX3qpG4WxSpWIgbbGft6UWs3csPSqwTv5l9CpsM4eqlPn9BOB4Lj1zdkQMmaN3/nnu89CcC1Ht98e7HV64ECc70NpJf2mTaNPtWfMYbAB10xkgi6poUPdaXCTJi7hffll0WO6+M/iDx50CT6Q0Js2dTNEBbz7bnCCirCz/xfGN+Cii9wcFhkZwZGOv6MDT3EnnZlPTUK/QcyeDdurNWcrrp/5J5/AjBnuy87w4e7zb9CgYP1nnnFzllx6qbv1IKZRo4IfVMU1l0F8SX/jRnjuueKPZUwJ2IBrJvoEFAGPPOKyYUD79oRcmZw0KThDUbyqV3fNOa/7ZoRq185l3FNPjTo42Z49MHJkZXwjs7MTNzDbRRe59TPOcM9KBsN4qrDev/7lvkx06uS+DdSv7y4FLFsWvF/os89ih1uvnvvsKdKtt7pB3CZOjK/Jxpp3TIpY0q9o2rWDNWtCy7Zti14X3Bl4ZmYwObVqFUz6LVuWPOFD8Iy9Vy8OHnQnxpUr49pWwsyY4eZHGT8ePv8czqAv62lJZxaE3OEaULeuG3gzYNSoyMMef7xL+qU+M11JzvTr1XPP8Yyzb0wpsqRfEaxd68a079bNJfzLLnNnpYEz/PChif1U3TjpJ53k1qPNUFRS55wDL78MNWvSvbtrPgkMa19QAOed50ZQvuSS0OloAc6hiFNygiMlt2rl5tHu1CmyzttvuzP78LkyDlsg6cfTpj98uPtADQxxbEySWJt+OsvLc+PUt2vn5uscNcqVf/xxfGej4BJYdnZw5qgTTihZDL/6FaxfT06Oa/r/3//g/gbP89EjC5j4dVPmznWfQzt2uEdmpkv4EJnwwX1W+F15pfsxv/jCNfNkZ7v5USZNip7wwX1uXX55yX6MuJTkTD8wN2u6jOdiyo947uBK5sPuyC1F06dHvzO1TRvVatWib/v730PXn3oq9Jg7d6q288aFb9Ei9msHRrt88klVVW3btuibZd99NzjEffjjqqvckObz5rlDb9ig2r+/6pgxiXnbDtn69W70yb17Ux2JqYAorVE2TTkWq1P59OnRb7B6+eXIi6jZ2aHrtWu7qfO6d4+vGQNYuRK+/77oOhMmuOlJAx54IDgbnv96L7iu/hMnxvXSydWiRegsU8aUQda8k85i9cpp0yZ6+eWXh17d3LLF9aYJF7ghypf0A9d2Dx50o/z+739u208/RR9BISArC04/3X2O+FWtCqNHu96fxpjSY0k/nZ0XfQyamKpXD70BKtaY6V7SLyhQZs1y3SA7dHDPlSq5KWjnznVVH/1L6K6/+U3o+ltvuWb/gMBnTJMmbqyZXr1K9iMYY4pmST9d5edHL3/iidj7iMTVj3FPvkv6mzdDjx7R63zAxQB8QXCQteXLgzc+NWni7nzt0SN4bfiPf3T3YX3zTXLnvTamIrE2/XS1KcZMTuHj3oSrWzdq8b59rltlp06wOrcKxwFC7Db96fSJ2N6hg2vqGT/ede8PtBL17eva8zt3dp873bsXHaIx5tBZ0k9HH30UefWzOIGhDho2DCnOzXVn6Ndd50ZUrl4d6uytzEZCk/7kya77fUDHjjBtGtSpAzk57u5XcEk92nAyJe0Jaow5NJb001H//iXfJzCGQeBOUU/4DUx790Itr1UwkPQfftgleYD773frfj17uocxJvUs6aebwxjLZc4cOP74TAKjw4f31hw82F1YrZ5XDW6BGmeczKoX3b1fIu7G31bR5x4xxpQRlvTTzfr1oesnn+yGUSjGpEluiGKgsNFmzpzQOrffHphEpA50n03NY47hCN9cJ4cyt4oxJrni6r0jIv1EZLmIrBSRe6Jsv0ZEtojIPO9xvW/bYBFZ4T1soJFE++mn4HLlykV3kgd4+ml4+unChA8wnJFcx0uF6z17ui75Xf0TXWZnR85uZYwp84o90xeRTOA54GzcfLmzRWSSRs6A9Y6qDg3btwHwZyAbdwI5x9v351KJ3oT66qvgwDXgOs3v3h297oIF7gPijDNci5Bv+PtHGR5StVs3jDFpIp7mne7ASnUTmyMi44ALiW+u23OBz1R1m7fvZ0A/4O1DC9cUqXfv0PXKlYPTSUHoxCbHuwnNCgpcD51YLr0UbrmlFGM0xqRUPEm/JbDOt54LRLsl5xIR6QN8B9ypquti7NsyfEcRGQIMAWgTa4gAU7SdOyPLevaEZ591TTyPPBKcuAPXhn/00W6U5QULInf94gs36uXFFycwZmNM0sWT9CVKWfhdOR8Bb6vqfhG5CXgNODPOfVHVMcAYgOzs7PhG8TKh/DOHgBtG+Zpr3ABpjz0WsmnzZkLa8P0mT3ZTCp55ZmLCNMakVjxJPxfw99ZuBWzwV1BV/8he/wT+6tv39LB9p5Y0SBOH/ftD1884I+q0gwCLFkU/xMqVcOSRcPbZpRybMabMiKf3zmygvYi0E5EqwEBgkr+CiDRA0FtqAAAd4ElEQVT3rfYHlnrLnwLniEh9EakPnOOVmdK0bBncdVdoWWCMA48qLFnizuAvuMCVhffDP+KIBMZojCkTij3TV9V8ERmKS9aZwMuqulhERuAG7Z8E3CYi/YF8YBtwjbfvNhF5GPfBATAicFHXlKLA7bB+vrH0N250Y9D7TZzobtxdvtx9ZixaFHrN1xiTnkTjnAgjWbKzszUnJyfVYZR9Z5/tZgofPRpuuily+48/8sy4Zmze7Hpm/vOfwU3Dh8PIkckL1RiTeCIyR1Wzi6tnd+SWN3l58OCDLuFD9IQPfDS5akgPzYCjj7aEb0xFZuPplzeTJsGjjxZbbeDgKlHLr7qqtAMyxpQndqZf3kQbUK1ly4gxd/bj2vT79HFDHC9a5Nr1wwbRNMZUMHamX95s3x5ZVr9+yI1XAAe9z/Pf/c713Dn2WGjQADLsN25MhWYpoLzZvDmyrGbNmNm8evUEx2OMKVcs6Zc30QbKGTwY9ZL+Ao4P2WTNOcYYP0v65c3ChaHr998PN91EXoFr3rmAf1GDX5gxA2bOtLtrjTGh7EJueTB5MjRu7K7IzpgRuq1uXVavERrnZVAFuPO+GtQ7sgannJKSSI0xZZwl/bJu714499zQsrvvhrlz4bPPeGGM8O6/Ybz3q7zzD5lI/RTEaYwpF6x5p6xbuzayrHFj6NcPgOkrmvLf/8L51aZQcPudSL26yY3PGFOu2Jl+Wbd6dUTRDq3D2MrXMp3mjGMgAF2v7ULGM12SHZ0xppyxpF+W7dkT9Uz/mVfq8uCSSsAVhWXnnJO8sIwx5Zcl/bKsZs2oxUvX14kos0lPjDHxsDb9smTjRrjxRti3Dz7+OGa1H3aEJv0vv4Q6kZ8DxhgTwc70y5I//hHeeMOdtg8cGLPaDkIv1vbqlejAjDHpws70y5Jffomr2k7q8MUX0LYt3HNPgmMyxqSVuJK+iPQTkeUislJEYqYZERkgIioi2d56lojsFZF53mN0aQWelgIjaIbPdxumXec6nHmmu8b7l78kPixjTPooNumLSCbwHHAe0Am4QkQ6RalXG7gN+CZs0ypV7eI9os/4YZxA0t8anGf+myuf5RRmcC0vF5YtXBt9wnNjjClOPGf63YGVqrpaVQ8A44ALo9R7GHgc2FeK8VUs+fnu+aefCovGvZnH15zCq1xbWDbxI2uVM8YcmniyR0tgnW891ysrJCJdgdaqGq3LSTsR+VZE/icip0Z7AREZIiI5IpKzZcuWeGNPD1u3Bs/sA806vpmxXvGS/eWXB3fp0ydZwRlj0k08SV+ilBXOpi4iGcDTwLAo9X4E2qhqV+Au4C0RiehcqKpjVDVbVbMbN24cX+TponFjaNTILe8L/ZL0V+5mB25s5HHjkh2YMSYdxdNlMxdo7VtvBWzwrdcGjgOmighAM2CSiPRX1RxgP4CqzhGRVUAHIKcUYk8P6n1+DhsWkfR3UYvPP3fTHAJw/fWwYQPGGHOo4kn6s4H2ItIOWA8MBH4b2KiqO4BGgXURmQr8QVVzRKQxsE1VD4rIEUB7IHIwGQNPPQW1Qy/Qtji2AWed5Sv45z+TG5MxJu0U27yjqvnAUOBTYCnwrqouFpERItK/mN37AAtEZD4wHrhJVbcdbtBpYccOuOOO0LJdu0JW+9/QNIkBGWMqgrjuyFXVT4BPwsoeiFH3dN/yBGDCYcSXvt54A/72N7fcuDFEuYDdokuTJAdljEl31vcvFaZPh2XLguudgrc9jObGwuWMutYf3xhTuizpp0KfPvDcc2550iT49a8LN+2kDh8MGg8ZGXDUUSkK0BiTrizpJ9v114euX3ABy3a3Cim6+M1L3N25tWolMTBjTEVgST/ZXnopZDVnbgYdH7yM97kYgKvO2ZSKqIwxFYQl/RSbNg1A+DzrBgCaF1g/fGNM4ljST6EChGHDQASem+JdzD3xxNQGZYxJa5b0k2HHDjcpSli3zINkAu6mXMlqC8uXw8iRKQjQGFNR2MxZyTBmDLzzDrQKvWCb7739d97pFXTokOTAjDEVjSX9RFq8GP7zH9d+A2j+wZDR6w6SWTj0jjHGJIMl/US691746CPo2RMA+dszIZur17a33xiTXNamn0iBIZNnzIi6OfP8fkkMxhhjLOknVs2aMTfldesBr76avFiMMQZL+omVlxdzU+Vjj4Zq1ZIYjDHGWNJPrAMHChe/b3Fy6Da7gmuMSQFL+oly4IDrn9+sGf93+Q8cseHL0O2W9I0xKRBX0heRfiKyXERWisg9RdQbICIqItm+snu9/ZaLyLmlEXS5cMop8P775Neux/97pzUF3o1YhU44ITVxGWMqtGL7DIpIJvAccDZuvtzZIjJJVZeE1asN3AZ84yvrhJte8VigBfC5iHRQ1YOl9yOUUXPnArAnvwoAN98M/MPbNmsWdOuWmriMMRVaPGf63YGVqrpaVQ8A44ALo9R7GHgc8M/ufSEwTlX3q+oaYKV3vArjl/3uc/Wxx3yFJ53kxss3xpgkiyfztATW+dZzvbJCItIVaK2qH5d0X2//ISKSIyI5W6JMG1jufPZZ4WL+rr106AB16qQwHmOM8cST9CVKWeFVSBHJAJ4GhpV038IC1TGqmq2q2Y0bN44jpDLunHMKF2XXTrp0SWEsxhjjE884ALlAa996K8A/6Htt4DhgqrgxZpoBk0Skfxz7pr3a7KJp01RHYYwxTjxn+rOB9iLSTkSq4C7MTgpsVNUdqtpIVbNUNQv4GuivqjlevYEiUlVE2gHtgVml/lOUYXXZyZ/+5K2cdBL2CWCMSaViz/RVNV9EhgKfApnAy6q6WERGADmqOqmIfReLyLvAEiAfuKVC9NwJU9hiNatCfd4ZY8og0TJ2k1B2drbm5OSkOozDI2GXMsrYe2yMST8iMkdVs4urZ/0GS1sR4+0YY0yqWdIvLTt3wowZ5O3YE1oeGF7ZGGPKAEv6pWXAAOjVi7nnDw+WDR0KCxemLiZjjAljSb+0zJ4NQI/ZzwXLevSAZs1SFJAxxkSypJ9IRUyiYowxqWBJvzSo8suOKBdwa9RIfizGGFMES/qlYfhwauovkeV2pm+MKWMs6ZeG55+PXl61anLjMMaYYljSLwUa62JtpXiGNjLGmOSxpF8KNCNGcrekb4wpYyzpl4KD4Un//vvdc1ZW0mMxxpiiWNI/XMceS+XF80PLrr7ajbdTu3ZqYjLGmBgs6R+uJUsiyypXTn4cxhgTB0v6iWBJ3xhTRlnSTwS7gGuMKaPiSvoi0k9ElovIShG5J8r2m0RkoYjME5EvRaSTV54lInu98nkiMrq0f4BUijlMvp3pG2PKqGJPSUUkE3gOOBs35+1sEZmkqv7G7LdUdbRXvz/wFNDP27ZKVdNuavDly+GYYzRylnewpG+MKbPiOdPvDqxU1dWqegAYB1zor6CqO32rNSF6LkwXGzbAuedCNfYFC6++OrhsSd8YU0bFk/RbAut867leWQgRuUVEVgGPA7f5NrUTkW9F5H8icuphRVtG3H5rAXu+30wTNgcL69YNLlubvjGmjIon6UuUsogzeVV9TlWPBO4GvLuT+BFoo6pdgbuAt0SkTsQLiAwRkRwRydmyZUv80adI/xVPspmmfE9WsNDfJz8zM+kxGWNMPOJJ+rlAa996K2BDEfXHARcBqOp+Vd3qLc8BVgEdwndQ1TGqmq2q2Y0bN4439pQ5edOHkYX+YZTDJ0Y3xpgyIp6kPxtoLyLtRKQKMBCY5K8gIu19qxcAK7zyxt6FYETkCKA9sLo0Ak+VlSth0eYmkRtsRE1jTDlQbOOzquaLyFDgUyATeFlVF4vICCBHVScBQ0WkL5AH/AwM9nbvA4wQkXzgIHCTqm5LxA+SLIMGQUSfVYBq1ZIdijHGlFhcVxxV9RPgk7CyB3zLt8fYbwIw4XACLEt++glyZh2kDT9EbrQzfWNMOWDdTOKgCv/5D0ycCG/xW7oxN7ixShU4cMDO9I0x5YINwxCHjz+GP5y/mFYv3M/lvOsKr7kGpk+HC71bFuxM3xhTDljSL87cueTlbuILzuJ+HgmWZ2RA795QUODWrZumMaYcsOadovz4I3TrxuktjqUyYROfB5L9wYPu2ZK+MaYcsDP9ovz8MwANNiwmg4LQbZb0jTHlkCX9ouwLjq0j4TchB4bYtKRvjClHLOkXxZf0I870Bwxwz4Gkn5EBxx0HdSJGmTDGmDLDkn4027ax/fsdPD4imPSrVPIl/Xfegf793XIg6VeqBAsWwI4dSQzUGGNKxpJ+NA0bUqt9M6Z+6mveKTgY3N6sWXD5jDPcc9u2NuaOMabMs6QfEDYNVqW8fSHj5UuB70y/YcPg8j33wJo10N4//JAxxpRNlvQB5sxxbfJTpoQUZx+3L3r9mjWDyxkZkJWVuNiMMaYUWdIHmDbNPX8YOmRyXEnfGGPKEUv6EBxC4cCBkOLWjWIkff/Y+cYYU45Y0geYPNk9HzjA30cF2+6b1ouR9KtXT0JQxhhT+izpFxS44TMB9u9n85hgE0+DSrui75Nhb5sxpnyy7LV3b3D5wAFGLL4kuP7gg+65HEzhaIwx8Ygr6YtIPxFZLiIrRSRi4igRuUlEForIPBH5UkQ6+bbd6+23XETOLc3gS8UvwYHUdv50IHL7VVe53j3ffJPEoIwxJjGKTfreHLfPAecBnYAr/End85aqHq+qXYDHgae8fTvh5tQ9FugHPB+YMzelCgpg61a3vGdPYfHyKetD64nAww9D69bQvXsSAzTGmMSI50y/O7BSVVer6gFgHHChv4Kq7vSt1oTC0ckuBMap6n5VXQOs9I6XWg89BI0awebNIUn/JHJC6515prvT1hhj0kQ84+m3BNb51nOBHuGVROQW4C6gCnCmb9+vw/ZtGWXfIcAQgDZt2sQT9+H56CP3PH48vPde5PZ27dxdtldfHVq+eTPk5SU+PmOMSZB4zvSjDSijEQWqz6nqkcDdwP0l3HeMqmaranbjZFw0DfTLv+UWmDo1cnu9eu65bt3Q8saNoUWLhIZmjDGJFE/SzwVa+9ZbARuKqD8OuOgQ902O4uazDQycVqtW4mMxxpgkiifpzwbai0g7EamCuzA7yV9BRPyjjV0ArPCWJwEDRaSqiLQD2gOzDj/sw1StWtHbA0m/uHrGGFPOFNumr6r5IjIU+BTIBF5W1cUiMgLIUdVJwFAR6QvkAT8Dg719F4vIu8ASIB+4RVUPRn2hZCruTD8wRr7deWuMSTNxTYyuqp8An4SVPeBbvr2IfR8BHjnUABOiuKR/221w3XVw5JHJiccYY5KkYt6RW1yzzbXXuvH1wy/kGmNMOVcxk35xZ/rGGJOmLOl7OjbZmoJAjDEmueJq0087UZp3jj+tAdz4OeyKMbKmMcakgYqZ9CtF/ti33gqcelbyYzHGmCSqWM07l1wCPXoEu2T69O6dgniMMSbJKlbSf/99mDUratKXaANGGGNMmqlYST9g//5UR2CMMSlRMZP+88+HrG4aMzFFgRhjTHJVzKTv8xU9aXht/1SHYYwxSVHhk/4R7StF68xjjDFpqcIn/Zp1LeMbYyqOCp/0K1e3pG+MqTgs6dewpG+MqTgqfMarVLXCvwXGlHt5eXnk5uayb9++VIeScNWqVaNVq1ZUrlz5kPaPK+OJSD/gb7hJVF5U1cfCtt8FXI+bKGULcJ2qfu9tOwgs9Kr+oKplq6uMRkzZa4wpZ3Jzc6lduzZZWVlIGt9pqaps3bqV3Nxc2rVrd0jHKLZ5R0QygeeA84BOwBUi0ims2rdAtqp2BsYDj/u27VXVLt6jbCV8gN27Ux2BMeYw7du3j4YNG6Z1wgcQERo2bHhY32jiadPvDqxU1dWqegA38fmF/gqqOkVV93irX+MmQC8fdu5MdQTGmFKQ7gk/4HB/zniSfktgnW891yuL5XfAv33r1UQkR0S+FpGLou0gIkO8OjlbtmyJI6RStG1bcl/PGGNSKJ6kH+1jJWpDuIhcCWQDT/iK26hqNvBb4BkRiZh4VlXHqGq2qmY3btw4jpBKYPNmGDECCgqib1+zpnRfzxhT4Wzfvp3nw4Z3icf555/P9u3bExBRbPEk/VygtW+9FbAhvJKI9AWGA/1VtXBEM1Xd4D2vBqYCXQ8j3pK74Qb4859h+vTIbVddBWPHJjUcY0z6iZX0D0YZ0dfvk08+oV69eokKK6p4eu/MBtqLSDtgPTAQd9ZeSES6Ai8A/VR1s6+8PrBHVfeLSCOgF6EXeRMv0GYf7c1//fWkhmKMSbw77oB580r3mF26wDPPxN5+zz33sGrVKrp06ULlypWpVasWzZs3Z968eSxZsoSLLrqIdevWsW/fPm6//XaGDBkCQFZWFjk5OezevZvzzjuP3r17M2PGDFq2bMnEiROpXr166f4gxHGmr6r5wFDgU2Ap8K6qLhaRESIS6I3zBFALeE9E5onIJK+8I5AjIvOBKcBjqrqk1H+Kon8A91xBLvIYY5Lvscce48gjj2TevHk88cQTzJo1i0ceeYQlS1y6e/nll5kzZw45OTmMGjWKrVsj5+ResWIFt9xyC4sXL6ZevXpMmDAhIbHG1U9fVT8BPgkre8C33DfGfjOA4w8nwMNmSd+YCqWoM/Jk6d69e0g/+lGjRvHBBx8AsG7dOlasWEHDhg1D9mnXrh1dunQBoFu3bqxduzYhsaX/7aixLuAaY0yC1KxZs3B56tSpfP7558ycOZMaNWpw+umnR+1nX7Vq1cLlzMxM9u7dm5DYKs7YO/n5qY7AGJOmateuza5du6Ju27FjB/Xr16dGjRosW7aMr7/+OsnRhUr7M30tUATYt30f1VIdjDEmLTVs2JBevXpx3HHHUb16dZo2bVq4rV+/fowePZrOnTtz9NFHc/LJJ6cwUhAtY2PPZGdna05OTqkdb31WL1p+P4NLeZf3uCx0Yxn72Y0xh2bp0qV07Ngx1WEkTbSfV0TmePdEFSntm3f27nWJvQZ7iqlpjDHpL+2ad775xt2Ee9JJsGcP7PvFXci1pG+MMWmW9FUhvLnsa2Kc6d95Z5KiMsaYsiOtmnd++imyTLykX5cdoRueeioJERljTNmSVkl/3LjY2245dUHyAjHGmDIqrZJ+3m3DUCTk0Z3ZADScPtFVuuIK+PDDFEZpjDGpk1ZJ/5yaX7KcDjzIn2NXGjsWLrww9nZjjEmwWrVqAbBhwwYGDBgQtc7pp59OaXZfD0irpN/kQC6rmvZi0HcPxq5kY/AYY8qIFi1aMH78+KS+Zvr03snPp2HeRvY1bEn79mHbnnwShg1LSVjGmCRLwdjKd999N23btuX3v/89AA8++CAiwrRp0/j555/Jy8tj5MiRXBjWyrB27Vp+9atfsWjRIvbu3cu1117LkiVL6Nixo429U6xNm8ikgF/qRZnJ8a67kh+PMabCGDhwIO+8807h+rvvvsu1117LBx98wNy5c5kyZQrDhg2jqBEQ/vGPf1CjRg0WLFjA8OHDmTNnTkJiTZ8z/RYtaFf7Jy7tXCW0PDAm9UMPwZERMzUaY9JNCsZW7tq1K5s3b2bDhg1s2bKF+vXr07x5c+68806mTZtGRkYG69evZ9OmTTRr1izqMaZNm8Ztt90GQOfOnencuXNCYk2fpC9C7t6GVKrvrbduDevWwW9+49YfeCDmrsYYc7gGDBjA+PHj2bhxIwMHDmTs2LFs2bKFOXPmULlyZbKysqIOqewnSbjmGFfzjoj0E5HlIrJSRO6Jsv0uEVkiIgtE5AsRaevbNlhEVniPwaUZvN+BA2705MJhrBcvhi1bEvVyxhgTYuDAgYwbN47x48czYMAAduzYQZMmTahcuTJTpkzh+++/L3L/Pn36MNabs3vRokUsWJCYe4uKTfoikgk8B5wHdAKuEJFOYdW+BbJVtTMwHm8eXBFpAPwZ6AF0B/7szZtb6n75xT17PaGgdm1o1CgRL2WMMRGOPfZYdu3aRcuWLWnevDmDBg0iJyeH7Oxsxo4dyzHHHFPk/jfffDO7d++mc+fOPP7443Tv3j0hccbTvNMdWKmqqwFEZBxwIVA4162qTvHV/xq40ls+F/hMVbd5+34G9APePvzQI112GRTzvhpjTMIsXLiwcLlRo0bMnDkzar3du3cDbmL0RYsWAVC9enXGFTWsQCmJJ+m3BNb51nNxZ+6x/A74dxH7RnSvEZEhwBCANm3axBFSpPr1wXfx3BhjTBTxtOlHu7IQtd+RiFwJZANPlGRfVR2jqtmqmt24ceM4QjLGGHMo4kn6uUBr33orYEN4JRHpCwwH+qvq/pLsa4wxh6uszQKYKIf7c8aT9GcD7UWknYhUAQYCk/wVRKQr8AIu4W/2bfoUOEdE6nsXcM/xyowxptRUq1aNrVu3pn3iV1W2bt1KtWqHPuN3sW36qpovIkNxyToTeFlVF4vICCBHVSfhmnNqAe95/Ux/UNX+qrpNRB4Gb6hLGBG4qGuMMaWlVatW5ObmsqUCdNOuVq0arVq1OuT9035idGOMqQhsYnRjjDERLOkbY0wFYknfGGMqkDLXpi8iW4CiB6koWiMgyhTpKWdxlYzFVTIWV8mkY1xtVbXYG53KXNI/XCKSE8/FjGSzuErG4ioZi6tkKnJc1rxjjDEViCV9Y4ypQNIx6Y9JdQAxWFwlY3GVjMVVMhU2rrRr0zfGGBNbOp7pG2OMicGSvjHGVCBpk/SLm8c3wa/9sohsFpFFvrIGIvKZNzfwZ4FpIsUZ5cW5QEROTGBcrUVkiogsFZHFInJ7WYhNRKqJyCwRme/F9ZBX3k5EvvHiescb1RURqeqtr/S2ZyUiLl98mSLyrYh8XMbiWisiC0VknojkeGVl4e+snoiMF5Fl3t/aKamOS0SO9t6nwGOniNyR6ri817rT+7tfJCJve/8PyfsbU9Vy/8CN/rkKOAKoAswHOiXx9fsAJwKLfGWPA/d4y/cAf/WWz8fNLCbAycA3CYyrOXCit1wb+A43z3FKY/OOX8tbrgx8473eu8BAr3w0cLO3/HtgtLc8EHgnwb/Pu4C3gI+99bIS11qgUVhZWfg7ew243luuAtQrC3H54ssENgJtUx0XbubANUB139/WNcn8G0vom52sB3AK8Klv/V7g3iTHkEVo0l8ONPeWmwPLveUXgCui1UtCjBOBs8tSbEANYC5uCs6fgErhv1PcsN6neMuVvHqSoHhaAV8AZwIfe0kg5XF5r7GWyKSf0t8lUMdLYlKW4gqL5Rzgq7IQF8EpZBt4fzMf4+YST9rfWLo078Q1F2+SNVXVHwG85yZeeUpi9b4WdsWdVac8Nq8JZR6wGfgM901tu6rmR3ntwri87TuAhomIC3gG+CNQ4K03LCNxgZtqdLKIzBE3rzSk/nd5BLAFeMVrEntRRGqWgbj8BgJve8spjUtV1wP/D/gB+BH3NzOHJP6NpUvSj3se3zIg6bGKSC1gAnCHqu4sqmqUsoTEpqoHVbUL7sy6O9CxiNdOSlwi8itgs6rO8RenOi6fXqp6InAecIuI9CmibrJiq4Rr2vyHqnYFfsE1m6Q6Lvdirm28P/BecVWjlCXib6w+cCHQDmgB1MT9PmO9dqnHlS5JvyzOxbtJRJoDeM+BaSSTGquIVMYl/LGq+n5Zig1AVbcDU3HtqPVEJDCbm/+1C+PyttcFEjEDWy+gv4isBcbhmnieKQNxAaCqG7znzcAHuA/LVP8uc4FcVf3GWx+P+xBIdVwB5wFzVXWTt57quPoCa1R1i6rmAe8DPUni31i6JP1i5/FNgUnAYG95MK49PVB+tddb4GRgR+DrZmkTEQFeApaq6lNlJTYRaSwi9bzl6rh/hKXAFGBAjLgC8Q4A/qteI2dpUtV7VbWVqmbh/ob+q6qDUh0XgIjUFJHagWVcO/UiUvy7VNWNwDoROdorOgtYkuq4fK4g2LQTeP1UxvUDcLKI1PD+PwPvV/L+xhJ5ASWZD9zV9+9wbcPDk/zab+Pa5/Jwn8y/w7W7fQGs8J4beHUFeM6LcyGQncC4euO+Ci4A5nmP81MdG9AZ+NaLaxHwgFd+BDALWIn7Ol7VK6/mra/0th+RhN/p6QR776Q8Li+G+d5jceBvPNW/S++1ugA53u/zQ6B+GYmrBrAVqOsrKwtxPQQs8/723wCqJvNvzIZhMMaYCiRdmneMMcbEwZK+McZUIJb0jTGmArGkb4wxFYglfWOMqUAs6RtjTAViSd8YYyqQ/w+GWzjZ39t7rwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_loss, 'b', label='train')\n",
    "plt.plot(valid_loss, 'r', label='valid')\n",
    "plt.legend()\n",
    "plt.title(\"Loss\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(train_acc, 'b', label='train')\n",
    "plt.plot(valid_acc, 'r', label='valid')\n",
    "plt.legend(loc=4)\n",
    "plt.title(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = model.predict_classes(x_test)\n",
    "y_test_label = np.argmax(y_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy in testing data: 0.5446927374301676\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy in testing data:', accuracy_score(y_test_label, y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 4, 0, 5, 4, 3, 2, 2, 0, 0, 5, 4, 2, 0, 3, 2, 2, 5, 0, 5, 1, 5,\n",
       "       2, 5, 4, 5, 2, 2, 2, 1, 1, 2, 5, 1, 3, 3, 4, 5, 0, 4, 5, 0, 0, 5,\n",
       "       0, 4, 1, 0, 2, 5, 4, 2, 4, 5, 5, 5, 3, 5, 2, 2, 0, 3, 5, 2, 0, 4,\n",
       "       1, 3, 5, 0, 0, 0, 4, 2, 0, 0, 2, 3, 5, 5, 0, 1, 1, 4, 0, 0, 5, 1,\n",
       "       4, 1, 4, 0, 2, 5, 5, 1, 0, 4, 5, 4, 1, 2, 5, 0, 0, 4, 2, 0, 5, 5,\n",
       "       5, 4, 0, 4, 2, 1, 0, 1, 3, 4, 4, 3, 5, 2, 2, 5, 0, 4, 5, 0, 0, 0,\n",
       "       0, 0, 5, 0, 4, 4, 2, 2, 0, 0, 2, 5, 4, 4, 2, 5, 0, 5, 4, 3, 5, 0,\n",
       "       5, 0, 5, 0, 0, 3, 5, 0, 1, 4, 5, 5, 5, 1, 1, 4, 1, 5, 5, 5, 0, 0,\n",
       "       0, 0, 0, 5, 1, 4, 4, 0, 1, 0, 5, 0, 0, 0, 3, 5, 2, 4, 0, 3, 0, 0,\n",
       "       0, 2, 5, 0, 0, 5, 5, 0, 1, 4, 3, 3, 1, 5, 2, 4, 0, 2, 5, 0, 5, 1,\n",
       "       0, 2, 4, 0, 0, 1, 0, 4, 0, 4, 1, 3, 2, 4, 1, 1, 3, 4, 4, 1, 1, 5,\n",
       "       0, 1, 4, 1, 4, 0, 1, 0, 5, 2, 0, 5, 5, 0, 2, 4, 4, 0, 3, 5, 3, 4,\n",
       "       5, 3, 4, 0, 0, 0, 3, 5, 3, 0, 0, 1, 5, 0, 1, 2, 1, 5, 5, 1, 3, 1,\n",
       "       0, 5, 0, 4, 0, 5, 2, 5, 4, 5, 0, 1, 1, 5, 4, 3, 0, 4, 5, 1, 5, 5,\n",
       "       1, 5, 1, 5, 5, 4, 4, 4, 2, 1, 5, 5, 0, 5, 1, 0, 0, 5, 5, 0, 1, 5,\n",
       "       0, 5, 2, 5, 4, 2, 4, 1, 5, 0, 3, 2, 0, 2, 4, 3, 0, 5, 4, 5, 2, 0,\n",
       "       5, 5, 0, 5, 0, 1])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.global_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "model.save('my_model_v1.h5')\n",
    "\n",
    "# load model\n",
    "another_model = tf.keras.models.load_model('my_model_v1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>appearedTimeOfDay</th>\n",
       "      <th>appearedHour</th>\n",
       "      <th>appearedMinute</th>\n",
       "      <th>terrainType</th>\n",
       "      <th>closeToWater</th>\n",
       "      <th>city</th>\n",
       "      <th>continent</th>\n",
       "      <th>weather</th>\n",
       "      <th>temperature</th>\n",
       "      <th>...</th>\n",
       "      <th>cooc_142</th>\n",
       "      <th>cooc_143</th>\n",
       "      <th>cooc_144</th>\n",
       "      <th>cooc_145</th>\n",
       "      <th>cooc_146</th>\n",
       "      <th>cooc_147</th>\n",
       "      <th>cooc_148</th>\n",
       "      <th>cooc_149</th>\n",
       "      <th>cooc_150</th>\n",
       "      <th>cooc_151</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MTA5MTEwOTYxMzM0NzA2NDEzNzM=</td>\n",
       "      <td>morning</td>\n",
       "      <td>8</td>\n",
       "      <td>26</td>\n",
       "      <td>13</td>\n",
       "      <td>False</td>\n",
       "      <td>Ljubljana</td>\n",
       "      <td>Europe</td>\n",
       "      <td>MostlyCloudy</td>\n",
       "      <td>16.8</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MTY0OTUyMTM2MDExMjg3MjczMjU=</td>\n",
       "      <td>night</td>\n",
       "      <td>2</td>\n",
       "      <td>35</td>\n",
       "      <td>13</td>\n",
       "      <td>True</td>\n",
       "      <td>Los_Angeles</td>\n",
       "      <td>America</td>\n",
       "      <td>PartlyCloudy</td>\n",
       "      <td>18.6</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MTQ0ODU1OTEzOTU1NTAyNzI4NjE=</td>\n",
       "      <td>night</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>New_York</td>\n",
       "      <td>America</td>\n",
       "      <td>PartlyCloudy</td>\n",
       "      <td>30.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MTY1ODg1MzEwNzYzNDUzMDUwNTM=</td>\n",
       "      <td>morning</td>\n",
       "      <td>7</td>\n",
       "      <td>38</td>\n",
       "      <td>13</td>\n",
       "      <td>True</td>\n",
       "      <td>London</td>\n",
       "      <td>Europe</td>\n",
       "      <td>MostlyCloudy</td>\n",
       "      <td>18.5</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MTY0NTg2OTA5MTkzOTE2MTc1MTc=</td>\n",
       "      <td>night</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>12</td>\n",
       "      <td>False</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>America</td>\n",
       "      <td>Clear</td>\n",
       "      <td>20.2</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 183 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             id appearedTimeOfDay  appearedHour  \\\n",
       "0  MTA5MTEwOTYxMzM0NzA2NDEzNzM=           morning             8   \n",
       "1  MTY0OTUyMTM2MDExMjg3MjczMjU=             night             2   \n",
       "2  MTQ0ODU1OTEzOTU1NTAyNzI4NjE=             night             0   \n",
       "3  MTY1ODg1MzEwNzYzNDUzMDUwNTM=           morning             7   \n",
       "4  MTY0NTg2OTA5MTkzOTE2MTc1MTc=             night             1   \n",
       "\n",
       "   appearedMinute  terrainType  closeToWater         city continent  \\\n",
       "0              26           13         False    Ljubljana    Europe   \n",
       "1              35           13          True  Los_Angeles   America   \n",
       "2               5            0          True     New_York   America   \n",
       "3              38           13          True       London    Europe   \n",
       "4              27           12         False      Chicago   America   \n",
       "\n",
       "        weather  temperature    ...     cooc_142  cooc_143 cooc_144  cooc_145  \\\n",
       "0  MostlyCloudy         16.8    ...        False     False    False     False   \n",
       "1  PartlyCloudy         18.6    ...        False     False    False     False   \n",
       "2  PartlyCloudy         30.0    ...        False     False    False     False   \n",
       "3  MostlyCloudy         18.5    ...        False     False    False     False   \n",
       "4         Clear         20.2    ...        False     False    False     False   \n",
       "\n",
       "   cooc_146  cooc_147  cooc_148  cooc_149  cooc_150  cooc_151  \n",
       "0     False     False     False     False     False     False  \n",
       "1     False      True     False     False     False     False  \n",
       "2     False     False     False     False     False     False  \n",
       "3     False     False     False     False     False     False  \n",
       "4     False     False     False     False     False     False  \n",
       "\n",
       "[5 rows x 183 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_temp = df_all_dum[ train.shape[0]: ]\n",
    "#X_test_temp.shape\n",
    "#Y_test = train['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Y_test.shape\n",
    "#y_one_hot_test = np.zeros((len(Y_test), 10))  \n",
    "#y_one_hot_test[np.arange(len(Y_test)), Y_test] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test_temp.id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1791,)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test=X_test_temp.values\n",
    "y_predict = another_model.predict_classes(X_test)\n",
    "#y_test_label = np.argmax(Y_test.values, axis=1)\n",
    "#print('Accuracy in testing data:', accuracy_score(y_test_label, y_predict))\n",
    "y_predict.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_test_label = np.argmax(Y_test, axis=1)\n",
    "#print('Accuracy in testing data:', accuracy_score(y_test_label, y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 3., ..., 0., 0., 0.],\n",
       "       [3., 1., 1., ..., 0., 0., 0.],\n",
       "       [3., 1., 1., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 1., 3., ..., 0., 0., 0.],\n",
       "       [1., 0., 3., ..., 0., 0., 0.],\n",
       "       [1., 1., 3., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_temp = df_all_dum[ train.shape[0]: ]\n",
    "pd.DataFrame({'class': y_predict} ,index = testID).to_csv('pokemon_submission.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice\n",
    "Build a cat-dog-classifier with tf.keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
