{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gziNjJzn5XKA"
   },
   "source": [
    "![](https://cdn-images-1.medium.com/max/1600/1*jX6Gwn1rt4da7e-yUj84IQ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8xHkobL45XKB"
   },
   "source": [
    "### 請先在terminal執行\n",
    "`pip install --user catboost --no-cache-dir`\n",
    "\n",
    "`pip install --user lightgbm`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {}
     ]
    },
    "colab_type": "code",
    "id": "eE2EJUER5XKD",
    "outputId": "8d35f9d4-dc46-41e3-d753-c8974e09cb39"
   },
   "outputs": [],
   "source": [
    "import sys, os, psutil\n",
    "\n",
    "\n",
    "def cpuStats():\n",
    "    \"\"\" @author: RDizzl3 @address: https://www.kaggle.com/rdizzl3\"\"\"\n",
    "    print(\"########## CPU STATS ############\")\n",
    "    pid = os.getpid()\n",
    "    print(pid)\n",
    "    py = psutil.Process(pid)\n",
    "    memoryUse = py.memory_info()[0] / 2. ** 30\n",
    "    print('memory GB:', memoryUse)\n",
    "    print(\"########## CPU STATS ############\")\n",
    "\n",
    "import gc\n",
    "gc.enable()\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.sparse import csr_matrix, hstack, vstack\n",
    "\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "# from sklearn.cross_validation import KFold\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from multiprocessing import Process, Pool\n",
    "import functools\n",
    "\n",
    "import re\n",
    "import unidecode\n",
    "import math\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as ctb\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "9KfCQinC5XKJ"
   },
   "outputs": [],
   "source": [
    "PATH = 'data/'\n",
    "train = pd.read_table(PATH + 'train.tsv', engine='c')\n",
    "test = pd.read_table(PATH + 'test.tsv', engine='c')\n",
    "\n",
    "train = train.loc[train.price > 0]\n",
    "train.reset_index(drop=True, inplace=True)\n",
    "y = np.log1p(train[\"price\"].values)\n",
    "test_id = test.test_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8CqhxKOW5XKM"
   },
   "source": [
    "## Helper Function，請執行後忽略。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "jo97yIg55XKN"
   },
   "outputs": [],
   "source": [
    "Hash_binary = True\n",
    "\n",
    "def handle_missing_inplace(dataset):\n",
    "    dataset['category_name'].fillna(value='missing', inplace=True)\n",
    "    dataset['brand_name'].fillna(value='missing', inplace=True)\n",
    "    dataset['item_description'].fillna(value='No description yet', inplace=True)\n",
    "\n",
    "    \n",
    "def preprocess(text): # 這是之前的作業的簡易版\n",
    "    non_alphanums = re.compile(u'[^A-Za-z0-9]+')\n",
    "    # regex for short = re 請參考 http://ccckmit.wikidot.com/regularexpression \n",
    "    \n",
    "    text = unidecode.unidecode(text)\n",
    "    text = str(text).lower()\n",
    "    return u\" \".join(\n",
    "        [x for x in [y for y in non_alphanums.sub(' ', text).strip().split(\" \")]])\n",
    "        # strip split 請參考 http://ericbbs.blogspot.tw/2009/07/python-strip-split.html\n",
    "        # [x for x in.....] 這文言文：是 list comprehension\n",
    "\n",
    "    \n",
    "### 以下是multithread ，請自行忽略 ，多執行緒不在本課程範圍。 \n",
    "\n",
    "def multi_hash(data=None, hash_vec=None, n_jobs=4):\n",
    "\n",
    "    p = Pool(n_jobs)\n",
    "    csr_parts = p.map(hash_vec.fit_transform, np.array_split(data, n_jobs))\n",
    "    p.close()\n",
    "    p.join()\n",
    "    return vstack(csr_parts).tocsr\n",
    "\n",
    "def multi_apply(df=None, feat_list=None, func=None, axis=0, raw=True, n_jobs=4):\n",
    "\n",
    "    p = Pool(n_jobs)\n",
    "    f_ = p.map(functools.partial(apply_func, func=func, axis=axis, raw=raw),\n",
    "               np.array_split(df[feat_list], n_jobs))\n",
    "    f_ = pd.concat(f_, axis=0, ignore_index=True)\n",
    "    p.close()\n",
    "    p.join()\n",
    "    return f_.values\n",
    "\n",
    "def apply_func_series(data=None, func=None):\n",
    "\n",
    "    return data.apply(func)\n",
    "\n",
    "def multi_apply_series(df=None, feature=None, func=None, n_jobs=4):\n",
    "\n",
    "    p = Pool(n_jobs)\n",
    "    f_ = p.map(functools.partial(apply_func_series, func=func),\n",
    "               np.array_split(df[feature], n_jobs))\n",
    "    f_ = pd.concat(f_, axis=0, ignore_index=True)\n",
    "    p.close()\n",
    "    p.join()\n",
    "    return f_.values\n",
    "    \n",
    "\n",
    "def apply_func(data=None, func=None, axis=0, raw=True):\n",
    "\n",
    "    return data.apply(func, axis=axis, raw=raw)\n",
    "\n",
    "\n",
    "def preprocess_text_features(df):\n",
    "\n",
    "    df[\"item_description\"] = multi_apply_series(df=df[[\"item_description\"]],\n",
    "                                                feature=\"item_description\",\n",
    "                                                func=preprocess,\n",
    "                                                n_jobs=4)\n",
    "    df[\"name\"] = multi_apply_series(df=df[[\"name\"]],\n",
    "                                    feature=\"name\",\n",
    "                                    func=preprocess,\n",
    "                                    n_jobs=4)\n",
    "    \n",
    "def get_hashing_features(df, Hash_binary, start_time):\n",
    "    # df = pd.concat([train, test])\n",
    "    dim = 20\n",
    "    ha = HashingVectorizer(\n",
    "        n_features=2 ** dim,\n",
    "        ngram_range=(1, 2),\n",
    "        norm=None,\n",
    "        alternate_sign=False,\n",
    "        binary=Hash_binary\n",
    "        # stop_words='english'\n",
    "    )\n",
    "\n",
    "    X_name = ha.fit_transform(df['name'])\n",
    "    cpuStats()\n",
    "    X_name += ha.fit_transform(df['item_description'])\n",
    "    cpuStats()\n",
    "    \n",
    "    print('[{}] Finished hashing'.format(time.time() - start_time))\n",
    "    return X_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wk30L3x85XKQ"
   },
   "source": [
    "## 預處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {},
      {}
     ]
    },
    "colab_type": "code",
    "id": "yiWHTwAO5XKR",
    "outputId": "df74b11e-ab1e-4cf0-fff2-555068ee9318"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:5: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## CPU STATS ############\n",
      "28607\n",
      "memory GB: 1.5297737121582031\n",
      "########## CPU STATS ############\n",
      "########## CPU STATS ############\n",
      "28607\n",
      "memory GB: 2.5126876831054688\n",
      "########## CPU STATS ############\n",
      "[121.27781701087952] Finished hashing\n",
      "有 1048576 欄位\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "handle_missing_inplace(train) # 處理 NaN \n",
    "\n",
    "nrows = train.shape[0]\n",
    "merge = pd.concat([train, test])\n",
    "del train, test \n",
    "gc.collect()\n",
    "\n",
    "preprocess_text_features(df=merge)\n",
    "merge = get_hashing_features(merge, Hash_binary, start_time) # Hash Trick\n",
    "\n",
    "print('有 {} 欄位'.format(merge.shape[1]) )\n",
    "\n",
    "csr_train = merge[:nrows]\n",
    "csr_test = merge[nrows:]\n",
    "del merge\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P0oV3ELL5XKX"
   },
   "source": [
    "### 避免Hub，跑太久，使用L1 Selection，選特徵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {},
      {}
     ]
    },
    "colab_type": "code",
    "id": "Pt3OF1iT5XKZ",
    "outputId": "2dd51df5-2a45-4fa8-fdf7-557a792b008f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[123.21011924743652] Starting SGD l1 selection\n",
      "Features reduced from    1048576 to     446994\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('[{}] Starting SGD l1 selection'.format(time.time() - start_time))\n",
    "sgd_l1 = SGDRegressor(max_iter=30, penalty=\"l1\", random_state=1, alpha=1e-6)\n",
    "sgd_l1.fit(csr_train, y)\n",
    "good_feats = np.abs(np.array(sgd_l1.coef_)) > 1e-6 \n",
    "print(\"Features reduced from %10d to %10d\" % (csr_train.shape[1], int(good_feats.sum())))\n",
    "\n",
    "csr_train = csr_train[:, good_feats]\n",
    "csr_test = csr_test[:, good_feats]\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KCzsP4pL5XKd"
   },
   "source": [
    "### XGBoost Lightgbm Catboost\n",
    "- XGBoost [參數](https://github.com/dmlc/xgboost/blob/master/doc/parameter.md)\n",
    "- LightGBM [參數](http://lightgbm.readthedocs.io/en/latest/Parameters.html)\n",
    "- CatBoost [參數](https://tech.yandex.com/catboost/doc/dg/concepts/parameter-tuning-docpage/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "YpwI1e-l5XKe"
   },
   "outputs": [],
   "source": [
    "class Xgb(object):\n",
    "    def __init__(self, seed=2018, params=None):\n",
    "        self.param = params\n",
    "        self.param['seed'] = seed\n",
    "        self.nrounds = params.pop('nrounds', 100) # 避免跑太久，所以設100\n",
    "\n",
    "    def train(self, xtra, ytra, xte, yte):\n",
    "        dtrain = xgb.DMatrix(xtra, label=ytra)\n",
    "        dvalid = xgb.DMatrix(xte, label=yte)\n",
    "        watchlist = [(dtrain, 'train'), (dvalid, 'eval')]\n",
    "        self.gbdt = xgb.train(self.param, dtrain, self.nrounds,\n",
    "            watchlist, verbose_eval=20)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.gbdt.predict(xgb.DMatrix(x))\n",
    "\n",
    "class Lgb(object):\n",
    "    def __init__(self, seed=2018, params=None):\n",
    "        self.param = params\n",
    "        self.param['seed'] = seed\n",
    "        self.nrounds = params.pop('nrounds', 100)# 避免跑太久，所以設100\n",
    "\n",
    "    def train(self, xtra, ytra, xte, yte):\n",
    "        #ytra = ytra.ravel()\n",
    "        #yte = yte.ravel()\n",
    "        dtrain = lgb.Dataset(xtra, label=ytra)\n",
    "        dvalid = lgb.Dataset(xte, label=yte)\n",
    "        watchlist = [(dtrain, 'train'), (dvalid, 'eval')]\n",
    "        self.gbdt = lgb.train(self.param, dtrain, self.nrounds)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.gbdt.predict(x)\n",
    "\n",
    "class Cat(object):\n",
    "    def __init__(self, seed=2018, params=None):\n",
    "        self.seed = seed\n",
    "        self.param = params\n",
    "        self.nrounds = 100 # 避免跑太久，所以設100\n",
    "\n",
    "    def train(self, xtra, ytra, xte, yte):\n",
    "        self.gbdt = ctb.CatBoostRegressor(depth=4,\n",
    "            iterations=self.nrounds, random_seed=self.seed,\n",
    "            use_best_model=True)\n",
    "\n",
    "        xtra = pd.DataFrame(xtra)\n",
    "        ytra = pd.DataFrame(ytra)\n",
    "        xte = pd.DataFrame(xte)\n",
    "        yte = pd.DataFrame(yte)\n",
    "\n",
    "        self.gbdt.fit(X=xtra, y=ytra, eval_set=(xte, yte),\n",
    "                      use_best_model=True)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.gbdt.predict(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1XfClWFE5XKh"
   },
   "source": [
    "## Meta KFold with OOF (Out Of Fold)\n",
    "### K折交叉驗證"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "E-eVN6cb5XKi"
   },
   "outputs": [],
   "source": [
    "fold = 5 # 手動設置要幾個fold\n",
    "\n",
    "def oof(model, ntrain, ntest, kf, train, labels, test):\n",
    "    # model, 用的模型\n",
    "    # ntrain, 訓練集的row number\n",
    "    # ntest,  測試集的row number\n",
    "    # kf,     Kfold obj\n",
    "    # train,  訓練集\n",
    "    # labels, 目標\n",
    "    # test    測試集\n",
    "    \n",
    "    # 先配置空間\n",
    "    oof_train = np.zeros((ntrain,))\n",
    "    oof_test = np.zeros((ntest,))\n",
    "    oof_test_skf = np.empty((fold, ntest)) # fold X ntest 空間 \n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(kf.split(train)): # 開始分割\n",
    "        x_tr = train[train_index]\n",
    "        y_tr = labels[train_index]\n",
    "        x_te = train[test_index]\n",
    "        y_te = labels[test_index]\n",
    "\n",
    "        model.train(x_tr, y_tr, x_te, y_te) # 訓練 (fold-1)個 fold\n",
    "\n",
    "        oof_train[test_index] = model.predict(x_te) # 去預測 train left fold，稱作meta-train\n",
    "        oof_test_skf[i, :] = model.predict(test) # 去預測 test，稱作meta-test\n",
    "\n",
    "    oof_test[:] = oof_test_skf.mean(axis=0) # all folds score 取平均\n",
    "    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ghsor_8j5XKl"
   },
   "source": [
    "### Level 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "98trQ2pI5XKl"
   },
   "outputs": [],
   "source": [
    "def level_1(train, labels, test):\n",
    "    #train = train\n",
    "    #test = test\n",
    "    #labels = labels\n",
    "\n",
    "    ntrain = train.shape[0]\n",
    "    ntest = test.shape[0]\n",
    "\n",
    "    kf = KFold(n_splits=fold, shuffle=True, \n",
    "               random_state=2018)\n",
    "    lgb_params = {}\n",
    "    lgb_params['boosting_type'] = 'gbdt'\n",
    "    lgb_params['objective'] = 'regression'\n",
    "    lgb_params['metric'] = 'rmse'\n",
    "    lgb_params['num_leaves'] = 2**5\n",
    "    lgb_params['max_depth'] = 4\n",
    "    lgb_params['feature_fraction'] = 0.9\n",
    "    lgb_params['bagging_fraction'] = 0.95\n",
    "    lgb_params['bagging_freq'] = 5\n",
    "    lgb_params['learning_rate'] = 0.3\n",
    "\n",
    "    xgb_params = {}\n",
    "    xgb_params['booster'] = 'gbtree'\n",
    "    xgb_params['objective'] = 'reg:linear'\n",
    "    xgb_params['learning_rate'] = 0.3\n",
    "    xgb_params['max_depth'] = 4\n",
    "    xgb_params['subsample'] = 0.8\n",
    "    xgb_params['colsample_bytree'] = 0.7\n",
    "    xgb_params['colsample_bylevel'] = 0.7\n",
    "\n",
    "    cat_params = {}\n",
    "    cat_params['learning_rate'] = 0.3\n",
    "    cat_params['depth'] = 3\n",
    "    cat_params['bagging_temperature'] = 0.8\n",
    "    cat_params['loss_function']='RMSE'\n",
    "    cat_params['eval_metric']='RMSE'\n",
    "    \n",
    "    cg = Cat(seed=2018, params=cat_params)\n",
    "    xg = Xgb(seed=2018, params=xgb_params)\n",
    "    lg = Lgb(seed=2018, params=lgb_params)\n",
    "    \n",
    "    ##########################################################################\n",
    "    xg_oof_train, xg_oof_test = oof(xg, ntrain, ntest, kf, train, labels, test)\n",
    "    lg_oof_train, lg_oof_test = oof(lg, ntrain, ntest, kf, train, labels, test)\n",
    "    cg_oof_train, cg_oof_test = oof(cg, ntrain, ntest, kf, train, labels, test)\n",
    "    ##########################################################################\n",
    "    \n",
    "    print(\"CG-CV: {}\".format(mean_squared_error(labels, cg_oof_train)))\n",
    "    print(\"XG-CV: {}\".format(mean_squared_error(labels, xg_oof_train)))\n",
    "    print(\"LG-CV: {}\".format(mean_squared_error(labels, lg_oof_train)))\n",
    "\n",
    "    x_train = np.concatenate((cg_oof_train, xg_oof_train, lg_oof_train), axis=1)\n",
    "    x_test = np.concatenate((cg_oof_test, xg_oof_test, lg_oof_test), axis=1)\n",
    "\n",
    "    np.save(arr=x_train, file='x_concat_train.npy')\n",
    "    np.save(arr=x_test, file='x_concat_test.npy')\n",
    "    np.save(arr=labels, file='y_labels.npy')\n",
    "\n",
    "    return x_train, labels, x_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uvstV4lK5XKo"
   },
   "source": [
    "### Level 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "H0m3QDXS5XKp"
   },
   "outputs": [],
   "source": [
    "def level_2():\n",
    "    train = np.load('x_concat_train.npy')\n",
    "    labels = np.load('y_labels.npy')\n",
    "    test = np.load('x_concat_test.npy')\n",
    "\n",
    "    dtrain = xgb.DMatrix(train, label=labels)\n",
    "    dtest = xgb.DMatrix(test)\n",
    "\n",
    "    xgb_params = {}\n",
    "    xgb_params[\"objective\"] = \"reg:linear\"\n",
    "    xgb_params[\"eta\"] = 0.1\n",
    "    xgb_params[\"subsample\"] = 0.9\n",
    "    xgb_params[\"max_depth\"] = 5\n",
    "    xgb_params['eval_metric'] = 'rmse'\n",
    "    xgb_params['min_child_weight'] = 10\n",
    "    xgb_params['seed'] = 2018\n",
    "\n",
    "    res = xgb.cv(xgb_params, dtrain, num_boost_round=500, nfold=5, seed=2018, stratified=False,\n",
    "                 early_stopping_rounds=25, verbose_eval=10, show_stdv=True)\n",
    "\n",
    "    best_nrounds = res.shape[0] - 1\n",
    "    cv_mean = res.iloc[-1, 0]\n",
    "    cv_std = res.iloc[-1, 1]\n",
    "\n",
    "    print('')\n",
    "    print('Ensemble-CV: {0}+{1}'.format(cv_mean, cv_std))\n",
    "    bst = xgb.train(xgb_params, dtrain, best_nrounds)\n",
    "\n",
    "    preds = np.expm1(bst.predict(dtest)) # 一開始把目標取了np.log1p()，現在inverse回去\n",
    "    return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {}
     ]
    },
    "colab_type": "code",
    "id": "xhkBJqHr5XKr",
    "outputId": "0bc05bbb-56ae-4653-9796-00c320326422"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ntrain\n",
      "1481661\n",
      "[0]\ttrain-rmse:1.88488\teval-rmse:1.88422\n",
      "[20]\ttrain-rmse:0.659102\teval-rmse:0.659262\n",
      "[40]\ttrain-rmse:0.635362\teval-rmse:0.636036\n",
      "[60]\ttrain-rmse:0.620507\teval-rmse:0.621588\n",
      "[80]\ttrain-rmse:0.609511\teval-rmse:0.611007\n",
      "[99]\ttrain-rmse:0.601611\teval-rmse:0.603455\n",
      "[0]\ttrain-rmse:1.88478\teval-rmse:1.88758\n",
      "[20]\ttrain-rmse:0.660638\teval-rmse:0.66147\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    x_train, labels, x_test = level_1(csr_train, y, csr_test)\n",
    "    preds = level_2()\n",
    "    sub = pd.DataFrame()\n",
    "    sub['id'] = test_id\n",
    "    sub['price'] = preds\n",
    "    sub.to_csv('stacking.csv', index=False)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DrI521Bm5XKt"
   },
   "source": [
    "- 我們已同意將完整版solution開源，如果您有興趣 [here](https://github.com/goldentom42/kaggle_mercari_2017/blob/master/mercari.py)\n",
    "    - 完整版因為資源限制，所以跟Excercise 99%作法不同，也更powerful。\n",
    "- Kudos to Teammates [Olivier](), [Mark Peng](), [Rand Xie](), [Yifan Xie]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "H9cDEkvx5XKu"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "06_Pratice_stacking.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
