{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    " \n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation,Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D,MaxPooling1D\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "from keras.layers.recurrent import LSTM,GRU\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from keras.models import model_from_json\n",
    "import numpy as np\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_all shape : (1141, 8)\n"
     ]
    }
   ],
   "source": [
    "df_all = pd.read_csv('data/19999_question_category_b_sap_v4.csv')\n",
    "print(f\"df_all shape : {df_all.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['SAP-168', 'SAP-2X8'], dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping = df_all.category.astype('category').cat.categories\n",
    "mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from /project/at081-group3/Project_19999/19999_CNN/jieba/dict_taiwan.txt ...\n",
      "Loading model from cache /tmp/jieba.u1430022de19ce5d7760cc83d31504a1a.cache\n",
      "Loading model cost 0.519 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "from NLP_JiebaSegmentor import JiebaSegmentor\n",
    "jieba_dict_path1 = \"jieba/dict_taiwan.txt\"\n",
    "jieba_dict_path2 = \"jieba/userdict.txt\"\n",
    "jieba_dict_path3 = \"jieba/dict.txt.big\"\n",
    "jieba_dict_path4 = \"jieba/dict.txt.small\"\n",
    "jieba_dict_path5 = \"jieba/dict_system.txt\"\n",
    "jieba_stopwords_path = \"jieba/stopwords.txt\"\n",
    "js = JiebaSegmentor(dict_path=jieba_dict_path1,\n",
    "                    userdict=[jieba_dict_path2,\n",
    "                              jieba_dict_path3,\n",
    "                              jieba_dict_path4],\n",
    "                    stopwords=True,\n",
    "                    stopwords_path=jieba_stopwords_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cut_raw_0 = js.lcut('FTP無法安裝', return_type='pandas')\n",
    "test_cut_raw_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train=np.loadtxt(\"x_train.txt\",dtype=int)\n",
    "# y_train=np.loadtxt(\"y_train.txt\",dtype=int)\n",
    " \n",
    "# indices = np.arange(x_train.shape[0])\n",
    "# np.random.shuffle(indices)\n",
    "# x_train = x_train[indices]\n",
    "# y_train = y_train[indices]\n",
    " \n",
    "# print('Loading data...')\n",
    "# #x_train=np.loadtxt(\"x_train.txt\",dtype=int)\n",
    "# #y_train=np.loadtxt(\"y_train.txt\",dtype=int)\n",
    "# x_test=x_train[20000:]\n",
    "# y_test=y_train[20000:]\n",
    "# x_train=x_train[:20000]\n",
    "# y_train=y_train[:20000]\n",
    "# #x_test=x_train\n",
    "# #y_test=y_train\n",
    "# print(len(x_train), 'train sequences')\n",
    "# print(len(x_test), 'test sequences')\n",
    "# print(x_train[:1])\n",
    "# print('Pad sequences (samples x time)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.groupby('category')['category']\\\n",
    "             .agg({'count': 'count'})\\\n",
    "             .sort_values(by='count', ascending=False)\\\n",
    "             .reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_sample = df_all[df_all['category'] == 'SAP-168'].sample(n=300, random_state=1)\n",
    "# df_all = df_all[df_all['category'] != 'SAP-168']\n",
    "# frames = [df_all, df_sample]\n",
    "# df_all = pd.concat(frames)\n",
    "# print(df_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w_pandas = js.lcut('無法會員登入 顯示訊息 無效的帳號或不存在 帳號'.strip(), return_type='pandas')\n",
    "# w_list = list(w_pandas['word'])\n",
    "# w_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_to_word(s):\n",
    "    w_pandas = js.lcut(s, return_type='pandas')\n",
    "    w_list = list(w_pandas['word'])\n",
    "    combie = ''\n",
    "    for i,w in enumerate(w_list):\n",
    "        if w is ' ':\n",
    "            continue\n",
    "        combie = combie + w\n",
    "        if i < len(w_list) - 1:\n",
    "            combie = combie + ','\n",
    "            \n",
    "    return combie    \n",
    "\n",
    "# print (cut_to_word('可以幫我聯絡'.strip()))\n",
    "df_all['cut_words'] = df_all['description_clean'].apply(lambda s: cut_to_word(s.strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 資料打散\n",
    "df_all = df_all.sample(frac=1).reset_index(drop=True)\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 切分資料\n",
    "X = df_all['cut_words']\n",
    "y = df_all['category_target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.3, random_state = 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label 做 onehot\n",
    "y_train_one_hot = np_utils.to_categorical(y_train)\n",
    "y_val_one_hot = np_utils.to_categorical(y_val)\n",
    "print (y_train_one_hot[0])\n",
    "print (y_val_one_hot[0])\n",
    "# 總共類別數\n",
    "num_classes = len((y_train_one_hot[0]))\n",
    "print ('num_classes = {}'.format(num_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 標點符號過濾\n",
    "# WORD_FILTERS = '!\"#$&()*+,-./:;<=>?@[\\\\]^_{|}~\\t\\n'\n",
    "# 字典數量\n",
    "NUM_WORDS = 5001\n",
    "# 向量長度\n",
    "MAX_LEN = 50\n",
    "\n",
    "token_intent = Tokenizer(num_words=NUM_WORDS,\n",
    "                        split=\",\")\n",
    "# token_intent = Tokenizer(filters=WORD_FILTERS,\n",
    "#                         num_words=NUM_WORDS,\n",
    "#                         split=\",\")\n",
    "\n",
    "# 斷詞後全部合併丟到fit_on_texts,組出字典\n",
    "token_intent.fit_on_texts(X_train)\n",
    "token_intent.fit_on_texts(X_val)\n",
    "\n",
    "def preprocessing(x, max_len):\n",
    "\n",
    "    #將文字轉為數字序列\n",
    "    train_seq_intent = token_intent.texts_to_sequences(x)\n",
    "\n",
    "    # 截長補短，讓所有數字序列長度一樣\n",
    "    data = sequence.pad_sequences(train_seq_intent, maxlen=MAX_LEN)\n",
    "    # print train_data_intent.shape\n",
    "    return data\n",
    "\n",
    "x_train = preprocessing(X_train, max_len=MAX_LEN)\n",
    "x_val = preprocessing(X_val, max_len=MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "訓練model\n",
    "\"\"\"\n",
    "# set parameters:\n",
    "\n",
    "batch_size = 20\n",
    "filters = 32\n",
    "kernel_size = 7\n",
    "epochs = 50\n",
    "train_ratio = 0.9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.add(Conv1D(activation=\"relu\", padding=\"same\", filters=64, kernel_size=5))\n",
    "# model.add(MaxPooling1D(pool_size=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_1DCNN_model(num_classes, vocab_size, sentence_max_len):\n",
    "    print('Build model...')\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Embedding(input_dim=vocab_size, \n",
    "                        output_dim=32, \n",
    "                        input_length=sentence_max_len))\n",
    "\n",
    "    model.add(Dropout(0.75))\n",
    "    \n",
    "    model.add(Conv1D(filters,\n",
    "                     kernel_size,\n",
    "                     padding='valid',\n",
    "                     activation='relu',\n",
    "                     strides=1))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "                     \n",
    "    # We add a vanilla hidden layer:\n",
    "#     model.add(Dense(128))\n",
    "#     model.add(Dropout(0.5))\n",
    "#     model.add(Activation('relu'))\n",
    "    model.add(Dense(32))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_1DCNN_model(num_classes, NUM_WORDS, MAX_LEN)\n",
    "model.compile(optimizer='Adam', \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '{}.h5'.format('19999_1dcnn_sap')\n",
    "\n",
    "checkpoint = ModelCheckpoint(model_path, monitor='val_loss', save_best_only=True, verbose=1)\n",
    "earlystop = EarlyStopping(monitor='val_acc', patience=25, verbose=1)\n",
    "\n",
    "model_history = model.fit(x_train, y_train_one_hot,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split= 1 - train_ratio,\n",
    "          callbacks = [checkpoint, earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import matplotlib.pyplot as plt\n",
    "def show_train_history(train_history,train,validation):\n",
    "    plt.plot(train_history.history[train])\n",
    "    plt.plot(train_history.history[validation])\n",
    "    plt.title('Train History')\n",
    "    plt.ylabel(train)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_train_history(model_history,'acc','val_acc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_train_history(model_history,'loss','val_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_cat_name(x): \n",
    "        return mapping[x]\n",
    "    \n",
    "def predict(test):\n",
    "    model = load_model(model_path)\n",
    "    y_predict_probability = model.predict(test)\n",
    "    y_predict = model.predict_classes(test)\n",
    "\n",
    "    return to_cat_name(y_predict), y_predict, y_predict_probability\n",
    "\n",
    "y_predict_name, y_predict, y_predict_probability = predict(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_name = to_cat_name(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict_classes(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.predict(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict_df = pd.DataFrame({'1_sentence':df_test.sentence,\n",
    "#               '2_y_predict':y_predict,\n",
    "#               '3_y_predict_name':y_predict_name,\n",
    "#               '4_answer':df_test.target,\n",
    "#               '5_y_predict_probability':list(y_predict_probability)})\n",
    "# predict_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_df = pd.DataFrame({\n",
    "              '1_sentence':X_val,\n",
    "              '2_y_predict':y_predict,\n",
    "              '3_y_predict_name':y_predict_name,\n",
    "              '4_answer':y_val,\n",
    "              '5_answer_name':answer_name,\n",
    "              '6_y_predict_probability':list(y_predict_probability)})\n",
    "# predict_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy\n",
    "accuracy = 1 - (float(len(predict_df[predict_df['2_y_predict'] != predict_df['4_answer']])))/(len(predict_df))\n",
    "print ('accuracy : {} !!!!!!!!'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_df[predict_df['2_y_predict'] != predict_df['4_answer']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "pd.crosstab(np.array(y_predict_name), np.array(answer_name),\n",
    "            rownames=['predict'], colnames=['answer'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = '{}.h5'.format('19999_1dcnn')\n",
    "# model.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model_json = model.to_json()\n",
    "# with open(\"model.json\", \"w\") as json_file:\n",
    "#     json_file.write(model_json)\n",
    "# model.save_weights(\"model.h5\")\n",
    "# json_file = open('model.json', 'r')\n",
    "# loaded_model_json = json_file.read()\n",
    "# json_file.close()\n",
    "# loaded_model = model_from_json(loaded_model_json)\n",
    "# # load weights into new model\n",
    "# loaded_model.load_weights(\"model.h5\")\n",
    "# print(\"Loaded model from disk\")\n",
    "# loaded_model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "# score = loaded_model.evaluate(x_test, y_test, verbose=0)\n",
    "# print(score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
